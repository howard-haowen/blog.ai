{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-02-16-Adding a custom tokenizer to spaCy and extracting keywords.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1kBP5GlMMgXDnDrv6z5_l6aXvAtc1iPYE",
      "authorship_tag": "ABX9TyM3a7cvsbbKp3A1m8PyDNI3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUy9F0x2xf3H"
      },
      "source": [
        "# Adding a custom tokenizer to spaCy and extracting keywords \n",
        ">  This post shows how to plug in a custom tokenizer to spaCy and gets decent results for the extraction of keywords from texts in traditional Chinese.  \n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- categories: [keyword-extraction, spacy, textacy, ckip-transformers, jieba, textrank, rake]\n",
        "- image: images/keywords.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCHRkukEG2fQ"
      },
      "source": [
        "![](https://github.com/howard-haowen/blog.ai/raw/master/images/keywords.png \"Credit: Alex Hallatt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJroVGIC0e0P"
      },
      "source": [
        "# Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeU-wUIk0iIk"
      },
      "source": [
        "spaCy is an `industrial-strength natural language processing` library in Python, and supports multiple human languages, including Chinese. For segmenting Chinese texts into words, spaCy uses Jieba or PKUSeg under the hood. However, neither of them beats CKIP Transformers in accuracy when it comes to traditional Chinese (see my previous [post](https://howard-haowen.github.io/blog.ai/tokenization/jieba/pkuseg/pyhanlp/snownlp/ckip-transformers/2021/01/29/Many-ways-to-segment-Chinese.html) for a comparison). So I'll show how to plug in CKIP Transformers to `spaCy` to get the best out of both. \n",
        "\n",
        "For the purpose of demonstration, I'll situate this integration in a pipeline for extracting keywords from texts. Compared with other NLP tasks, keyword extraction is a relatively easy job. TextRank and RAKE seem to be among the most widely adopted algorithms for keyword extraction. I tried most of the methods mentioned in [this article](https://monkeylearn.com/keyword-extraction/), but there doesn't seem to be any easy-peasy implementation of TextRank or RAKE that produces decent results for traditional Chinese texts. So the first part of this post walks through a pipeline that actually works, and the second part records other methods that failed. I included the second part because I believe in this quote:\n",
        "\n",
        "> “We learn wisdom from failure much more than from success. We often discover what will do, by finding out what will not do; and probably he who never made a mistake never made a discovery.” ― Samuel Smiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkazzb9fJueC"
      },
      "source": [
        "> Note: TextRank is based on Google's PageRank, which is used to compute the rank of webpages. This [article](https://nlpforhackers.io/textrank-text-summarization/) on Natural Language Processing for Hackers demonstrates the connection between the two. From it I learned a tidbit: I always assumed that `Page` as in PageRank refers to webpages, but it turns out to be the family name of Larry Page, the creator of PageRank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrUhHptHeG45"
      },
      "source": [
        "# Working pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK8FOJIbJd4U"
      },
      "source": [
        "## Set variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzG1bPd4Qkcb"
      },
      "source": [
        "Let's start with defining two variables that users of our keyword extraction program might want to modify: `CUSTOM_STOPWORDS` for a list of words that users definitely hope to exclude from keyword candidates and `KW_NUM` for the number of keywords that they'd like to extract from a document.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maau2fTOJf4E"
      },
      "source": [
        "CUSTOM_STOPWORDS = [\r\n",
        "                    \"民眾\",\"朋友\",\"市民\",\"人數\", \"全民\",\"人員\",\"人士\",\"里民\",\r\n",
        "                    \"影本\",\"系統\", \"項目\", \"證件\", \"資格\",\"公民\", \"對象\",\"個人\",\r\n",
        "                    ]\r\n",
        "\r\n",
        "KW_NUM = 10"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBRwC5uweJs0"
      },
      "source": [
        "## Preprocess texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsEFjEbvSXCA"
      },
      "source": [
        "I took an announcement from Land Administration Bureau of Kaohsiung City Goverment as a sample text, but you can basically take any text in traditional Chinese to test the program. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYlK8e8hTO_Y"
      },
      "source": [
        "> Tip: To run the program with your own text, follow the following steps: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehoQTmQ1j8FC"
      },
      "source": [
        "1. Click on `Open in Colab` at the upper right corner of this page. \n",
        "2. Click on `File` and then `Save a copy in Drive`.  \n",
        "3. Replace the following text with your own text. \n",
        "4. Click on `Runtime` and then `Run all`. \n",
        "5. Go to the section `Put it together` to see the outcome. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "Y_zk2OX5aW94",
        "outputId": "cabbbc4c-bc07-4dbf-967c-b4446f2d08c2"
      },
      "source": [
        "#collapse\r\n",
        "\r\n",
        "raw_text = '''\r\n",
        "市府地政局109年度第4季開發區土地標售，共計推出8標9筆優質建地，訂於109年12月16日開標，合計總底價12 億4049萬6164 元。\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "第93期重劃區，原為國軍眷村，緊鄰國定古蹟-「原日本海軍鳳山無線電信所」，市府為保存古蹟同時活化眷村遷移後土地，以重劃方式整體開發，新闢住宅區、道路、公園及停車場，使本區具有歷史文化內涵與綠色休閒特色，生活機能更加健全。地政局首次推出1筆大面積土地，面積約2160坪，地形方整，雙面臨路，利於規劃興建景觀大樓，附近有市場、學校、公園及大東文化園區，距捷運大東站、鳳山國中站及鳳山火車站僅數分鐘車程，交通四通八達，因土地稀少性及區位條件絕佳，勢必成為投資人追逐焦點。\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "第87期重劃區，位於省道台1線旁，鄰近捷運南岡山站，重劃後擁有完善的道路系統、公園綠地及毗鄰醒村懷舊文化景觀建築群，具備優質居住環境及交通便捷要件，地政局一推出土地標售，即掀起搶標熱潮，本季再釋出1筆面積約93坪土地，臨20米介壽路及鵬程東路，附近有岡山文化中心、兆湘國小、公13、公14、陽明公園及劉厝公園，區位條件佳，投資人準備搶進！\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "第77期市地重劃區，位於鳳山區快速道路省道台88線旁，近中山高五甲系統交流道，近年推出土地標售皆順利完銷。本季再推出2筆土地，其中1筆面積約526坪，臨保華一路，適合商業使用；1筆面積107坪，位於代德三街，自用投資兩相宜。\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "高雄大學區段徵收區，為北高雄優質文教特區，優質居住環境，吸引投資人進駐，本季再推出2標2筆土地，其中1筆第三種商業區土地，面積約639坪，位於大學26街，近高雄大學正門及萬坪藍田公園，地形方正，使用強度高，適合興建優質住宅大樓；另1筆住三用地，面積約379坪，臨28米藍昌路，近高雄大學及中山高中，交通便捷。\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "另第37期重劃區及前大寮農地重劃區各推出1至2筆土地，價格合理。\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "第4季土地標售作業於109年12月1日公告，投資大眾可前往地政局土地開發處土地處分科索取標售海報及標單，或直接上網高雄房地產億年旺網站、地政局及土地開發處網站查詢下載相關資料，在期限前完成投標，另再提醒投標人，本年度已更新投標單格式，投標大眾請注意應以新式投標單投標以免投標無效作廢。\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "為配合防疫需求，本季開標作業除於地政局第一會議室辦理外，另將於地政局Facebook粉絲專頁同步直播，請大眾多加利用。\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "洽詢專線：(07)3373451或(07)3314942\r\n",
        "\r\n",
        "高雄房地產億年旺網站（網址：http://eland.kcg.gov.tw/）\r\n",
        "\r\n",
        "高雄市政府地政局網站（網址：http://landp.kcg.gov.tw/）\r\n",
        "\r\n",
        "高雄市政府地政局土地開發處網站（網址：http://landevp.kcg.gov.tw/）　\r\n",
        "'''\r\n",
        "raw_text[-300:]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'及土地開發處網站查詢下載相關資料，在期限前完成投標，另再提醒投標人，本年度已更新投標單格式，投標大眾請注意應以新式投標單投標以免投標無效作廢。\\n\\n \\n\\n為配合防疫需求，本季開標作業除於地政局第一會議室辦理外，另將於地政局Facebook粉絲專頁同步直播，請大眾多加利用。\\n\\n \\n\\n洽詢專線：(07)3373451或(07)3314942\\n\\n高雄房地產億年旺網站（網址：http://eland.kcg.gov.tw/）\\n\\n高雄市政府地政局網站（網址：http://landp.kcg.gov.tw/）\\n\\n高雄市政府地政局土地開發處網站（網址：http://landevp.kcg.gov.tw/）\\u3000\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6kr39q0U5Qp"
      },
      "source": [
        "I find this lightweight library [`nlp2`](https://github.com/voidful/nlp2) quite handy for text cleaning. The `clean_all` function removes URL links, HTML elements, and unused tags. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg67jr9-pIwF"
      },
      "source": [
        "> Note: I want to give a shoutout to [Eric Lam](https://github.com/voidful), who created `nlp2` and other useful NLP tools such as `NLPrep`, `TFkit`, and `nlp2go`.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5BGDyNnekxw"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "!pip install nlp2\r\n",
        "from nlp2 import clean_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jedaxlh1XE7L"
      },
      "source": [
        "After cleaning, our sample text looks like this. Notice that all the URL links are gone now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "Bkiyp-FXF8sN",
        "outputId": "6ee851d2-4bff-40f5-c259-47ebe226ebf8"
      },
      "source": [
        "text = clean_all(raw_text)\r\n",
        "text[-300:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'合理。\\n\\n \\n\\n第4季土地標售作業於109年12月1日公告，投資大眾可前往地政局土地開發處土地處分科索取標售海報及標單，或直接上網高雄房地產億年旺網站、地政局及土地開發處網站查詢下載相關資料，在期限前完成投標，另再提醒投標人，本年度已更新投標單格式，投標大眾請注意應以新式投標單投標以免投標無效作廢。\\n\\n \\n\\n為配合防疫需求，本季開標作業除於地政局第一會議室辦理外，另將於地政局Facebook粉絲專頁同步直播，請大眾多加利用。\\n\\n \\n\\n洽詢專線： 3373451或 3314942\\n\\n高雄房地產億年旺網站（網址： ）\\n\\n高雄市政府地政局網站（網址： ）\\n\\n高雄市政府地政局土地開發處網站（網址： ）'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHwWC0wheQ11"
      },
      "source": [
        "## Install `spacy` and `ckip-transformers`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgMNggG8Ul-2"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "!pip install -U pip setuptools wheel\r\n",
        "!pip install -U spacy\r\n",
        "!python -m spacy download zh_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J84EbAeza7y3"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "!pip install -U ckip-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_k5F9nzi18C"
      },
      "source": [
        "## Tokenize texts with `ckip-transformers`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaj2xSJyXqow"
      },
      "source": [
        "Let's create a driver for word segmentation and one for parts of speech. CKIP Transformers also has a built-in driver for named entity recognition, i.e.  `CkipNerChunker`. But we won't use it here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23rTu70HZeh0"
      },
      "source": [
        "> Tip: By default, CPU is used. If you want to use GPU to speed up word segmentation, initialize `ws_driver` this way instead: `ws_driver = CkipWordSegmenter(device=-1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Pyph0Jbeos"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger\r\n",
        "ws_driver  = CkipWordSegmenter()\r\n",
        "pos_driver = CkipPosTagger()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj1ugf7EXZjU"
      },
      "source": [
        "> Important: Make sure that the input to `ws_driver()` is a list even if you're only dealing with a single text. Otherwise, words won't be properly segmented. Notice that the input to `pos_driver()` is the output of `ws_driver()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s0csUXSb9jr"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "ws  = ws_driver([text])\r\n",
        "pos = pos_driver(ws)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgOUvL-jbSWK"
      },
      "source": [
        "Here're the segmented tokens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "732V31K6cFVc",
        "outputId": "198d56e5-125d-416c-e8a6-62f1eb1b414c"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "tokens = ws[0]\r\n",
        "print(tokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['市府', '地政局', '109年度', '第4', '季', '開發區', '土地', '標售', '，', '共計', '推出', '8', '標', '9', '筆', '優質', '建地', '，', '訂', '於', '109年', '12月', '16日', '開標', '，', '合計', '總底價', '12 億', '4049萬', '6164 ', '元', '。', '\\n\\n \\n\\n', '第93', '期', '重劃區', '，', '原', '為', '國軍', '眷村', '，', '緊鄰', '國定', '古蹟', '-', '「', '原', '日本', '海軍', '鳳山', '無線', '電信所', '」', '，', '市府', '為', '保存', '古蹟', '同時', '活化', '眷村', '遷移', '後', '土地', '，', '以', '重劃', '方式', '整體', '開發', '，', '新', '闢', '住宅區', '、', '道路', '、', '公園', '及', '停車場', '，', '使', '本', '區', '具有', '歷史', '文化', '內涵', '與', '綠色', '休閒', '特色', '，', '生活', '機能', '更加', '健全', '。', '地政局', '首次', '推出', '1', '筆', '大', '面積', '土地', '，', '面積', '約', '2160', '坪', '，', '地形', '方整', '，', '雙面', '臨', '路', '，', '利於', '規劃', '興建', '景觀', '大樓', '，', '附近', '有', '市場', '、', '學校', '、', '公園', '及', '大東', '文化', '園區', '，', '距', '捷運', '大東站', '、', '鳳山', '國中站', '及', '鳳山', '火車站', '僅', '數', '分鐘', '車程', '，', '交通', '四通八達', '，', '因', '土地', '稀少性', '及', '區位', '條件', '絕佳', '，', '勢必', '成為', '投資人', '追逐', '焦點', '。', '\\n\\n \\n\\n', '第87', '期', '重劃區', '，', '位於', '省道', '台1線', '旁', '，', '鄰近', '捷運', '南', '岡山站', '，', '重劃', '後', '擁有', '完善', '的', '道路', '系統', '、', '公園', '綠地', '及', '毗鄰', '醒村', '懷舊', '文化', '景觀', '建築群', '，', '具備', '優質', '居住', '環境', '及', '交通', '便捷', '要件', '，', '地政局', '一', '推出', '土地', '標售', '，', '即', '掀起', '搶標', '熱潮', '，', '本', '季', '再', '釋出', '1', '筆', '面積', '約', '93', '坪', '土地', '，', '臨', '20', '米', '介壽路', '及', '鵬程東路', '，', '附近', '有', '岡山', '文化', '中心', '、', '兆湘', '國小', '、', '公13', '、', '公14', '、', '陽明', '公園', '及', '劉厝', '公園', '，', '區位', '條件', '佳', '，', '投資人', '準備', '搶進', '！', '\\n\\n \\n\\n', '第77', '期', '市地', '重劃區', '，', '位於', '鳳山區', '快速', '道路', '省道', '台88', '線', '旁', '，', '近', '中山高', '五甲', '系統', '交流道', '，', '近年', '推出', '土地', '標售', '皆', '順利', '完銷', '。', '本', '季', '再', '推出', '2', '筆', '土地', '，', '其中', '1', '筆', '面積', '約', '526', '坪', '，', '臨', '保華一路', '，', '適合', '商業', '使用', '；', '1', '筆', '面積', '107', '坪', '，', '位於', '代德三街', '，', '自用', '投資', '兩', '相宜', '。', '\\n\\n \\n\\n', '高雄', '大學', '區段', '徵收區', '，', '為', '北', '高雄', '優質', '文教', '特區', '，', '優質', '居住', '環境', '，', '吸引', '投資人', '進駐', '，', '本', '季', '再', '推出', '2', '標', '2', '筆', '土地', '，', '其中', '1', '筆', '第三', '種', '商業區', '土地', '，', '面積', '約', '639', '坪', '，', '位於', '大學', '26街', '，', '近', '高雄', '大學', '正門', '及', '萬', '坪', '藍田', '公園', '，', '地形', '方正', '，', '使用', '強度', '高', '，', '適合', '興建', '優質', '住宅', '大樓', '；', '另', '1', '筆', '住三', '用地', '，', '面積', '約', '379', '坪', '，', '臨', '28', '米', '藍昌路', '，', '近', '高雄', '大學', '及', '中山', '高中', '，', '交通', '便捷', '。', '\\n\\n \\n\\n', '另', '第37', '期', '重劃區', '及', '前', '大寮', '農地', '重劃區', '各', '推出', '1', '至', '2', '筆', '土地', '，', '價格', '合理', '。', '\\n\\n \\n\\n', '第4', '季', '土地', '標售', '作業', '於', '109年', '12月', '1日', '公告', '，', '投資', '大眾', '可', '前往', '地政局', '土地', '開發處', '土地處', '分科', '索取', '標售', '海報', '及', '標單', '，', '或', '直接', '上網', '高雄', '房地產', '億年旺', '網站', '、', '地政局', '及', '土地', '開發處', '網站', '查詢', '下載', '相關', '資料', '，', '在', '期限', '前', '完成', '投標', '，', '另', '再', '提醒', '投標人', '，', '本', '年度', '已', '更新', '投標單', '格式', '，', '投標', '大眾', '請', '注意', '應', '以', '新式', '投標單', '投標', '以免', '投標', '無效', '作廢', '。', '\\n\\n \\n\\n', '為', '配合', '防疫', '需求', '，', '本', '季', '開標', '作業', '除', '於', '地政局', '第一', '會議室', '辦理', '外', '，', '另', '將', '於', '地政局', 'Facebook', '粉絲', '專頁', '同步', '直播', '，', '請', '大眾', '多加', '利用', '。', '\\n\\n \\n\\n', '洽詢', '專線', '：', ' 3373', '451', '或', ' 3314942', '\\n', '\\n', '高雄', '房地產', '億', '年', '旺', '網站', '（', '網址', '：', ' ', '）', '\\n', '\\n', '高雄市', '政府', '地政局', '網站', '（', '網址', '：', ' ', '）', '\\n', '\\n', '高雄市', '政府', '地政局', '土地', '開發處', '網站', '（', '網址', '：', ' ', '）']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RuniUOtbtjF"
      },
      "source": [
        "By contrast, Jieba produced lots of wrongly segmented tokens, which is precisely why we prefer CKIP Transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTPVkp1bs8kU",
        "outputId": "32165270-419b-41d8-8561-fdc15d0aa701"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "import jieba\r\n",
        "print(list(jieba.cut(text)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.958 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['市府', '地', '政局', '109', '年度', '第', '4', '季開', '發區', '土地', '標售', '，', '共計', '推出', '8', '標', '9', '筆優質', '建地', '，', '訂', '於', '109', '年', '12', '月', '16', '日', '開標', '，', '合計', '總底價', '12', ' ', '億', '4049', '萬', '6164', ' ', '元', '。', '\\n', '\\n', ' ', '\\n', '\\n', '第', '93', '期重', '劃區', '，', '原為國', '軍', '眷村', '，', '緊鄰', '國定', '古', '蹟', '-', '「', '原', '日本海', '軍鳳山', '無線', '電信', '所', '」', '，', '市府', '為', '保存', '古', '蹟', '同時', '活化', '眷村', '遷移', '後', '土地', '，', '以', '重劃', '方式', '整體', '開發', '，', '新闢', '住宅', '區', '、', '道路', '、', '公園', '及', '停車場', '，', '使本區', '具有', '歷史', '文化', '內涵', '與', '綠色', '休閒', '特色', '，', '生活', '機能', '更加', '健全', '。', '地', '政局', '首次', '推出', '1', '筆大面積', '土地', '，', '面積', '約', '2160', '坪', '，', '地形', '方整', '，', '雙面', '臨路', '，', '利', '於', '規劃', '興建景', '觀大樓', '，', '附近', '有', '市場', '、', '學校', '、', '公園', '及', '大東', '文化', '園區', '，', '距捷', '運大東', '站', '、', '鳳山國', '中站', '及鳳', '山火', '車站', '僅數', '分鐘', '車程', '，', '交通', '四通', '八達', '，', '因', '土地', '稀少', '性及', '區位', '條件', '絕佳', '，', '勢必成', '為', '投資人', '追逐', '焦點', '。', '\\n', '\\n', ' ', '\\n', '\\n', '第', '87', '期重', '劃區', '，', '位', '於', '省', '道', '台', '1', '線旁', '，', '鄰近', '捷運', '南岡山', '站', '，', '重劃', '後', '擁有', '完善', '的', '道路', '系統', '、', '公園', '綠地', '及', '毗', '鄰醒', '村懷舊', '文化', '景觀', '建築群', '，', '具備', '優質', '居住', '環境', '及', '交通', '便捷', '要件', '，', '地', '政局', '一', '推出', '土地', '標售', '，', '即', '掀起', '搶標', '熱潮', '，', '本季', '再釋', '出', '1', '筆面', '積約', '93', '坪', '土地', '，', '臨', '20', '米', '介壽路', '及鵬程', '東路', '，', '附近', '有岡山', '文化', '中心', '、', '兆', '湘國', '小', '、', '公', '13', '、', '公', '14', '、', '陽明', '公園', '及', '劉厝公園', '，', '區位', '條件', '佳', '，', '投資人', '準備', '搶進', '！', '\\n', '\\n', ' ', '\\n', '\\n', '第', '77', '期市', '地', '重劃區', '，', '位', '於', '鳳山區', '快速道路', '省道', '台', '88', '線旁', '，', '近', '中山', '高', '五甲', '系統', '交流', '道', '，', '近年', '推出', '土地', '標售', '皆', '順利', '完銷', '。', '本季', '再', '推出', '2', '筆', '土地', '，', '其中', '1', '筆面', '積約', '526', '坪', '，', '臨保華', '一路', '，', '適合', '商業', '使用', '；', '1', '筆面積', '107', '坪', '，', '位', '於', '代德三街', '，', '自用', '投資', '兩', '相宜', '。', '\\n', '\\n', ' ', '\\n', '\\n', '高雄', '大學區', '段', '徵收', '區', '，', '為', '北高雄', '優質', '文教', '特區', '，', '優質', '居住', '環境', '，', '吸引', '投資人', '進駐', '，', '本季', '再', '推出', '2', '標', '2', '筆', '土地', '，', '其中', '1', '筆', '第三', '種商業區', '土地', '，', '面積', '約', '639', '坪', '，', '位', '於', '大學', '26', '街', '，', '近高雄', '大學', '正門', '及', '萬坪', '藍田公園', '，', '地形', '方正', '，', '使用', '強度', '高', '，', '適合', '興建', '優質', '住宅', '大樓', '；', '另', '1', '筆住', '三', '用地', '，', '面積', '約', '379', '坪', '，', '臨', '28', '米', '藍昌路', '，', '近高雄', '大學及', '中山', '高中', '，', '交通', '便捷', '。', '\\n', '\\n', ' ', '\\n', '\\n', '另', '第', '37', '期重', '劃區', '及', '前', '大', '寮', '農地', '重劃區', '各', '推出', '1', '至', '2', '筆', '土地', '，', '價格', '合理', '。', '\\n', '\\n', ' ', '\\n', '\\n', '第', '4', '季', '土地', '標售', '作業', '於', '109', '年', '12', '月', '1', '日', '公告', '，', '投資大眾', '可', '前往', '地', '政局', '土地', '開發處', '土地', '處', '分科', '索取', '標售', '海報', '及', '標單', '，', '或', '直接', '上網', '高雄房', '地產', '億年', '旺', '網站', '、', '地', '政局', '及', '土地', '開發處', '網站', '查詢', '下載', '相關', '資料', '，', '在', '期限', '前', '完成', '投標', '，', '另', '再', '提醒', '投標', '人', '，', '本年度', '已', '更新', '投標', '單', '格式', '，', '投標', '大眾', '請', '注意', '應以', '新式', '投標單', '投標', '以免', '投標', '無效作', '廢', '。', '\\n', '\\n', ' ', '\\n', '\\n', '為', '配合', '防疫', '需求', '，', '本季', '開標', '作業', '除', '於', '地', '政局', '第一', '會議室', '辦理外', '，', '另將', '於', '地', '政局', 'Facebook', '粉絲', '專頁', '同步', '直播', '，', '請大眾', '多加', '利用', '。', '\\n', '\\n', ' ', '\\n', '\\n', '洽詢', '專線', '：', ' ', '3373451', '或', ' ', '3314942', '\\n', '\\n', '高雄房', '地產', '億年', '旺', '網站', '（', '網址', '：', ' ', '）', '\\n', '\\n', '高雄市', '政府', '地', '政局', '網站', '（', '網址', '：', ' ', '）', '\\n', '\\n', '高雄市', '政府', '地', '政局', '土地', '開發處', '網站', '（', '網址', '：', ' ', '）']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMtQM3DOjLzx"
      },
      "source": [
        "## Feed tokenized results to `spacy` using `WhitespaceTokenizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVBOqboMGSGd"
      },
      "source": [
        "The [official website of spaCy](https://spacy.io/usage/linguistic-features#native-tokenizers) describes several ways of adding a custom tokenizer. The simplest is to define the  `WhitespaceTokenizer` class, which tokenizes a text on space characters. The output of tokenization can then be fed into subsequent operations down the pipeline, including `tagger` for parts-of-speech (POS) tagging, `parser` for dependency parsing, and `ner` for named entity recognition. This is possible primarily because `tokenizer` creates a `Doc` object whereas the other three steps operate on the `Doc` object, as illustrated in this graph. \n",
        "![](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yZt_vEqgCpQ"
      },
      "source": [
        "> Note: The original code for `words` is `words = text.split(\" \")`, but it caused an error to my text. So I revised it into `words = text.strip().split()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW2V7EvpXIak"
      },
      "source": [
        "from spacy.tokens import Doc\r\n",
        "\r\n",
        "class WhitespaceTokenizer:\r\n",
        "    def __init__(self, vocab):\r\n",
        "        self.vocab = vocab\r\n",
        "\r\n",
        "    def __call__(self, text):\r\n",
        "        words = text.strip().split()\r\n",
        "        return Doc(self.vocab, words=words)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEPButNuMo5A"
      },
      "source": [
        "Next, let's load the `zh_core_web_sm` model for Chinese, which we'll need for POS tagging. Then here comes the crucial part: `nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)`. This line of code sets the default tokenizer from Jieba to `WhitespaceTokenizer`, which we just defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRByRjfjWgnS"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load('zh_core_web_sm')\r\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOLly_E6jApR"
      },
      "source": [
        "Then we join the tokenized result from CKIP Transformers to a single string of space-seperated tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "IFFr8Jp1dzOm",
        "outputId": "9a544d68-9931-4908-a597-efc94510cede"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "token_str = \" \".join(tokens)\r\n",
        "token_str"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'市府 地政局 109年度 第4 季 開發區 土地 標售 ， 共計 推出 8 標 9 筆 優質 建地 ， 訂 於 109年 12月 16日 開標 ， 合計 總底價 12 億 4049萬 6164  元 。 \\n\\n \\n\\n 第93 期 重劃區 ， 原 為 國軍 眷村 ， 緊鄰 國定 古蹟 - 「 原 日本 海軍 鳳山 無線 電信所 」 ， 市府 為 保存 古蹟 同時 活化 眷村 遷移 後 土地 ， 以 重劃 方式 整體 開發 ， 新 闢 住宅區 、 道路 、 公園 及 停車場 ， 使 本 區 具有 歷史 文化 內涵 與 綠色 休閒 特色 ， 生活 機能 更加 健全 。 地政局 首次 推出 1 筆 大 面積 土地 ， 面積 約 2160 坪 ， 地形 方整 ， 雙面 臨 路 ， 利於 規劃 興建 景觀 大樓 ， 附近 有 市場 、 學校 、 公園 及 大東 文化 園區 ， 距 捷運 大東站 、 鳳山 國中站 及 鳳山 火車站 僅 數 分鐘 車程 ， 交通 四通八達 ， 因 土地 稀少性 及 區位 條件 絕佳 ， 勢必 成為 投資人 追逐 焦點 。 \\n\\n \\n\\n 第87 期 重劃區 ， 位於 省道 台1線 旁 ， 鄰近 捷運 南 岡山站 ， 重劃 後 擁有 完善 的 道路 系統 、 公園 綠地 及 毗鄰 醒村 懷舊 文化 景觀 建築群 ， 具備 優質 居住 環境 及 交通 便捷 要件 ， 地政局 一 推出 土地 標售 ， 即 掀起 搶標 熱潮 ， 本 季 再 釋出 1 筆 面積 約 93 坪 土地 ， 臨 20 米 介壽路 及 鵬程東路 ， 附近 有 岡山 文化 中心 、 兆湘 國小 、 公13 、 公14 、 陽明 公園 及 劉厝 公園 ， 區位 條件 佳 ， 投資人 準備 搶進 ！ \\n\\n \\n\\n 第77 期 市地 重劃區 ， 位於 鳳山區 快速 道路 省道 台88 線 旁 ， 近 中山高 五甲 系統 交流道 ， 近年 推出 土地 標售 皆 順利 完銷 。 本 季 再 推出 2 筆 土地 ， 其中 1 筆 面積 約 526 坪 ， 臨 保華一路 ， 適合 商業 使用 ； 1 筆 面積 107 坪 ， 位於 代德三街 ， 自用 投資 兩 相宜 。 \\n\\n \\n\\n 高雄 大學 區段 徵收區 ， 為 北 高雄 優質 文教 特區 ， 優質 居住 環境 ， 吸引 投資人 進駐 ， 本 季 再 推出 2 標 2 筆 土地 ， 其中 1 筆 第三 種 商業區 土地 ， 面積 約 639 坪 ， 位於 大學 26街 ， 近 高雄 大學 正門 及 萬 坪 藍田 公園 ， 地形 方正 ， 使用 強度 高 ， 適合 興建 優質 住宅 大樓 ； 另 1 筆 住三 用地 ， 面積 約 379 坪 ， 臨 28 米 藍昌路 ， 近 高雄 大學 及 中山 高中 ， 交通 便捷 。 \\n\\n \\n\\n 另 第37 期 重劃區 及 前 大寮 農地 重劃區 各 推出 1 至 2 筆 土地 ， 價格 合理 。 \\n\\n \\n\\n 第4 季 土地 標售 作業 於 109年 12月 1日 公告 ， 投資 大眾 可 前往 地政局 土地 開發處 土地處 分科 索取 標售 海報 及 標單 ， 或 直接 上網 高雄 房地產 億年旺 網站 、 地政局 及 土地 開發處 網站 查詢 下載 相關 資料 ， 在 期限 前 完成 投標 ， 另 再 提醒 投標人 ， 本 年度 已 更新 投標單 格式 ， 投標 大眾 請 注意 應 以 新式 投標單 投標 以免 投標 無效 作廢 。 \\n\\n \\n\\n 為 配合 防疫 需求 ， 本 季 開標 作業 除 於 地政局 第一 會議室 辦理 外 ， 另 將 於 地政局 Facebook 粉絲 專頁 同步 直播 ， 請 大眾 多加 利用 。 \\n\\n \\n\\n 洽詢 專線 ：  3373 451 或  3314942 \\n \\n 高雄 房地產 億 年 旺 網站 （ 網址 ：   ） \\n \\n 高雄市 政府 地政局 網站 （ 網址 ：   ） \\n \\n 高雄市 政府 地政局 土地 開發處 網站 （ 網址 ：   ）'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5HwztKUkI3N"
      },
      "source": [
        "Next, we feed `token_str`, our tokenized text, to `nlp` to create a spaCy `Doc` object. From this point on, we are able to leverage the power of spaCy. For every token in a `Doc` object, we have access to its text via the attribute `.text` and its parts-of-speech label via the attribute `.pos_`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAkdgw1SXMKp",
        "outputId": "5f91960f-0ad4-46b0-a12d-7df98d66f01c"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "doc = nlp(token_str)\r\n",
        "print([token.text for token in doc])\r\n",
        "print([token.pos_ for token in doc])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['市府', '地政局', '109年度', '第4', '季', '開發區', '土地', '標售', '，', '共計', '推出', '8', '標', '9', '筆', '優質', '建地', '，', '訂', '於', '109年', '12月', '16日', '開標', '，', '合計', '總底價', '12', '億', '4049萬', '6164', '元', '。', '第93', '期', '重劃區', '，', '原', '為', '國軍', '眷村', '，', '緊鄰', '國定', '古蹟', '-', '「', '原', '日本', '海軍', '鳳山', '無線', '電信所', '」', '，', '市府', '為', '保存', '古蹟', '同時', '活化', '眷村', '遷移', '後', '土地', '，', '以', '重劃', '方式', '整體', '開發', '，', '新', '闢', '住宅區', '、', '道路', '、', '公園', '及', '停車場', '，', '使', '本', '區', '具有', '歷史', '文化', '內涵', '與', '綠色', '休閒', '特色', '，', '生活', '機能', '更加', '健全', '。', '地政局', '首次', '推出', '1', '筆', '大', '面積', '土地', '，', '面積', '約', '2160', '坪', '，', '地形', '方整', '，', '雙面', '臨', '路', '，', '利於', '規劃', '興建', '景觀', '大樓', '，', '附近', '有', '市場', '、', '學校', '、', '公園', '及', '大東', '文化', '園區', '，', '距', '捷運', '大東站', '、', '鳳山', '國中站', '及', '鳳山', '火車站', '僅', '數', '分鐘', '車程', '，', '交通', '四通八達', '，', '因', '土地', '稀少性', '及', '區位', '條件', '絕佳', '，', '勢必', '成為', '投資人', '追逐', '焦點', '。', '第87', '期', '重劃區', '，', '位於', '省道', '台1線', '旁', '，', '鄰近', '捷運', '南', '岡山站', '，', '重劃', '後', '擁有', '完善', '的', '道路', '系統', '、', '公園', '綠地', '及', '毗鄰', '醒村', '懷舊', '文化', '景觀', '建築群', '，', '具備', '優質', '居住', '環境', '及', '交通', '便捷', '要件', '，', '地政局', '一', '推出', '土地', '標售', '，', '即', '掀起', '搶標', '熱潮', '，', '本', '季', '再', '釋出', '1', '筆', '面積', '約', '93', '坪', '土地', '，', '臨', '20', '米', '介壽路', '及', '鵬程東路', '，', '附近', '有', '岡山', '文化', '中心', '、', '兆湘', '國小', '、', '公13', '、', '公14', '、', '陽明', '公園', '及', '劉厝', '公園', '，', '區位', '條件', '佳', '，', '投資人', '準備', '搶進', '！', '第77', '期', '市地', '重劃區', '，', '位於', '鳳山區', '快速', '道路', '省道', '台88', '線', '旁', '，', '近', '中山高', '五甲', '系統', '交流道', '，', '近年', '推出', '土地', '標售', '皆', '順利', '完銷', '。', '本', '季', '再', '推出', '2', '筆', '土地', '，', '其中', '1', '筆', '面積', '約', '526', '坪', '，', '臨', '保華一路', '，', '適合', '商業', '使用', '；', '1', '筆', '面積', '107', '坪', '，', '位於', '代德三街', '，', '自用', '投資', '兩', '相宜', '。', '高雄', '大學', '區段', '徵收區', '，', '為', '北', '高雄', '優質', '文教', '特區', '，', '優質', '居住', '環境', '，', '吸引', '投資人', '進駐', '，', '本', '季', '再', '推出', '2', '標', '2', '筆', '土地', '，', '其中', '1', '筆', '第三', '種', '商業區', '土地', '，', '面積', '約', '639', '坪', '，', '位於', '大學', '26街', '，', '近', '高雄', '大學', '正門', '及', '萬', '坪', '藍田', '公園', '，', '地形', '方正', '，', '使用', '強度', '高', '，', '適合', '興建', '優質', '住宅', '大樓', '；', '另', '1', '筆', '住三', '用地', '，', '面積', '約', '379', '坪', '，', '臨', '28', '米', '藍昌路', '，', '近', '高雄', '大學', '及', '中山', '高中', '，', '交通', '便捷', '。', '另', '第37', '期', '重劃區', '及', '前', '大寮', '農地', '重劃區', '各', '推出', '1', '至', '2', '筆', '土地', '，', '價格', '合理', '。', '第4', '季', '土地', '標售', '作業', '於', '109年', '12月', '1日', '公告', '，', '投資', '大眾', '可', '前往', '地政局', '土地', '開發處', '土地處', '分科', '索取', '標售', '海報', '及', '標單', '，', '或', '直接', '上網', '高雄', '房地產', '億年旺', '網站', '、', '地政局', '及', '土地', '開發處', '網站', '查詢', '下載', '相關', '資料', '，', '在', '期限', '前', '完成', '投標', '，', '另', '再', '提醒', '投標人', '，', '本', '年度', '已', '更新', '投標單', '格式', '，', '投標', '大眾', '請', '注意', '應', '以', '新式', '投標單', '投標', '以免', '投標', '無效', '作廢', '。', '為', '配合', '防疫', '需求', '，', '本', '季', '開標', '作業', '除', '於', '地政局', '第一', '會議室', '辦理', '外', '，', '另', '將', '於', '地政局', 'Facebook', '粉絲', '專頁', '同步', '直播', '，', '請', '大眾', '多加', '利用', '。', '洽詢', '專線', '：', '3373', '451', '或', '3314942', '高雄', '房地產', '億', '年', '旺', '網站', '（', '網址', '：', '）', '高雄市', '政府', '地政局', '網站', '（', '網址', '：', '）', '高雄市', '政府', '地政局', '土地', '開發處', '網站', '（', '網址', '：', '）']\n",
            "['NOUN', 'NOUN', 'NUM', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'VERB', 'NUM', 'CCONJ', 'NUM', 'ADV', 'VERB', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'PUNCT', 'NOUN', 'NOUN', 'NUM', 'PROPN', 'NOUN', 'NUM', 'NUM', 'PUNCT', 'NUM', 'NUM', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'PROPN', 'PUNCT', 'PUNCT', 'ADJ', 'PROPN', 'NOUN', 'VERB', 'ADV', 'NOUN', 'PUNCT', 'PUNCT', 'NOUN', 'VERB', 'VERB', 'PROPN', 'VERB', 'VERB', 'NOUN', 'NOUN', 'PART', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'NOUN', 'ADV', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'PUNCT', 'VERB', 'DET', 'NUM', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADV', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'ADV', 'VERB', 'PUNCT', 'NOUN', 'ADV', 'VERB', 'NUM', 'NUM', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'NUM', 'NUM', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'NUM', 'NOUN', 'PUNCT', 'NOUN', 'PROPN', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'PROPN', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'NUM', 'PUNCT', 'ADP', 'NOUN', 'NOUN', 'CCONJ', 'NOUN', 'ADV', 'VERB', 'PUNCT', 'VERB', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'NUM', 'NUM', 'NOUN', 'PUNCT', 'ADJ', 'NOUN', 'NOUN', 'PART', 'PUNCT', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'PART', 'NOUN', 'VERB', 'PART', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'CCONJ', 'VERB', 'PROPN', 'NOUN', 'NOUN', 'PROPN', 'NOUN', 'PUNCT', 'VERB', 'PROPN', 'NOUN', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'ADV', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'ADJ', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADV', 'NOUN', 'NUM', 'NUM', 'VERB', 'NOUN', 'NUM', 'NUM', 'NOUN', 'PUNCT', 'VERB', 'NUM', 'NUM', 'NOUN', 'CCONJ', 'VERB', 'PUNCT', 'NOUN', 'VERB', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'VERB', 'PUNCT', 'VERB', 'VERB', 'NOUN', 'PUNCT', 'NUM', 'NUM', 'NOUN', 'NOUN', 'PUNCT', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PART', 'PUNCT', 'ADV', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'ADV', 'NOUN', 'VERB', 'PUNCT', 'DET', 'NOUN', 'ADV', 'VERB', 'NUM', 'NUM', 'NOUN', 'PUNCT', 'NOUN', 'NUM', 'NUM', 'VERB', 'NOUN', 'NUM', 'NUM', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'PROPN', 'NOUN', 'VERB', 'PUNCT', 'NUM', 'NUM', 'VERB', 'NUM', 'NUM', 'PUNCT', 'NOUN', 'VERB', 'PUNCT', 'VERB', 'VERB', 'NOUN', 'VERB', 'PUNCT', 'PROPN', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADV', 'VERB', 'NUM', 'ADJ', 'NUM', 'NUM', 'NOUN', 'PUNCT', 'NOUN', 'NUM', 'NOUN', 'NUM', 'CCONJ', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'NUM', 'NUM', 'PUNCT', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'ADJ', 'PROPN', 'NOUN', 'NOUN', 'CCONJ', 'NUM', 'NUM', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'PUNCT', 'VERB', 'NOUN', 'VERB', 'PUNCT', 'ADV', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'DET', 'NUM', 'NOUN', 'VERB', 'VERB', 'PUNCT', 'VERB', 'NOUN', 'NUM', 'NUM', 'PUNCT', 'VERB', 'NUM', 'NUM', 'NOUN', 'PUNCT', 'ADJ', 'PROPN', 'NOUN', 'CCONJ', 'PROPN', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'PUNCT', 'DET', 'NUM', 'NUM', 'NOUN', 'CCONJ', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'ADV', 'VERB', 'NUM', 'CCONJ', 'NUM', 'NUM', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'PUNCT', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'PUNCT', 'VERB', 'NOUN', 'VERB', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'CCONJ', 'NOUN', 'PUNCT', 'CCONJ', 'ADV', 'NOUN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'ADV', 'VERB', 'VERB', 'VERB', 'VERB', 'PUNCT', 'ADP', 'NOUN', 'PART', 'VERB', 'NOUN', 'PUNCT', 'ADV', 'ADV', 'VERB', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADV', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'PROPN', 'ADV', 'VERB', 'VERB', 'ADP', 'ADJ', 'NOUN', 'VERB', 'ADV', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'VERB', 'VERB', 'ADP', 'ADP', 'NOUN', 'NUM', 'NOUN', 'NOUN', 'PART', 'PUNCT', 'DET', 'ADV', 'VERB', 'NOUN', 'PROPN', 'PROPN', 'ADV', 'ADV', 'VERB', 'PUNCT', 'VERB', 'PROPN', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'NUM', 'NOUN', 'CCONJ', 'NUM', 'PROPN', 'NOUN', 'NUM', 'NUM', 'VERB', 'VERB', 'PUNCT', 'VERB', 'PUNCT', 'PUNCT', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'PUNCT', 'VERB', 'PUNCT', 'PUNCT', 'PROPN', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'PUNCT', 'VERB', 'PUNCT', 'PUNCT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22I89HVugBr"
      },
      "source": [
        "The POS tagging is made possible by the `zh_core_web_sm` model. Notice that spaCy uses coarse labels such as `NOUN` and `VERB`. By contrast, CKIP Transformers adopts a more fine-grained tagset, such as `Nc` for locative nouns and `Nd` for temporal nouns. Here're the POS labels for the same text produced by CKIP Transformers. We'll be using the spaCy's POS tagging to filter out words that we don't want in the candicate pool for keywords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvc-BFjvbKoG",
        "outputId": "4901c9e1-eeab-45de-fc77-e1fff566764e"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "pos_tags = pos[0]\r\n",
        "print(pos_tags)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Nc', 'Nc', 'Nd', 'Neu', 'Nd', 'Nc', 'Na', 'VC', 'COMMACATEGORY', 'VJ', 'VC', 'Neu', 'Nf', 'Neu', 'Nf', 'A', 'Na', 'COMMACATEGORY', 'VJ', 'P', 'Nd', 'Nd', 'Nd', 'VA', 'COMMACATEGORY', 'VG', 'Na', 'Neu', 'Neu', 'Neu', 'Nf', 'PERIODCATEGORY', 'WHITESPACE', 'Neu', 'Nf', 'Nc', 'COMMACATEGORY', 'D', 'VG', 'Na', 'Nc', 'COMMACATEGORY', 'VJ', 'A', 'Na', 'DASHCATEGORY', 'PARENTHESISCATEGORY', 'A', 'Nc', 'Nc', 'Nc', 'A', 'Nc', 'PARENTHESISCATEGORY', 'COMMACATEGORY', 'Nc', 'P', 'VC', 'Na', 'Nd', 'VHC', 'Nc', 'VC', 'Ng', 'Na', 'COMMACATEGORY', 'P', 'Nv', 'Na', 'Na', 'VC', 'COMMACATEGORY', 'VH', 'VC', 'Nc', 'PAUSECATEGORY', 'Na', 'PAUSECATEGORY', 'Nc', 'Caa', 'Nc', 'COMMACATEGORY', 'VL', 'Nes', 'Nc', 'VJ', 'Na', 'Na', 'Na', 'Caa', 'Na', 'Nv', 'Na', 'COMMACATEGORY', 'Na', 'Na', 'Dfa', 'VHC', 'PERIODCATEGORY', 'Nc', 'D', 'VC', 'Neu', 'Nf', 'VH', 'Na', 'Na', 'COMMACATEGORY', 'Na', 'Da', 'Neu', 'Nf', 'COMMACATEGORY', 'Na', 'VH', 'COMMACATEGORY', 'A', 'VCL', 'Na', 'COMMACATEGORY', 'VK', 'VC', 'VC', 'Na', 'Na', 'COMMACATEGORY', 'Nc', 'V_2', 'Nc', 'PAUSECATEGORY', 'Nc', 'PAUSECATEGORY', 'Nc', 'Caa', 'Nb', 'Na', 'Nc', 'COMMACATEGORY', 'P', 'Na', 'Nc', 'PAUSECATEGORY', 'Nc', 'Nc', 'Caa', 'Nc', 'Nc', 'Da', 'Neu', 'Nf', 'Na', 'COMMACATEGORY', 'Na', 'VH', 'COMMACATEGORY', 'Cbb', 'Na', 'Na', 'Caa', 'Na', 'Na', 'VH', 'COMMACATEGORY', 'D', 'VG', 'Na', 'VC', 'Na', 'PERIODCATEGORY', 'WHITESPACE', 'Neu', 'Nf', 'Nc', 'COMMACATEGORY', 'VCL', 'Nc', 'Nc', 'Ncd', 'COMMACATEGORY', 'VJ', 'Na', 'Nc', 'Nc', 'COMMACATEGORY', 'VC', 'Ng', 'VJ', 'VH', 'DE', 'Na', 'Na', 'PAUSECATEGORY', 'Nc', 'Na', 'Caa', 'VH', 'Nc', 'VH', 'Na', 'Na', 'Na', 'COMMACATEGORY', 'VJ', 'A', 'VA', 'Na', 'Caa', 'Na', 'VH', 'Na', 'COMMACATEGORY', 'Nc', 'D', 'VC', 'Na', 'Nv', 'COMMACATEGORY', 'D', 'VC', 'VD', 'Na', 'COMMACATEGORY', 'Nes', 'Nd', 'D', 'VC', 'Neu', 'Nf', 'Na', 'Da', 'Neu', 'Nf', 'Na', 'COMMACATEGORY', 'P', 'Neu', 'Nf', 'Nc', 'Caa', 'Nc', 'COMMACATEGORY', 'Nc', 'V_2', 'Nc', 'Na', 'Nc', 'PAUSECATEGORY', 'Nb', 'Nc', 'PAUSECATEGORY', 'Na', 'PAUSECATEGORY', 'Na', 'PAUSECATEGORY', 'Nb', 'Nc', 'Caa', 'Nc', 'Nc', 'COMMACATEGORY', 'Na', 'Na', 'VH', 'COMMACATEGORY', 'Na', 'VF', 'VA', 'EXCLAMATIONCATEGORY', 'WHITESPACE', 'Neu', 'Nf', 'Na', 'Nc', 'COMMACATEGORY', 'VCL', 'Nc', 'VH', 'Na', 'Nc', 'Nc', 'Nf', 'Ncd', 'COMMACATEGORY', 'VJ', 'Nc', 'Nc', 'Na', 'Na', 'COMMACATEGORY', 'Nd', 'VC', 'Na', 'Nv', 'D', 'VH', 'VC', 'PERIODCATEGORY', 'Nes', 'Nd', 'D', 'VC', 'Neu', 'Nf', 'Na', 'COMMACATEGORY', 'Nep', 'Neu', 'Nf', 'Na', 'Da', 'Neu', 'Nf', 'COMMACATEGORY', 'P', 'Nc', 'COMMACATEGORY', 'VH', 'Na', 'VC', 'SEMICOLONCATEGORY', 'Neu', 'Nf', 'Na', 'Neu', 'Nf', 'COMMACATEGORY', 'VCL', 'Nc', 'COMMACATEGORY', 'A', 'Na', 'Neu', 'VH', 'PERIODCATEGORY', 'WHITESPACE', 'Nc', 'Nc', 'Na', 'VC', 'COMMACATEGORY', 'VG', 'Ncd', 'Nc', 'A', 'Na', 'Nc', 'COMMACATEGORY', 'A', 'Nv', 'Na', 'COMMACATEGORY', 'VJ', 'Na', 'VCL', 'COMMACATEGORY', 'Nes', 'Nd', 'D', 'VC', 'Neu', 'Nf', 'Neu', 'Nf', 'Na', 'COMMACATEGORY', 'Nep', 'Neu', 'Nf', 'Neu', 'Nf', 'Nc', 'Na', 'COMMACATEGORY', 'Na', 'Da', 'Neu', 'Nf', 'COMMACATEGORY', 'VCL', 'Nc', 'Nc', 'COMMACATEGORY', 'VH', 'Nc', 'Nc', 'Na', 'Caa', 'Neu', 'Nf', 'Nb', 'Nc', 'COMMACATEGORY', 'Na', 'VH', 'COMMACATEGORY', 'Nv', 'Na', 'VH', 'COMMACATEGORY', 'VH', 'VC', 'A', 'Na', 'Na', 'SEMICOLONCATEGORY', 'Nes', 'Neu', 'Nf', 'VCL', 'Na', 'COMMACATEGORY', 'Na', 'Da', 'Neu', 'Nf', 'COMMACATEGORY', 'P', 'Neu', 'Nf', 'Nc', 'COMMACATEGORY', 'VH', 'Nc', 'Nc', 'Caa', 'Nb', 'Nc', 'COMMACATEGORY', 'Na', 'VH', 'PERIODCATEGORY', 'WHITESPACE', 'Cbb', 'Neu', 'Nf', 'Nc', 'Caa', 'Nes', 'Nc', 'Na', 'Nc', 'D', 'VC', 'Neu', 'Caa', 'Neu', 'Nf', 'Na', 'COMMACATEGORY', 'Na', 'VH', 'PERIODCATEGORY', 'WHITESPACE', 'Neu', 'Nd', 'Na', 'VC', 'Na', 'P', 'Nd', 'Nd', 'Nd', 'VE', 'COMMACATEGORY', 'VC', 'Nh', 'D', 'VCL', 'Nc', 'Na', 'Nv', 'Na', 'Nc', 'VD', 'Nv', 'Na', 'Caa', 'Na', 'COMMACATEGORY', 'Caa', 'VH', 'VA', 'Nc', 'Na', 'Nb', 'Nc', 'PAUSECATEGORY', 'Nc', 'Caa', 'Na', 'VC', 'Nc', 'VE', 'VC', 'VH', 'Na', 'COMMACATEGORY', 'P', 'Na', 'Ng', 'VC', 'VA', 'COMMACATEGORY', 'Cbb', 'D', 'VE', 'Na', 'COMMACATEGORY', 'Nes', 'Na', 'D', 'VC', 'Na', 'Na', 'COMMACATEGORY', 'VA', 'Nh', 'VF', 'VK', 'D', 'P', 'A', 'Na', 'VA', 'Cbb', 'VA', 'VH', 'VH', 'PERIODCATEGORY', 'WHITESPACE', 'P', 'VC', 'VA', 'Na', 'COMMACATEGORY', 'Nes', 'Nd', 'Nv', 'Na', 'P', 'P', 'Nc', 'Neu', 'Nc', 'VC', 'Ng', 'COMMACATEGORY', 'Cbb', 'D', 'P', 'Nc', 'FW', 'Na', 'Na', 'VH', 'D', 'COMMACATEGORY', 'VF', 'Nh', 'D', 'VC', 'PERIODCATEGORY', 'WHITESPACE', 'VE', 'Na', 'COLONCATEGORY', 'FW', 'Neu', 'Caa', 'FW', 'WHITESPACE', 'WHITESPACE', 'Nc', 'Na', 'Nb', 'Nf', 'VH', 'Nc', 'PARENTHESISCATEGORY', 'Na', 'COLONCATEGORY', 'WHITESPACE', 'PARENTHESISCATEGORY', 'WHITESPACE', 'WHITESPACE', 'Nc', 'Na', 'Nc', 'Nc', 'PARENTHESISCATEGORY', 'Na', 'COLONCATEGORY', 'WHITESPACE', 'PARENTHESISCATEGORY', 'WHITESPACE', 'WHITESPACE', 'Nc', 'Na', 'Nc', 'Na', 'VC', 'Nc', 'PARENTHESISCATEGORY', 'Na', 'COLONCATEGORY', 'WHITESPACE', 'PARENTHESISCATEGORY']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk2FIM5zHofx"
      },
      "source": [
        "## Convert stopwords in `spaCy` from simplified to Taiwanese traditional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asuQfl2W0CYa"
      },
      "source": [
        "spaCy comes with a built-in set of stopwords (basically words that we'd like to ignore), accessible via `spacy.lang.zh.stop_words`. To make good use of it, let's convert all the words from simplified characters to traditional ones with the help of `OpenCC`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAKtWVe1H16I"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "!pip install OpenCC\r\n",
        "import opencc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvFoOfjN1ubH"
      },
      "source": [
        "`OpenCC` does not just convert characters mechanically. It has the ability to convert words from simplified characters to their equivalent phrasing in Taiwan Mandarin, which is done by `s2twp.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5flQLF_lH3LC",
        "outputId": "f6bca457-30ab-4431-a299-1ec4ee16b390"
      },
      "source": [
        "from spacy.lang.zh.stop_words import STOP_WORDS\r\n",
        "converter = opencc.OpenCC('s2twp.json')\r\n",
        "spacy_stopwords_sim = list(STOP_WORDS)\r\n",
        "print(spacy_stopwords_sim[:5])\r\n",
        "spacy_stopwords_tra = [converter.convert(w) for w in spacy_stopwords_sim]\r\n",
        "print(spacy_stopwords_tra[:5])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['因为', '奇', '嘿嘿', '其次', '偏偏']\n",
            "['因為', '奇', '嘿嘿', '其次', '偏偏']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKM9o5DrkWs8"
      },
      "source": [
        "## Define a class for implementing TextRank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4upuIcLkdB-"
      },
      "source": [
        "If you're dealing with English texts, you can implement TextRank quite easily with [`textaCy`](https://github.com/chartbeat-labs/textacy), the tagline of which is `NLP, before and after spaCy`. But I couldn't get it to work for Chinese texts, so I had to implement TextRank from scratch. Luckily, I got a jump-start from this [gist](https://gist.github.com/BrambleXu/3d47bbdbd1ee4e6fc695b0ddb88cbf99), which offers a blueprint for the following definitions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI9kqA3UkaqK"
      },
      "source": [
        "#collapse\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class TextRank4Keyword():\r\n",
        "    \"\"\"Extract keywords from text\"\"\"\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        self.d = 0.85 # damping coefficient, usually is .85\r\n",
        "        self.min_diff = 1e-5 # convergence threshold\r\n",
        "        self.steps = 10 # iteration steps\r\n",
        "        self.node_weight = None # save keywords and its weight\r\n",
        "\r\n",
        "    def set_stopwords(self, custom_stopwords):  \r\n",
        "        \"\"\"Set stop words\"\"\"\r\n",
        "        for word in set(spacy_stopwords_tra).union(set(custom_stopwords)):\r\n",
        "            lexeme = nlp.vocab[word]\r\n",
        "            lexeme.is_stop = True\r\n",
        "    \r\n",
        "    def sentence_segment(self, doc, candidate_pos, lower):\r\n",
        "        \"\"\"Store those words only in cadidate_pos\"\"\"\r\n",
        "        sentences = []\r\n",
        "        for sent in doc.sents:\r\n",
        "            selected_words = []\r\n",
        "            for token in sent:\r\n",
        "                # Store words only with cadidate POS tag\r\n",
        "                if token.pos_ in candidate_pos and token.is_stop is False:\r\n",
        "                    if lower is True:\r\n",
        "                        selected_words.append(token.text.lower())\r\n",
        "                    else:\r\n",
        "                        selected_words.append(token.text)\r\n",
        "            sentences.append(selected_words)\r\n",
        "        return sentences\r\n",
        "        \r\n",
        "    def get_vocab(self, sentences):\r\n",
        "        \"\"\"Get all tokens\"\"\"\r\n",
        "        vocab = OrderedDict()\r\n",
        "        i = 0\r\n",
        "        for sentence in sentences:\r\n",
        "            for word in sentence:\r\n",
        "                if word not in vocab:\r\n",
        "                    vocab[word] = i\r\n",
        "                    i += 1\r\n",
        "        return vocab\r\n",
        "    \r\n",
        "    def get_token_pairs(self, window_size, sentences):\r\n",
        "        \"\"\"Build token_pairs from windows in sentences\"\"\"\r\n",
        "        token_pairs = list()\r\n",
        "        for sentence in sentences:\r\n",
        "            for i, word in enumerate(sentence):\r\n",
        "                for j in range(i+1, i+window_size):\r\n",
        "                    if j >= len(sentence):\r\n",
        "                        break\r\n",
        "                    pair = (word, sentence[j])\r\n",
        "                    if pair not in token_pairs:\r\n",
        "                        token_pairs.append(pair)\r\n",
        "        return token_pairs\r\n",
        "        \r\n",
        "    def symmetrize(self, a):\r\n",
        "        return a + a.T - np.diag(a.diagonal())\r\n",
        "    \r\n",
        "    def get_matrix(self, vocab, token_pairs):\r\n",
        "        \"\"\"Get normalized matrix\"\"\"\r\n",
        "        # Build matrix\r\n",
        "        vocab_size = len(vocab)\r\n",
        "        g = np.zeros((vocab_size, vocab_size), dtype='float')\r\n",
        "        for word1, word2 in token_pairs:\r\n",
        "            i, j = vocab[word1], vocab[word2]\r\n",
        "            g[i][j] = 1\r\n",
        "            \r\n",
        "        # Get Symmeric matrix\r\n",
        "        g = self.symmetrize(g)\r\n",
        "        \r\n",
        "        # Normalize matrix by column\r\n",
        "        norm = np.sum(g, axis=0)\r\n",
        "        g_norm = np.divide(g, norm, where=norm!=0) # this is to ignore the 0 element in norm\r\n",
        "        \r\n",
        "        return g_norm\r\n",
        "    \r\n",
        "    # I revised this function to return keywords as a list\r\n",
        "    def get_keywords(self, number=10):\r\n",
        "        \"\"\"Print top number keywords\"\"\"\r\n",
        "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\r\n",
        "        keywords = []\r\n",
        "        for i, (key, value) in enumerate(node_weight.items()):\r\n",
        "            keywords.append(key)\r\n",
        "            if i > number:\r\n",
        "                break\r\n",
        "        return keywords\r\n",
        "\r\n",
        "    def analyze(self, text, \r\n",
        "                candidate_pos=['NOUN', 'VERB'], \r\n",
        "                window_size=5, lower=False, stopwords=list()):\r\n",
        "        \"\"\"Main function to analyze text\"\"\"\r\n",
        "        \r\n",
        "        # Set stop words\r\n",
        "        self.set_stopwords(stopwords)\r\n",
        "\r\n",
        "        # Pare text with spaCy\r\n",
        "        doc = nlp(token_str)\r\n",
        "        \r\n",
        "        # Filter sentences\r\n",
        "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\r\n",
        "        \r\n",
        "        # Build vocabulary\r\n",
        "        vocab = self.get_vocab(sentences)\r\n",
        "        \r\n",
        "        # Get token_pairs from windows\r\n",
        "        token_pairs = self.get_token_pairs(window_size, sentences)\r\n",
        "        \r\n",
        "        # Get normalized matrix\r\n",
        "        g = self.get_matrix(vocab, token_pairs)\r\n",
        "        \r\n",
        "        # Initionlization for weight(pagerank value)\r\n",
        "        pr = np.array([1] * len(vocab))\r\n",
        "        \r\n",
        "        # Iteration\r\n",
        "        previous_pr = 0\r\n",
        "        for epoch in range(self.steps):\r\n",
        "            pr = (1-self.d) + self.d * np.dot(g, pr)\r\n",
        "            if abs(previous_pr - sum(pr))  < self.min_diff:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                previous_pr = sum(pr)\r\n",
        "\r\n",
        "        # Get weight for each node\r\n",
        "        node_weight = dict()\r\n",
        "        for word, index in vocab.items():\r\n",
        "            node_weight[word] = pr[index]\r\n",
        "        \r\n",
        "        self.node_weight = node_weight"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1NkLmca5XjS"
      },
      "source": [
        "Now we can create an instace of the `TextRank4Keyword` class and call the `set_stopwords` function with our `CUSTOM_STOPWORDS` variable. This created a set of stopwords resulting from the union of both our custom stopwords and spaCy's built-in stopwords. And only words that meet these two criteria would become candidates for keywords:\n",
        "\n",
        "*   they are **not** in the set of stopwords; \n",
        "*   their POS labels are one of those listed in `candidate_pos`, which includes `NOUN` and `VERB` by default. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIOYN7N5iam-"
      },
      "source": [
        "tr4w = TextRank4Keyword()\r\n",
        "tr4w.set_stopwords(CUSTOM_STOPWORDS)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ptje0S_MLql"
      },
      "source": [
        "## Put it together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xKg1LDv7BAf"
      },
      "source": [
        "Let's put it all together by defining a main function for keyword extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciX3x9rTHPJj"
      },
      "source": [
        "def extract_keys_from_str(raw_text):\r\n",
        "  text = clean_all(raw_text) #clean the raw text\r\n",
        "  ws  = ws_driver([text]) #tokenize the text with CKIP Transformers\r\n",
        "  tokenized_text = \" \".join(ws[0]) #join a list into a string \r\n",
        "  tr4w.analyze(tokenized_text) #create a spaCy Doc object with the string and calculate weights for words\r\n",
        "  keys = tr4w.get_keywords(KW_NUM) #get top 10 keywords, as set by the KW_NUM variable\r\n",
        "  return keys"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JRsh7nk8PW4"
      },
      "source": [
        "Here're the top ten keywords for our sample text. The results are quite satisfactory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so2L7h8sM0ru",
        "outputId": "35a4957e-9507-444d-e7ec-3f69307b6b4b"
      },
      "source": [
        "keys = extract_keys_from_str(raw_text)\r\n",
        "keys = [k for k in keys if len(k) > 1]\r\n",
        "keys"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 221.73it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:05<00:00,  5.20s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['土地', '公園', '地政局', '文化', '推出', '面積', '標售', '道路', '優質', '投標']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7vHGTqI2cLU"
      },
      "source": [
        "As a comparison, here're the top 10 keywords produced by Jieba's implementation of TextRank, 7 of which are identical to the list above. Although extracting keywords with Jieba is quick and easy, it tends to give rise to wrongly segmented tokens, such as `政局` in this example, which should have been `地政局` for Land Administration Bureau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJFSdtzmoDo5",
        "outputId": "ef377874-28b0-4cfe-daf9-df43be089e88"
      },
      "source": [
        "import jieba.analyse as KE\r\n",
        "jieba_kw = KE.textrank(text, topK=10)\r\n",
        "jieba_kw"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['土地', '政局', '投標', '公園', '投資', '標售', '文化', '開發', '優質', '推出']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNseiLamw9UV"
      },
      "source": [
        "# Other libraries that failed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bpS51styO-Y"
      },
      "source": [
        "## [`textaCy`](https://github.com/chartbeat-labs/textacy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGEJABjaxHT3"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "!pip install textacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE52HLUv_dLh"
      },
      "source": [
        "With textaCy, you can load a spaCy language model and then create a spaCy `Doc` object using that model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "-Qe8CAqC0JTJ",
        "outputId": "d43753b9-e961-4cd5-9518-da50339d51df"
      },
      "source": [
        "import textacy\r\n",
        "zh = textacy.load_spacy_lang(\"zh_core_web_sm\")\r\n",
        "doc = textacy.make_spacy_doc(text, lang=zh)\r\n",
        "doc._.preview"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Doc(612 tokens: \"市府地政局109年度第4季開發區土地標售，共計推出8標9筆優質建地，訂於109年12月16日開...\")'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9nN-Ojp_ygO"
      },
      "source": [
        "textaCy implements four algorithms for keyword extraction, including TextRank. But I got useless results by calling the  `textacy.ke.textrank` function with `doc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIEz-3F6112z",
        "outputId": "19ca6bb7-b3b5-49e0-bf8b-737faeae9c71"
      },
      "source": [
        "import textacy.ke as ke\n",
        "ke.textrank(doc)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('     ', 6.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIiYf_0Hy4MS"
      },
      "source": [
        "## [`pyate`](https://pypi.org/project/pyate/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtk4i9BZxtW9"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "!pip install pyate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDralSWwLW43"
      },
      "source": [
        "`pyate` has a built-in `TermExtractionPipeline` class for extracting keywords, which can be added to spaCy's pipeline. But it didn't work and this error message showed up: `TypeError: load() got an unexpected keyword argument 'parser'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qqzJXVYNx0E1",
        "outputId": "bf7d4169-73b5-47d5-b2a4-b207b562ac7d"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "from pyate.term_extraction_pipeline import TermExtractionPipeline\r\n",
        "nlp.add_pipe(TermExtractionPipeline())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f5a8398fbc3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#collapse-output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_extraction_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTermExtractionPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTermExtractionPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mterm_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTermExtraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_term_extraction_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbasic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcombo_basic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombo_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcvalues\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mterm_extractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mterm_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyate/term_extraction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTermExtraction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# TODO: find some way to prevent redundant loading of csv files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyate/term_extraction.py\u001b[0m in \u001b[0;36mTermExtraction\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTermExtraction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# TODO: find some way to prevent redundant loading of csv files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load() got an unexpected keyword argument 'parser'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XQh-jUGODoU"
      },
      "source": [
        "I found on the documentation page that `pyate` only supports English and Italian, which may account for the error I got.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ry9ujEzGBP"
      },
      "source": [
        "## [`pytextrank`](https://pypi.org/project/pytextrank/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx9gy_ioyjDW"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "!pip install pytextrank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpGtDNAl2TSu"
      },
      "source": [
        "To add TextRank to the spaCy pipeline, I followed the [instructions](https://spacy.io/universe/project/spacy-pytextrank) found on spaCy's documentation. But an error popped up. Luckily, `ValueError` offers possible ways to fix the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "XB3ECu88zUic",
        "outputId": "450d07cd-46f4-4a29-ba15-93acbaa98a8d"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "import pytextrank\r\n",
        "tr = pytextrank.TextRank()\r\n",
        "nlp.add_pipe(tr.PipelineComponent, name='textrank', last=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cd319957f3b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytextrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytextrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipelineComponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'textrank'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0mcleanup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mn_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <bound method TextRank.PipelineComponent of <pytextrank.pytextrank.TextRank object at 0x7f4fea403550>> (name: 'textrank').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWfUMtKDFwwX"
      },
      "source": [
        "So I used the `@Language.factory` decorator to define a TextRank component, and then called the `nlp.add_pipe` function with `textrank`. But this didn't work either. The error message reads: `'Chinese' object has no attribute 'sents'`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHaNf_IJ0HUp"
      },
      "source": [
        "from spacy.language import Language\r\n",
        "\r\n",
        "tr = pytextrank.TextRank()\r\n",
        "\r\n",
        "@Language.factory(\"textrank\")\r\n",
        "def create_textrank_component(nlp: Language, name: str):\r\n",
        "    return tr.PipelineComponent(nlp)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "3Sovw6WW2Ee9",
        "outputId": "35a78ada-cf42-440c-ab8c-f4fa4b2f551f"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "nlp.add_pipe('textrank')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1c84bbe50472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#collapse-output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'textrank'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_python2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mn_process\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW023\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m             \u001b[0mn_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_threads\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mlink_vectors_to_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pretrained_vectors\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_default_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(cls, config, schema, overrides, validate)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m_make\u001b[0;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m_fill\u001b[0;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-fb02aff6bab9>\u001b[0m in \u001b[0;36mcreate_textrank_component\u001b[0;34m(nlp, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"textrank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_textrank_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipelineComponent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytextrank/pytextrank.py\u001b[0m in \u001b[0;36mPipelineComponent\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"phrases\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"textrank\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_textrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytextrank/pytextrank.py\u001b[0m in \u001b[0;36mcalc_textrank\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Chinese' object has no attribute 'sents'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ5N4tsg2NzQ"
      },
      "source": [
        "## [`rake-spacy`](https://pypi.org/project/rake-spacy/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO-y_Ak1H38v"
      },
      "source": [
        "I couldn't even install `rake-spacy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHZGxvRW2nVP",
        "outputId": "c68dc644-e36f-44e8-f068-e0c735e4fb21"
      },
      "source": [
        "!pip install rake-spacy"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement rake-spacy\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for rake-spacy\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMMHdr9hGaks"
      },
      "source": [
        "## [`rake-keyword`](https://pypi.org/project/rake-keyword/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEjLsRb1Gfy0"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "!pip install rake-keyword"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0QSlrvMIIop"
      },
      "source": [
        "According to the [documentation on PYPI](https://pypi.org/project/rake-keyword/), the import is done by `from rake import Rake`, but it didn't work.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "lmI8z7sVG-l0",
        "outputId": "c4ea8888-b90a-4f38-c3e9-222c2fe4b1e1"
      },
      "source": [
        "#collapse-output\r\n",
        "\r\n",
        "from rake import Rake"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-fe043f886018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#collapse-output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrake\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Rake'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tODRWxUSMPB"
      },
      "source": [
        "However, based on the [documentation on GitHub](https://github.com/u-prashant/RAKE), this is done by `from rake import RAKE` instead. But it didn't work either.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "m0oRI2b4RDcA",
        "outputId": "e3daa359-5d9e-4290-ac0a-fdc5ac78cb76"
      },
      "source": [
        "#collapse-output\n",
        "\n",
        "from rake import RAKE"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-59e63adf01ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#collapse-output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrake\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAKE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'RAKE'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9nbvNQB0odE"
      },
      "source": [
        "# Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtTEEeG40qp4"
      },
      "source": [
        "Integration of CKIP Transformers with spaCy and the TextRank algorithm generates decent results for extracting keywords from texts in traditional Chinese. Although there are many Python libraries out there that implement TextRank, none of them works better than the  `TextRank4Keyword` class crafted from scratch. Until I figure out how to properly add the TextRank component to the spaCy pipeline, I'll stick with my working pipeline shown here. As a final thought, spaCy recently released v3.0, which supports pretrained transformer models. I can't wait to give it a try and see how this would change the workflow of extracting keywords or other NLP tasks. But that'll have to wait until next post. "
      ]
    }
  ]
}