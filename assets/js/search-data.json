{
  
    
        "post0": {
            "title": "Many ways to segment Chinese",
            "content": "Intro . Unlike English, Chinese does not use spaces in its writing system, which can be a pain in the neck (or in the eyes, for that matter) if you&#39;re learning to read Chinese. In a way, it&#39;s like trying to make sense out of long German words like Lebensabschnittspartner, which roughly means &quot;the person I&#39;m with today&quot; (taken from David Sedaris&#39;s language lessons published on the New Yorker). We&#39;ll see how computer models can help us with breaking a stretch of Chinese text into words (called tokenization in NLP jargon). To give computer models a hard time, we&#39;ll test out this text without punctuations. . text = &quot;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&quot; . This text is challenging not only because it can be segmented multiple ways but also because it could potentially express quite different meanings depending on how you interprete it. For instance, this part 今年好煩惱少不得打官司 could either mean &quot;This year will be great for you. You&#39;ll have few worries. Don&#39;t file any lawsuit&quot; or &quot;This year, you&#39;ll be very worried. A lawsuit is inevitable&quot;. Either way, it sounds like the kind of aphorism you&#39;d find in fortune cookies. Now that you know the secret to aphorisms being always right is ambiguity, we&#39;ll turn to five Python libraries for doing the hard work for us. . Jieba . Of the five tools to be introduced here, Jieba is perhaps the most widely used one, and it&#39;s even pre-installed on Colab and supported by spaCy. Unfortunately, Jieba told us that a lawsuit is inevitable this year... 😭 . import jieba tokens = jieba.cut(text) jieba_default = &quot; | &quot;.join(tokens) print(jieba_default) . . Building prefix dict from the default dictionary ... Dumping model to file cache /tmp/jieba.cache Loading model cost 0.741 seconds. Prefix dict has been built successfully. . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸養 | 牛 | 隻 | 隻 | 大如山 | 老鼠 | 隻 | 隻 | 死 . The result is quite satisfying, except for 酸養, which is not even a word. Jieba is famouse for being super fast. If we run the segmentation function 1000000 times, top results we got are 256 nanoseconds per loop! . %timeit jieba.cut(text) . . The slowest run took 12.90 times longer than the fastest. This could mean that an intermediate result is being cached. 1000000 loops, best of 3: 256 ns per loop . Let&#39;s write a function for later use. . def Jieba_tokenizer(text): tokens = jieba.cut(text) result = &quot; | &quot;.join(tokens) return result . PKUSeg . As its name suggests, PKUSeg is built by the Language Computing and Machine Learning Group at Peking (aka. Beijing) University. It&#39;s been recently integrated into spaCy. . !pip install -U pkuseg . Collecting pkuseg Downloading https://files.pythonhosted.org/packages/ed/68/2dfaa18f86df4cf38a90ef024e18b36d06603ebc992a2dcc16f83b00b80d/pkuseg-0.0.25-cp36-cp36m-manylinux1_x86_64.whl (50.2MB) |████████████████████████████████| 50.2MB 66kB/s Requirement already satisfied, skipping upgrade: numpy&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from pkuseg) (1.19.5) Requirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.6/dist-packages (from pkuseg) (0.29.21) Installing collected packages: pkuseg Successfully installed pkuseg-0.0.25 . . Here&#39;s the result. . import pkuseg pku = pkuseg.pkuseg() result = pku.cut(text) result = &quot; | &quot;.join(result) result . . &#39;今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀 | 酒剛 | 剛 | 好 | 做 | 醋 | 格外 | 酸養 | 牛 | 隻隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死&#39; . Compared with Jieba, PKUSeg not only got more wrong tokens (酸養 and 酒剛) but also ran at a much slower speed. . %timeit pku.cut(text) . . 1000 loops, best of 3: 648 µs per loop . Yet, PKUSeg has one nice feature absent from Jieba. . Users have the option to choose from four domain-specific models, including news, web, medicine, and tourism. . This can be quite helpful if you&#39;re specifically dealing with texts in any of the four domains. Let&#39;s test the news domain with the first paragraph of a news article about Covid-19 published on Yahoo News. . article = &#39;&#39;&#39; 台灣新冠肺炎連續第6天零本土病例破功！中央流行疫情指揮中心指揮官陳時中今天宣布國內新增4例本土確定病例，均為桃園醫院感染事件之確診個案相關接觸者，其中3例為案863之同住家人(案907、909、910)，研判與案863、864、865為一起家庭群聚案，其中1人（案907）死亡，是相隔8個月以來再添死亡病例；另1例為案889之就醫相關接觸者(案908)。此外，今天也新增6例境外移入確定病例，分別自印尼(案901)、捷克(案902)及巴西(案903至906)入境。衛福部桃園醫院感染累計達19例(其中1人死亡)，全台達909例、8死。 &#39;&#39;&#39; . Here&#39;s the result with the default settinng. . pku = pkuseg.pkuseg() result = pku.cut(article) result = &quot; | &quot;.join(result) result . . &#39;台灣 | 新冠 | 肺炎 | 連續 | 第6 | 天 | 零 | 本土 | 病例 | 破功 | ！ | 中央 | 流行 | 疫情 | 指揮 | 中心 | 指揮官 | 陳時 | 中 | 今天 | 宣布 | 國內 | 新增 | 4 | 例 | 本土 | 確定 | 病例 | ， | 均 | 為 | 桃園 | 醫院 | 感染 | 事件 | 之 | 確 | 診個案 | 相關 | 接觸者 | ， | 其中 | 3 | 例 | 為案 | 863 | 之 | 同 | 住家人 | ( | 案 | 907 | 、 | 909 | 、 | 910 | ) | ， | 研判 | 與案 | 863 | 、 | 864 | 、 | 865 | 為 | 一起 | 家庭 | 群聚案 | ， | 其中 | 1 | 人 | （ | 案 | 907 | ） | 死亡 | ， | 是 | 相隔 | 8 | 個 | 月 | 以 | 來 | 再 | 添 | 死亡 | 病例 | ； | 另 | 1 | 例 | 為案 | 889 | 之 | 就 | 醫 | 相關 | 接觸者 | ( | 案 | 908 | ) | 。 | 此外 | ， | 今天 | 也 | 新增 | 6例 | 境外 | 移入 | 確定 | 病例 | ， | 分別 | 自 | 印尼 | ( | 案 | 901 | ) | 、 | 捷克 | ( | 案 | 902 | ) | 及 | 巴西 | ( | 案 | 903 | 至 | 906 | ) | 入境 | 。 | 衛福部 | 桃園 | 醫院 | 感染 | 累計 | 達 | 19 | 例 | ( | 其中 | 1 | 人 | 死亡 | ) | ， | 全 | 台 | 達 | 909 | 例 | 、 | 8 | 死 | 。&#39; . Here&#39;s the result with the model_name argument set to news. Both models made some mistakes here and there, but what&#39;s surprising to me is that the news-specific model even made a mistake when parsing 新冠肺炎, which literally means &quot;new coronavirus disease&quot; and refers to Covid-19. . pku = pkuseg.pkuseg(model_name=&#39;news&#39;) result = pku.cut(article) result = &quot; | &quot;.join(result) result . . Downloading: &#34;https://github.com/lancopku/pkuseg-python/releases/download/v0.0.16/news.zip&#34; to /root/.pkuseg/news.zip 100%|██████████| 43767759/43767759 [00:00&lt;00:00, 104004889.71it/s] . &#39;台灣 | 新 | 冠 | 肺 | 炎連 | 續 | 第6天 | 零本土 | 病例 | 破功 | ！ | 中央 | 流行疫情指揮中心 | 指揮 | 官 | 陳 | 時 | 中 | 今天 | 宣布 | 國內 | 新增 | 4例 | 本土 | 確定 | 病例 | ， | 均 | 為桃園醫院 | 感染 | 事件 | 之 | 確 | 診 | 個 | 案 | 相關 | 接觸 | 者 | ， | 其中 | 3例 | 為案 | 863 | 之 | 同 | 住 | 家人 | (案 | 907 | 、 | 909 | 、 | 910) | ， | 研判 | 與案 | 863 | 、 | 864 | 、 | 865為 | 一起 | 家庭 | 群 | 聚案 | ， | 其中 | 1 | 人 | （ | 案 | 907 | ） | 死亡 | ， | 是 | 相隔 | 8個月 | 以 | 來 | 再 | 添 | 死亡 | 病例 | ； | 另 | 1例 | 為案 | 889 | 之 | 就 | 醫 | 相關 | 接觸 | 者 | (案 | 908) | 。 | 此外 | ， | 今天 | 也 | 新增 | 6例 | 境外 | 移入 | 確定 | 病例 | ， | 分 | 別 | 自 | 印尼 | (案 | 901) | 、 | 捷克 | (案 | 902) | 及 | 巴西 | (案 | 903至906 | ) | 入境 | 。 | 衛 | 福部桃園醫院 | 感染 | 累 | 計達 | 19例 | ( | 其中 | 1 | 人 | 死亡 | ) | ， | 全 | 台 | 達 | 909例 | 、 | 8 | 死 | 。&#39; . Let&#39;s write a function for later use. . def PKU_tokenizer(text): pku = pkuseg.pkuseg() tokens = pku.cut(text) result = &quot; | &quot;.join(tokens) return result . PyHanLP . Next, we&#39;ll try PyHanLP. It&#39;ll take some time to download the model and data files (about 640MB in total). . !pip install pyhanlp . Collecting pyhanlp Downloading https://files.pythonhosted.org/packages/8f/99/13078d71bc9f77705a29f932359046abac3001335ea1d21e91120b200b21/pyhanlp-0.1.66.tar.gz (86kB) |████████████████████████████████| 92kB 9.0MB/s Collecting jpype1==0.7.0 Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB) |████████████████████████████████| 2.7MB 12.4MB/s Building wheels for collected packages: pyhanlp Building wheel for pyhanlp (setup.py) ... done Created wheel for pyhanlp: filename=pyhanlp-0.1.66-py2.py3-none-any.whl size=29371 sha256=cbe214d3e71b3e4e5692c0570e6eadbafc6845b99409abc5af1d790d9b7ee50f Stored in directory: /root/.cache/pip/wheels/25/8d/5d/6b642484b1abd87474914e6cf0d3f3a15d8f2653e15ff60f9e Successfully built pyhanlp Installing collected packages: jpype1, pyhanlp Successfully installed jpype1-0.7.0 pyhanlp-0.1.66 . from pyhanlp import * . 下载 https://file.hankcs.com/hanlp/hanlp-1.7.8-release.zip 到 /usr/local/lib/python3.6/dist-packages/pyhanlp/static/hanlp-1.7.8-release.zip 100.00%, 1 MB, 187 KB/s, 还有 0 分 0 秒 下载 https://file.hankcs.com/hanlp/data-for-1.7.5.zip 到 /usr/local/lib/python3.6/dist-packages/pyhanlp/static/data-for-1.7.8.zip 98.24%, 626 MB, 8117 KB/s, 还有 0 分 1 秒 . With PyHanLP, we got a similar parsing result, but without the error that Jieba produced. . tokens = HanLP.segment(text) token_list = [res.word for res in tokens] pyhan = &quot; | &quot;.join(token_list) print(pyhan) . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀 | 酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛 | 隻 | 隻 | 大 | 如山 | 老鼠 | 隻 | 隻 | 死 . However, PyHanLP is about 26 times slower than Jieba, as timed below. . %timeit HanLP.segment(text) . The slowest run took 11.80 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 3: 24.6 µs per loop . Let&#39;s write a function for later use. . def PyHan_tokenizer(text): tokens = HanLP.segment(text) token_list = [res.word for res in tokens] result = &quot; | &quot;.join(token_list) return result . SnowNLP . Next is SnowNLP, which I came across only recently. While PyHanLP is about 640MB in size, SnowNLP takes up only less than 40MB. . !pip install snownlp from snownlp import SnowNLP . Collecting snownlp Downloading https://files.pythonhosted.org/packages/3d/b3/37567686662100d3bce62d3b0f2adec18ab4b9ff2b61abd7a61c39343c1d/snownlp-0.12.3.tar.gz (37.6MB) |████████████████████████████████| 37.6MB 86kB/s Building wheels for collected packages: snownlp Building wheel for snownlp (setup.py) ... done Created wheel for snownlp: filename=snownlp-0.12.3-cp36-none-any.whl size=37760957 sha256=7de1997923cd51c8c45b896d9a29792e57652d5f55e3caf088212be684c50b36 Stored in directory: /root/.cache/pip/wheels/f3/81/25/7c197493bd7daf177016f1a951c5c3a53b1c7e9339fd11ec8f Successfully built snownlp Installing collected packages: snownlp Successfully installed snownlp-0.12.3 . SnowNLP gave a similar result, but made two parsing mistakes. Neither 做醋格 nor 外酸 is a legitimate word. . tokens = SnowNLP(text) token_list = [tokens.words][0] snow = &quot; | &quot;.join(token_list) print(snow) . 今 | 年 | 好 | 煩 | 惱 | 少不得 | 打 | 官司 | 釀 | 酒 | 剛 | 剛 | 好 | 做醋格 | 外酸 | 養 | 牛 | 隻 | 隻 | 大 | 如 | 山 | 老 | 鼠 | 隻 | 隻 | 死 . SnowNLP not only made more mistakes, but also took longer to run. . %timeit SnowNLP(text) . 10000 loops, best of 3: 35.4 µs per loop . But SnowNLP has a convenient feature inspired by TextBlob. Any instance of SnowNLP() has such attributes as words, pinyin (for romanization of words), tags (for parts of speech tags), and even sentiments, which calculates the probability of a text being positive. . print(tokens.words) . [&#39;今&#39;, &#39;年&#39;, &#39;好&#39;, &#39;煩&#39;, &#39;惱&#39;, &#39;少不得&#39;, &#39;打&#39;, &#39;官司&#39;, &#39;釀&#39;, &#39;酒&#39;, &#39;剛&#39;, &#39;剛&#39;, &#39;好&#39;, &#39;做醋格&#39;, &#39;外酸&#39;, &#39;養&#39;, &#39;牛&#39;, &#39;隻&#39;, &#39;隻&#39;, &#39;大&#39;, &#39;如&#39;, &#39;山&#39;, &#39;老&#39;, &#39;鼠&#39;, &#39;隻&#39;, &#39;隻&#39;, &#39;死&#39;] . print(tokens.pinyin) . [&#39;jin&#39;, &#39;nian&#39;, &#39;hao&#39;, &#39;煩&#39;, &#39;惱&#39;, &#39;shao&#39;, &#39;bu&#39;, &#39;de&#39;, &#39;da&#39;, &#39;guan&#39;, &#39;si&#39;, &#39;釀&#39;, &#39;jiu&#39;, &#39;剛&#39;, &#39;剛&#39;, &#39;hao&#39;, &#39;zuo&#39;, &#39;cu&#39;, &#39;ge&#39;, &#39;wai&#39;, &#39;suan&#39;, &#39;養&#39;, &#39;niu&#39;, &#39;隻&#39;, &#39;隻&#39;, &#39;da&#39;, &#39;ru&#39;, &#39;shan&#39;, &#39;lao&#39;, &#39;shu&#39;, &#39;隻&#39;, &#39;隻&#39;, &#39;si&#39;] . print(list(tokens.tags)) . [(&#39;今&#39;, &#39;Tg&#39;), (&#39;年&#39;, &#39;q&#39;), (&#39;好&#39;, &#39;a&#39;), (&#39;煩&#39;, &#39;Rg&#39;), (&#39;惱&#39;, &#39;Rg&#39;), (&#39;少不得&#39;, &#39;Rg&#39;), (&#39;打&#39;, &#39;v&#39;), (&#39;官司&#39;, &#39;n&#39;), (&#39;釀&#39;, &#39;u&#39;), (&#39;酒&#39;, &#39;n&#39;), (&#39;剛&#39;, &#39;i&#39;), (&#39;剛&#39;, &#39;Mg&#39;), (&#39;好&#39;, &#39;a&#39;), (&#39;做醋格&#39;, &#39;Ag&#39;), (&#39;外酸&#39;, &#39;Ng&#39;), (&#39;養&#39;, &#39;Dg&#39;), (&#39;牛&#39;, &#39;Ag&#39;), (&#39;隻&#39;, &#39;Bg&#39;), (&#39;隻&#39;, &#39;a&#39;), (&#39;大&#39;, &#39;a&#39;), (&#39;如&#39;, &#39;v&#39;), (&#39;山&#39;, &#39;n&#39;), (&#39;老&#39;, &#39;a&#39;), (&#39;鼠&#39;, &#39;Ng&#39;), (&#39;隻&#39;, &#39;Ag&#39;), (&#39;隻&#39;, &#39;Bg&#39;), (&#39;死&#39;, &#39;a&#39;)] . print(tokens.sentiments) . 0.04306320074116554 . Again, let&#39;s write a function for later use. . def Snow_tokenizer(text): tokens = SnowNLP(text) token_list = [tokens.words][0] result = &quot; | &quot;.join(token_list) return result . CKIP Transformers . While the four models above are primarily trained on simplified Chinese, CKIP Transformers is trained on traditional Chinese. It is created by the CKIP Lab at Academia Sinica. As its name suggests, CKIP Transformers is built on the Transformer architecture, such as BERT and ALBERT. . Note: Read this to find out How Google Changed NLP. . . !pip install -U ckip-transformers from ckip_transformers.nlp import CkipWordSegmenter . Collecting ckip-transformers Downloading https://files.pythonhosted.org/packages/19/53/81d1a8895cbbc02bf32771a7a43d78ad29a8c281f732816ac422bf54f937/ckip_transformers-0.2.1-py3-none-any.whl Collecting transformers&gt;=3.5.0 Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB) |████████████████████████████████| 1.8MB 22.8MB/s Requirement already satisfied, skipping upgrade: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (4.41.1) Requirement already satisfied, skipping upgrade: torch&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (1.7.0+cu101) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) |████████████████████████████████| 890kB 43.0MB/s Requirement already satisfied, skipping upgrade: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (0.8) Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (20.8) Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (3.0.12) Collecting tokenizers==0.9.4 Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB) |████████████████████████████████| 2.9MB 49.4MB/s Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (2019.12.20) Requirement already satisfied, skipping upgrade: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (3.4.0) Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (1.19.5) Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (2.23.0) Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.1.0-&gt;ckip-transformers) (3.7.4.3) Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.1.0-&gt;ckip-transformers) (0.16.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.15.0) Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (7.1.2) Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.0.0) Requirement already satisfied, skipping upgrade: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2.4.7) Requirement already satisfied, skipping upgrade: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (3.4.0) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2020.12.5) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2.10) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.24.3) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=010fd3e1a8d79574a0b5c323c333d1738886852c4c306fa9d161d1b51f7944b5 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sacremoses, tokenizers, transformers, ckip-transformers Successfully installed ckip-transformers-0.2.1 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2 . CKIP Transformers gives its users the freedom to choose between speed and accuracy. It comes with three levels; the smaller the number, the shorter the running time. All you need to do is pass a number to the level argument of CkipWordSegmenter(). Here&#39;re the models and F1 scores for each level: . Level 1: CKIP ALBERT Tiny, 96.66% | Level 2: CKIP ALBERT Base, 97.33% | Level 3: CKIP BERT Base, 97.60% | . By comparison, the F1 score for Jieba is only 81.18%. For more stats, visit the CKIP Lab&#39;s repo. . ws_driver = CkipWordSegmenter(level=1, device=0) . Here&#39;s the result at Level 1. What&#39;s suprising here is that this big chunk 大如山老鼠 was not further segmented. But this is not a mistake. It simply means that the model has learned it as an idiom. . tokens = ws_driver([text]) ckip_1 = &quot; | &quot;.join(tokens[0]) print(ckip_1) . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3284.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 3.98it/s] . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛 | 隻隻 | 大如山老鼠 | 隻隻 | 死 . . Of the five libraries covered here, CKIP Transformers by far takes the longest time to run. But where it lags behind in speed (i.e. 17.8 ms per loop for top 3 results), it makes it up in accuracy. . . Warning: Don&#8217;t toggle to show the output unless you really want to see a long list of details. . %timeit ws_driver([text]) . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1721.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 97.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1529.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1633.93it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 153.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4549.14it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 140.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1354.75it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.18it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1138.52it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2458.56it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1108.43it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 171.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1831.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 115.85it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3184.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.78it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3622.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 605.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1614.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.17it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2353.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.49it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2058.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3847.99it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1375.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.76it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1582.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 76.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3248.88it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.66it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3141.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.91it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2935.13it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2993.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 665.87it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1216.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 140.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 302.25it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.86it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3276.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 84.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 388.40it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4490.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.00it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4288.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.40it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3640.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 90.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 249.28it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 115.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1954.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 77.90it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 710.54it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 123.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1486.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 87.35it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1965.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 505.64it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3070.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2706.00it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 75.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2582.70it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 130.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 500.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.03it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 484.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 166.90it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 570.58it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 108.91it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2185.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 335.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 347.93it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3844.46it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 541.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.98it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2597.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4319.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4987.28it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 86.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 533.56it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.71it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 589.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.51it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 367.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4396.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 92.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 550.22it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3971.88it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 109.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 430.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 149.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2421.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3418.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 923.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1027.01it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 338.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3075.00it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 75.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1933.75it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 78.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4804.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5017.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4116.10it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 66.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3788.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 65.55it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3785.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5184.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 122.87it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 584.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.56it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2949.58it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.96it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1034.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.94it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3692.17it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 513.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1015.82it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 483.60it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.84it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 958.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.28it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4076.10it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 374.26it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 107.21it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 383.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3360.82it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 174.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5289.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.56it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 505.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 371.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 160.73it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4279.90it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 91.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2314.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.91it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1760.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 86.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2141.04it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2222.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 62.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3059.30it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5102.56it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 86.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1640.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 133.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1925.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5769.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4559.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1612.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 69.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2332.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 141.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3328.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.13it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4809.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 128.95it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4258.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5256.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 347.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.55it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5540.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 92.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2531.26it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 144.72it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2322.43it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.81it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5866.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3581.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3872.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4975.45it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2727.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 80.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4593.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5461.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.51it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3949.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 99.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4963.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.21it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2228.64it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.99it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5115.00it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 76.19it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 809.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.51it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5242.88it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 142.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5184.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.63it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5777.28it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5159.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1851.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.17it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 910.22it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 58.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1122.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 857.73it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 66.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3515.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.35it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1228.20it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5555.37it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 143.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5849.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 133.58it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5197.40it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2364.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 162.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 735.84it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 91.59it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4044.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 74.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1099.42it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 615.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2549.73it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 81.04it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 449.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.13it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2538.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2227.46it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5236.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4132.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1465.52it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 83.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1186.51it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.21it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1879.17it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 77.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2431.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3578.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4514.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4181.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 107.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5178.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4975.45it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4691.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 73.17it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2323.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 64.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2063.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 123.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 198.49it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4359.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 84.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5133.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1329.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 71.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1265.25it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 460.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.94it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4387.35it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.12it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4040.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1589.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4249.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4452.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 48.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 393.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 67.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 786.19it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 107.90it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 133.06it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 240.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.24it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2581.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5065.58it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.19it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3102.30it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 77.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4644.85it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.58it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4744.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2286.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 45.18it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2661.36it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.13it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1713.36it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 48.73it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 996.04it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 109.81it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2339.27it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1211.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.18it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1178.84it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4670.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.96it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4728.64it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 68.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4262.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.73it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3968.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4614.20it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4194.30it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4629.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4301.85it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4253.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 108.40it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5035.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5336.26it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.04it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5035.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 99.04it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5077.85it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 496.07it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 40.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2598.70it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.39it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5562.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 44.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3695.42it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 115.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.84it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4410.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5667.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.85it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4144.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3688.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 84.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 40.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 513.06it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2792.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 76.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1015.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 62.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 551.95it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 86.93it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 940.64it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 74.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 528.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 92.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4832.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5178.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 791.53it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.51it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4559.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1060.77it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 515.14it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 576.54it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4337.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 75.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4364.52it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 76.19it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4739.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4223.87it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 980.66it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4568.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 97.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4514.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 57.58it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3506.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 91.81it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4088.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.46it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4140.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 85.03it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4132.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 78.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4288.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.84it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4391.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 97.76it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4462.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 510.13it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.59it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4060.31it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.75it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4433.73it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 83.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2562.19it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4946.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5384.22it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1106.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 69.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4563.99it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2968.37it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2319.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 48.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5497.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5907.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.98it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5940.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 675.19it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 120.91it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5540.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.40it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1164.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.59it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 604.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5932.54it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.01it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1723.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5907.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6563.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 151.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4860.14it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6069.90it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 141.28it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5667.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5683.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 135.45it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6204.59it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6114.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4815.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 653.42it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 52.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5745.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5637.51it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.49it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5592.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 123.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4837.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 145.63it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1220.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 71.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 420.40it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 282.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.75it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 742.75it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5584.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 161.28it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3139.45it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 87.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 623.87it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 151.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 586.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 176.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5203.85it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 142.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5295.84it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 157.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 672.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6043.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 164.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 510.82it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5329.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3182.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6123.07it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 180.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5584.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 139.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6052.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5497.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 518.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6213.78it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 161.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2792.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 575.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 314.51it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.32it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 914.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.90it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5315.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 83.18it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 946.58it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 87.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 428.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 489.59it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.45it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2451.38it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 172.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2730.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 133.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 618.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.37it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 713.20it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.27it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1596.61it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 64.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 811.59it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2263.52it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 48.84it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3622.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2642.91it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 84.35it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 761.22it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 139.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2423.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 141.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 118.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6000.43it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.75it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5991.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 135.27it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2798.07it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 108.08it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3246.37it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.49it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5223.29it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6017.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 83.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3130.08it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 162.87it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2743.17it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2799.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 140.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1239.45it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.71it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3276.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 146.19it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1399.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6061.13it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 762.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 141.08it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5426.01it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5874.38it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.98it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4771.68it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 143.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3170.30it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.37it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3587.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 107.01it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4969.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5817.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.94it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5991.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.32it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5622.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 836.35it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5433.04it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1015.82it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.56it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5115.00it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 835.35it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 80.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2362.99it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 158.80it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2304.56it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 154.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6626.07it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 142.10it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5146.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 64.62it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 424.91it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4928.68it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 149.12it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5698.78it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 140.78it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6043.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.59it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5555.37it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.98it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6842.26it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 162.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5675.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 169.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5229.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 49.94it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3313.04it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 51.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 829.90it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 69.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 539.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 81.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 649.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1151.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.85it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3731.59it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 118.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3518.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 143.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3008.83it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 184.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2641.25it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 153.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 559.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2803.68it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 166.56it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2931.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 168.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3084.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 155.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3826.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.72it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 935.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 70.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2504.06it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.86it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2931.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2590.68it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 146.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5140.08it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.99it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1217.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1049.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 97.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1402.78it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.37it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 887.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 128.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1734.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 58.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4804.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.32it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3401.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3795.75it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4922.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2186.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 130.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5210.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5236.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 139.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3155.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.93it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5753.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 737.40it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2498.10it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.55it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4723.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 99.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3548.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 159.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3457.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.12it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 964.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.45it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1173.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2757.60it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 171.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3013.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2830.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 169.58it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3569.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1367.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2563.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5857.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 109.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1149.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5899.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5761.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 139.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5426.01it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 670.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 973.61it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 108.21it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5637.51it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 133.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 588.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2849.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 92.24it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2743.17it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5817.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5983.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3045.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 501.23it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2514.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5548.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.28it/s] . 100 loops, best of 3: 17.8 ms per loop . . . Let&#39;s reinstantiate the CkipWordSegmenter() class and set the level to 2 this time. . ws_driver = CkipWordSegmenter(level=2, device=0) . Here&#39;s the result at Level 2, where 大如山老鼠 was properly segmented into 大, 如, and 山老鼠. . tokens = ws_driver([text]) ckip_2 = &quot; | &quot;.join(tokens[0]) print(ckip_2) . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2253.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 47.86it/s] . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛好 | 做醋 | 格外 | 酸 | 養牛 | 隻隻 | 大 | 如 | 山老鼠 | 隻隻 | 死 . . Finally, let&#39;s create an instance of CkipWordSegmenter() at Level 3. . ws_driver = CkipWordSegmenter(level=3, device=0) . However, Level 3 didn&#39;t produce a better result than Level 2. For instance, 牛隻, though a legitimate token, is not appropriate in this context. . tokens = ws_driver([text]) ckip_3 = &quot; | &quot;.join(tokens[0]) print(ckip_3) . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 976.10it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 59.33it/s] . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛隻 | 隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 . . Here&#39;s the function for later use, which takes two arguments instead of one, unlike in previous cases. . def Ckip_tokenizer(text, level): ws_driver = CkipWordSegmenter(level=level, device=0) tokens = ws_driver([text]) result = &quot; | &quot;.join(tokens[0]) return result . Comparison . To compare the five libraries, let&#39;s write a general function. . def Tokenizer(text, style): if style == &#39;jieba&#39;: result = Jieba_tokenizer(text) elif style == &#39;pku&#39;: result = PKU_tokenizer(text) elif style == &#39;pyhan&#39;: result = PyHan_tokenizer(text) elif style == &#39;snow&#39;: result = Snow_tokenizer(text) elif style == &#39;ckip&#39;: res1 = Ckip_tokenizer(text, 1) res2 = Ckip_tokenizer(text, 2) res3 = Ckip_tokenizer(text, 3) result = f&quot;Level 1: {res1} nLevel 2: {res2} nLevel 3: {res3}&quot; output = f&quot;Result tokenized by {style}: n{result}&quot; return output . Now I&#39;m interested in finding out whether simplified or traditional Chinese would have any effect on segmentation results. In addition to the text we&#39;ve been trying (let&#39;s rename it as text_A), we&#39;ll also test another challenging text taken from the PyHanLP repo (let&#39;s call it text_B), which is intended to be ambiguous in multiple places. Given these two texts, two versions of Chinese scripts (simplified and traditional), and five segmentation libraries, we end up having in total 20 combinations of texts and libraries. . import itertools textA_tra = &quot;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&quot; textA_sim = &quot;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&quot; textB_tra = &quot;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&quot; textB_sim = &quot;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&quot; texts = [textA_tra, textA_sim, textB_tra, textB_sim] tokenizers = [&#39;jieba&#39;, &#39;pku&#39;, &#39;pyhan&#39;, &#39;snow&#39;,&#39;ckip&#39;] testing_tup = list(itertools.product(texts, tokenizers)) testing_tup . [(&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;jieba&#39;), (&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;pku&#39;), (&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;pyhan&#39;), (&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;snow&#39;), (&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;ckip&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;jieba&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;pku&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;pyhan&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;snow&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;ckip&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;jieba&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;pku&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;pyhan&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;snow&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;ckip&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;jieba&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;pku&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;pyhan&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;snow&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;ckip&#39;)] . Here&#39;re the results for traditional textA. . for sent in testing_tup[:5]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸養 | 牛 | 隻 | 隻 | 大如山 | 老鼠 | 隻 | 隻 | 死 Result tokenized by pku: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀 | 酒剛 | 剛 | 好 | 做 | 醋 | 格外 | 酸養 | 牛 | 隻隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 Result tokenized by pyhan: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀 | 酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛 | 隻 | 隻 | 大 | 如山 | 老鼠 | 隻 | 隻 | 死 Result tokenized by snow: 今 | 年 | 好 | 煩 | 惱 | 少不得 | 打 | 官司 | 釀 | 酒 | 剛 | 剛 | 好 | 做醋格 | 外酸 | 養 | 牛 | 隻 | 隻 | 大 | 如 | 山 | 老 | 鼠 | 隻 | 隻 | 死 . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1287.78it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.95it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1394.38it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 66.44it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 998.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.47it/s] . Result tokenized by ckip: Level 1: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛 | 隻隻 | 大如山老鼠 | 隻隻 | 死 Level 2: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛好 | 做醋 | 格外 | 酸 | 養牛 | 隻隻 | 大 | 如 | 山老鼠 | 隻隻 | 死 Level 3: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛隻 | 隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 . . Here&#39;re the results for the simplified version of the same text. Notice that the outcome can be quite different simply because a traditional text is converted to its simplified counterpart. . for sent in testing_tup[5:10]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚 | 好 | 做 | 醋 | 格外 | 酸 | 养牛 | 隻 | 隻 | 大如山 | 老鼠 | 隻 | 隻 | 死 Result tokenized by pku: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚 | 好 | 做 | 醋 | 格外 | 酸养 | 牛隻 | 隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 Result tokenized by pyhan: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚好 | 做 | 醋 | 格外 | 酸 | 养牛 | 隻 | 隻 | 大 | 如山 | 老鼠 | 隻 | 隻 | 死 Result tokenized by snow: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚 | 好 | 做醋 | 格外 | 酸 | 养 | 牛 | 隻 | 隻 | 大 | 如 | 山 | 老 | 鼠 | 隻 | 隻 | 死 . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 303.61it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 123.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 695.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 66.45it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 392.84it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.00it/s] . Result tokenized by ckip: Level 1: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿 | 酒 | 刚刚 | 好 | 做 | 醋 | 格外 | 酸 | 养 | 牛隻隻 | 大如山老鼠 | 隻隻 | 死 Level 2: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚 | 好 | 做醋 | 格外 | 酸 | 养 | 牛 | 隻隻 | 大 | 如 | 山老鼠 | 隻隻 | 死 Level 3: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚好 | 做 | 醋 | 格外 | 酸 | 养 | 牛隻 | 隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 . . Here&#39;re the results for traditional textB. Serious mistakes include 處女 (for &quot;virgin&quot;) and 口交 (for &quot;blowjob&quot;). Both are correct words in Chinese, but not the intended ones in this context. . for sent in testing_tup[10:15]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: 工信 | 處女 | 幹事 | 每月 | 經過 | 下屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口交 | 換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 Result tokenized by pku: 工信 | 處女 | 幹事 | 每月 | 經 | 過下 | 屬科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交 | 換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 Result tokenized by pyhan: 工 | 信 | 處女 | 幹 | 事 | 每月 | 經 | 過 | 下 | 屬 | 科室 | 都 | 要 | 親 | 口 | 交代 | 24 | 口交 | 換機 | 等 | 技 | 術 | 性 | 器件 | 的 | 安 | 裝 | 工作 Result tokenized by snow: 工 | 信 | 處 | 女 | 幹 | 事 | 每 | 月 | 經 | 過 | 下 | 屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交 | 換 | 機 | 等 | 技 | 術性 | 器件 | 的 | 安 | 裝 | 工作 . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 494.49it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.49it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 402.87it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.66it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3942.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.56it/s] . Result tokenized by ckip: Level 1: 工信 | 處女 | 幹事 | 每 | 月 | 經過 | 下屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 Level 2: 工信處 | 女 | 幹事 | 每 | 月 | 經過 | 下屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 Level 3: 工信處 | 女 | 幹事 | 每 | 月 | 經過 | 下屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 . . Here&#39;re the results for the simplified version of textB. In terms of textB, CKIP Transformers Level 2 and 3 are most stable, giving the same error-free results regardless of the writing sytems. . for sent in testing_tup[15:]: result = Tokenizer(sent[0], sent[1]) print(result) . . Result tokenized by jieba: 工信处 | 女干事 | 每月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Result tokenized by pku: 工信 | 处女 | 干事 | 每月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Result tokenized by pyhan: 工信处 | 女干事 | 每月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Result tokenized by snow: 工 | 信处女 | 干事 | 每月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1220.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 878.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 71.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1254.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.75it/s] . Result tokenized by ckip: Level 1: 工信处 | 女干 | 事 | 每 | 月 | 经过 | 下 | 属 | 科室 | 都 | 要 | 亲 | 口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Level 2: 工信处 | 女 | 干事 | 每 | 月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Level 3: 工信处 | 女 | 干事 | 每 | 月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 . . Recap . This post has tested five word segmentation tools against two challenging Chinese texts. Here&#39;re the takeaways: . If you value speed more than anything, Jieba is definitely the top choice. If you&#39;re dealing with traditional Chinese, it is a good practice to first convert your texts to simplified Chinese before feeding them to Jieba. Doing this may produce better results. . | If you care more about accuracy instead, it&#39;s best to use CKIP Transformers. Its Level 2 and 3 produce consistent results whether your texts are in traditional or simplified Chinese. . | Finally, if you hope to levarage the power of NLP libraries such as spaCy and Texthero (by the way, their slogan is really awesome: from zero to hero), you&#39;ll have to go for Jieba or PKUSeg. I hope spaCy will also add CKIP to its inventory of tokenizers in the near future. . | .",
            "url": "https://howard-haowen.github.io/blog.ai/tokenization/jieba/pkuseg/pyhanlp/snownlp/ckip-transformers/2021/01/30/Many-ways-to-segment-Chinese.html",
            "relUrl": "/tokenization/jieba/pkuseg/pyhanlp/snownlp/ckip-transformers/2021/01/30/Many-ways-to-segment-Chinese.html",
            "date": " • Jan 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Visualizing real estate prices with Altair",
            "content": "Intro . Several months into the journey of Python programming, I was already aware of visualization tools like Matplotlib Seaborn, and Plotly, which are commonly discussed on Medium. But I&#39;d never heard of Altair until I came across fastpages. Since I plan to keep writing on this fastpages-powered blog, I did some experiments with Altair. For illustration purprose, I&#39;ll be using the dataset of real estate prices in Kaohsiung, TW, which I&#39;ve cleaned and put together in my GitHub repo. For those of you who don&#39;t know Kaohsiung, it&#39;s selected by the New York Times as one of the 52 places to love in 2021. Maybe you&#39;ll consider buying an apartment in Kaohsiung after reading this post. Who knows? . Import dependencies . Altair is alrealdy pre-installed on Colab. So there&#39;s no need to pip-install it if you&#39;re doing this on Colab. . import pandas as pd import altair as alt from altair import datum . Load the dataset . The first thing to do is to git-clone the dataset into your environment. . !git clone -l -s https://github.com/howard-haowen/kh-real-estate cloned-repo %cd cloned-repo !ls . Cloning into &#39;cloned-repo&#39;... warning: --local is ignored remote: Enumerating objects: 100, done. remote: Counting objects: 100% (100/100), done. remote: Compressing objects: 100% (100/100), done. remote: Total 100 (delta 46), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (100/100), 3.30 MiB | 1.03 MiB/s, done. Resolving deltas: 100% (46/46), done. /content/cloned-repo catboost-model-feature-importance.png catboost-model-residuals.png catboost-model-feature-importance-shap-value.png compare-models.png catboost-model-learning-curve.png kh-house-prices.csv catboost-model-outliers.png kh-house-prices.pkl catboost-model.png LICENSE catboost-model-prediction-errors.png README.md . . Let&#39;s take a look at 5 random observations. . df = pd.read_pickle(&#39;kh-house-prices.pkl&#39;) df.sample(5) . . purpose trading_target land_area property_type living_room bedroom bathroom partition property_area is_managed total_floor parking_area parking_price parking_type land_use district trading_date trading_year built_date built_year price_per_sqm . 25204 住家用 | 房地(土地+建物)+車位 | 13.53 | 住宅大樓(11層含以上有電梯) | 2 | 4 | 2 | 有 | 129.39 | 有 | 13 | 0.00 | 0 | 坡道平面 | 商 | 楠梓區 | 2017-01-20 | 2017 | 1995-01-26 | 1995 | 33233.0 | . 19272 住家用 | 房地(土地+建物)+車位 | 18.24 | 住宅大樓(11層含以上有電梯) | 0 | 0 | 0 | 無 | 360.51 | 有 | 36 | 61.10 | 0 | 坡道平面 | 商 | 鼓山區 | 2016-05-20 | 2016 | 2014-06-26 | 2014 | 62717.0 | . 12575 住家用 | 房地(土地+建物)+車位 | 13.12 | 住宅大樓(11層含以上有電梯) | 2 | 3 | 2 | 有 | 145.90 | 有 | 15 | 12.66 | 840000 | 坡道機械 | 住 | 鼓山區 | 2015-07-14 | 2015 | 2014-05-15 | 2014 | 73101.0 | . 15299 住家用 | 房地(土地+建物)+車位 | 15.42 | 住宅大樓(11層含以上有電梯) | 2 | 3 | 2 | 有 | 125.39 | 有 | 15 | 11.24 | 0 | 坡道機械 | 住 | 左營區 | 2015-11-08 | 2015 | 2007-01-12 | 2007 | 43066.0 | . 31446 住家用 | 房地(土地+建物)+車位 | 13.91 | 住宅大樓(11層含以上有電梯) | 2 | 3 | 2 | 有 | 177.61 | 有 | 13 | 0.00 | 0 | 坡道機械 | 商 | 鼓山區 | 2017-12-12 | 2017 | 1996-04-05 | 1996 | 44479.0 | . The dataset includes 45717 observations and 21 columns. . df.shape . (45717, 21) . Most of the column names should be self-explanatory since I&#39;ve translated them from the original Chinese to English. . columns = df.columns.tolist() columns . [&#39;purpose&#39;, &#39;trading_target&#39;, &#39;land_area&#39;, &#39;property_type&#39;, &#39;living_room&#39;, &#39;bedroom&#39;, &#39;bathroom&#39;, &#39;partition&#39;, &#39;property_area&#39;, &#39;is_managed&#39;, &#39;total_floor&#39;, &#39;parking_area&#39;, &#39;parking_price&#39;, &#39;parking_type&#39;, &#39;land_use&#39;, &#39;district&#39;, &#39;trading_date&#39;, &#39;trading_year&#39;, &#39;built_date&#39;, &#39;built_year&#39;, &#39;price_per_sqm&#39;] . Here&#39;re some basic stats. . df.describe() . land_area living_room bedroom bathroom property_area total_floor parking_area parking_price trading_year built_year price_per_sqm . count 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 4.571700e+04 | 45717.000000 | 45717.000000 | 4.571700e+04 | . mean 24.949719 | 1.739987 | 2.921058 | 1.907540 | 145.261129 | 13.729947 | 6.606456 | 9.966087e+04 | 2016.760702 | 1999.837938 | 5.222278e+04 | . std 32.301563 | 0.583373 | 1.299294 | 1.084739 | 89.910644 | 7.810174 | 81.029070 | 5.323162e+05 | 1.699207 | 11.445783 | 2.236209e+04 | . min 0.010000 | 0.000000 | 0.000000 | 0.000000 | 0.020000 | 1.000000 | 0.000000 | 0.000000e+00 | 2012.000000 | 1913.000000 | 0.000000e+00 | . 25% 10.450000 | 2.000000 | 2.000000 | 1.000000 | 89.080000 | 8.000000 | 0.000000 | 0.000000e+00 | 2015.000000 | 1994.000000 | 3.849700e+04 | . 50% 16.630000 | 2.000000 | 3.000000 | 2.000000 | 128.440000 | 14.000000 | 0.000000 | 0.000000e+00 | 2017.000000 | 1999.000000 | 4.829400e+04 | . 75% 26.200000 | 2.000000 | 3.000000 | 2.000000 | 171.200000 | 15.000000 | 0.000000 | 0.000000e+00 | 2018.000000 | 2009.000000 | 6.233000e+04 | . max 2140.100000 | 22.000000 | 52.000000 | 50.000000 | 4119.900000 | 85.000000 | 17098.000000 | 1.000000e+07 | 2020.000000 | 2020.000000 | 1.048343e+06 | . MaxRowsError is the first trouble I got! It turns out that by default Altair only allows you to plot a dataset with a maximum of 5000 rows. . alt.Chart(df).mark_point().encode( x=&#39;trading_year&#39;, y=&#39;price_per_sqm&#39;, color=&#39;district&#39;, ).interactive() . MaxRowsError Traceback (most recent call last) /usr/local/lib/python3.6/dist-packages/altair/vegalite/v4/api.py in to_dict(self, *args, **kwargs) 361 copy = self.copy(deep=False) 362 original_data = getattr(copy, &#34;data&#34;, Undefined) --&gt; 363 copy.data = _prepare_data(original_data, context) 364 365 if original_data is not Undefined: /usr/local/lib/python3.6/dist-packages/altair/vegalite/v4/api.py in _prepare_data(data, context) 82 # convert dataframes or objects with __geo_interface__ to dict 83 if isinstance(data, pd.DataFrame) or hasattr(data, &#34;__geo_interface__&#34;): &gt; 84 data = _pipe(data, data_transformers.get()) 85 86 # convert string input to a URLData /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /usr/local/lib/python3.6/dist-packages/altair/vegalite/data.py in default_data_transformer(data, max_rows) 17 @curried.curry 18 def default_data_transformer(data, max_rows=5000): &gt; 19 return curried.pipe(data, limit_rows(max_rows=max_rows), to_values) 20 21 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /usr/local/lib/python3.6/dist-packages/altair/utils/data.py in limit_rows(data, max_rows) 82 &#34;than the maximum allowed ({}). &#34; 83 &#34;For information on how to plot larger datasets &#34; &gt; 84 &#34;in Altair, see the documentation&#34;.format(max_rows) 85 ) 86 return data MaxRowsError: The number of rows in your dataset is greater than the maximum allowed (5000). For information on how to plot larger datasets in Altair, see the documentation . alt.Chart(...) . . The limitation can be lifted by calling this function. . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . According to the official documentation, this is not a good solution. But I did it anyway because I didn&#39;t know better. I was then able to make a plot, but it only took seconds for my Colab notebook to crash. So the lesson learned is this: . . Warning: Never disable the restriction for max rows if you&#8217;re dealing with a huge amount of data! . A better way to deal with this is to pass data by URL, which only supports json and csv files. So I converted my dataframe to csv and then uploaded it to my GitHub repo. Then all that&#39;s needed to start using Altair is the URL to that file. . with open(&#39;kh-house-prices.csv&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as file: df.to_csv(file, index=False) . . Tip: For Altair to load your dataset properly, make sure the dataset is viewable by entering the URL in your browser. If your dataset is stored on GitHub, that means the URL has to start with https://raw.githubusercontent.com rather than https://github.com. . This URL is the data source from which we&#39;ll be making all the charts. . url= &quot;https://raw.githubusercontent.com/howard-haowen/kh-real-estate/main/kh-house-prices.csv&quot; . Simple charts . After we got the data loading and performance issue taken care of, let&#39;s break down the syntax of Altair. . I&#39;m a visual learner, so I personally think the easiest way to get started is to go to the Example Gallery and pick the kind of charts that you&#39;d like to draw. Most of the time, all you need to do is copy-paste the codes and change the data source as well as column names. . All fancy charts start with something simple.In the case of Altair, it&#39;s alt.Chart(), which takes either URL or a pandas DataFrame object (like df in our failed example above) as its argument. . Then you decide what kinds of marks you&#39;d like to draw on the chart by calling the .mark_X() function, where X could be circle if you want to represent an observation with a circle. Other types of marks used in this post include point, line, bar, and area. . Finally, you need to call the encode() function in order to map the properties of your dataset onto the chart you&#39;re making. In this example below, the function takes three arguments: . x for which column to be mapped to the x axis | y for which column to be mapped to the y axis | color for which column to be colored on the chart | . Once you pass url to alt.Chart() and the column names in your dataset to encode(), you&#39;ll get this chart. . alt.Chart(url).mark_circle().encode( x=&#39;built_date:T&#39;, y=&#39;price_per_sqm:Q&#39;, color=&#39;district:N&#39;,) . . . Note: If your data source is a dataframe, then column names are sufficient. But if your data source is an URL as is the case here, you have to specify your data types with :X right after the column names, where X can be one of these: . Q for quantitative data | O for ordinal data | N for nominal data | T for temporal data | G for geographic data | . And one thing that I like about Altair is that there&#39;re lots of predefined aggregate functions that you can use on the fly. For instance, you can pass temporal data to the function yearmonth(), which aggreates data points in terms of year and month. Or you can pass quantitative data to average(), which calculates the mean for you. This way, you won&#39;t have to create additional columns using pandas and keep your raw data as minimal as possible. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,) . . In pandas, we&#39;d filter data using df[FILTER]. In Altair, this is done by .transform_filter(). In the chart above, we see that the majority of data points gather in the lower right corner. So one way to zoom in is to set a range for built_year on the x axis, which represents the year a property was built. Suppose we want built_year to fall within 1950 and 2020, we do alt.FieldRangePredicate(field=&#39;built_year&#39;, range=[1950, 2020]). . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( alt.FieldRangePredicate(field=&#39;built_year&#39;, range=[1950, 2020]) ) . . Similarly, if we want price_per_sqm on the y axis, which represents property prices per square meter (in NT$ of course!) to be in the range of 10k and 300k, then we do alt.FieldRangePredicate(field=&#39;price_per_sqm&#39;, range=[10000, 300000]). . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( alt.FieldRangePredicate(field=&#39;price_per_sqm&#39;, range=[10000, 300000]) ) . . But what if we want to filter data from multiple columns? I found that an easy way to do that is to use datum.X, where X is a column name. Then the syntax is just like what you&#39;d see in pandas. Suppose we want built_year to be greater than 1950 and price_per_sqm less than 300k, then we do (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000). . . Important: It took me a while to figure what what kind of object datum is. It turns out that Altair is smart enough to take care of everything for you as long as you import datum. So be sure to do this: from altair import datum. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000) ) . . Finally, if you want to give viewers of your chart the liberty to zoom in and out, you can make an interactive chart simply by adding .interactive() to the end of your syntax. To see the effect, click on any grid of the following chart and then scroll your mouse or move two of your fingers up and down on your Magic Trackpad. . . Warning: Try not to make too many interactive charts if your dataset is huge because they can cause serious performance issues. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000) ).interactive() . . I think that&#39;s enough for the basics and for you to keep the ball rolling. Coming up are some of the numerous fancy charts that you can make with Altair. . Complex charts . Suppose we want to create a scatter plot where viewers can focus on data points from a particular district of their choice, the .add_selection() function can be quite handy. Let&#39;s first check out the unique districts in the datasets. (Btw, there&#39;re more districts in Kaohsiung. These are simply more densely populated areas.) . districts = df.district.unique().tolist() districts . [&#39;鼓山區&#39;, &#39;前金區&#39;, &#39;前鎮區&#39;, &#39;三民區&#39;, &#39;楠梓區&#39;, &#39;左營區&#39;, &#39;鳳山區&#39;, &#39;新興區&#39;, &#39;苓雅區&#39;] . We first create a variable selection, which we&#39;ll pass to .add_selection() later. The selection itself is a built-in function called alt.selection_single(), which takes the following arguments: . name for the name you want to display in the selection area | fields for a list of column names that views can choose from | init for a dictionary specifying the default value for each selectable column | bind for a dictionary specifying the way a column is to be selected (in this case, alt.binding_select() for a drop down box) and its possible values (indicated by the argument options) | . Additionally, if we want to display information about a data point upon mouseover, we can pass a list of column names to the argument tooltip of the .encode() function. . Importantly, for the interaction to work, we have to add .add_selection(selection) right before the .encode() function. . selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;district&#39;, ], init={&#39;district&#39;: &#39;左營區&#39;, }, bind={&#39;district&#39;: alt.binding_select(options=districts), } ) alt.Chart(url).mark_circle().add_selection(selection).encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;price_per_sqm:Q&#39;, color=alt.condition(selection, &#39;district:N&#39;, alt.value(&#39;lightgray&#39;)), tooltip=[&#39;property_type:N&#39;,&#39;property_area:Q&#39;,&#39;parking_area:Q&#39;, &#39;built_date:T&#39;,&#39;tradinng_date:T&#39;,&#39;price_per_sqm:Q&#39;], ).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 200000) ) . . We can also make two charts and then concatenat them vertically by calling the function alt.vconcat(), which takes chart objects and data as its arguments. . selection = alt.selection_multi(fields=[&#39;district&#39;]) top = alt.Chart().mark_line().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;mean(price_per_sqm):Q&#39;, color=&#39;district:N&#39; ).properties( width=600, height=200 ).transform_filter( selection ) bottom = alt.Chart().mark_bar().encode( x=&#39;yearmonth(trading_date):T&#39;, y=&#39;mean(price_per_sqm):Q&#39;, color=alt.condition(selection, alt.value(&#39;steelblue&#39;), alt.value(&#39;lightgray&#39;)) ).properties( width=600, height=100 ).add_selection( selection ) alt.vconcat( top, bottom, data=url ) . . We can make one chart respond to another chart based on selection on the second one. This can be useful when we want to have both a global and detailed view of the same chart. The key function we need is alt.Scale(). Watch the top chart change as you select different areas of the bottom chart. . brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(url).mark_area().encode( x = &#39;yearmonth(built_date):T&#39;, y = &#39;price_per_sqm:Q&#39; ).properties( width=600, height=200 ) upper = base.encode( alt.X(&#39;yearmonth(built_date):T&#39;, scale=alt.Scale(domain=brush)) ) lower = base.properties( height=60 ).add_selection(brush) upper &amp; lower . . Finally, you can also pick three random variables from your dataset and make a 3 times 3 grid of charts, with each varing in the x and y axis combination. To do that, we&#39;ll need to specify repetition in two places: once in the argument of the x and y axis (i.e. alt.repeat() within alt.X and alt.Y) and the other time in the outmost layer of the syntax (i.e. .repeat() at the very end). . alt.Chart(url).mark_circle().encode( alt.X(alt.repeat(&quot;column&quot;), type=&#39;quantitative&#39;), alt.Y(alt.repeat(&quot;row&quot;), type=&#39;quantitative&#39;), color=&#39;district:N&#39; ).properties( width=150, height=150 ).repeat( row=[&#39;property_area&#39;, &#39;price_per_sqm&#39;, &#39;built_year&#39;], column=[&#39;built_year&#39;, &#39;price_per_sqm&#39;, &#39;property_area&#39;] ) . . Recap . Altair is a Python library worth looking into if you want to show interactive charts on your websites and give your visitors some freedom to play with the outcome. This post only shows what I&#39;ve tried. If you wish to dig deeper into this library, uwdata/visualization-curriculum seems like a great resource, aside from the official documentation. Now that you know the average price of real estate in Kaohsiung, TW, would you consider moving down here? 👨‍💻 .",
            "url": "https://howard-haowen.github.io/blog.ai/visualization/real-estate-prices/altair/2021/01/24/Visualizing-real-estate-prices-with-Altair.html",
            "relUrl": "/visualization/real-estate-prices/altair/2021/01/24/Visualizing-real-estate-prices-with-Altair.html",
            "date": " • Jan 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "fastText embeddings for traditional Chinese",
            "content": "Intro . This video explains to you what fastText is all about as if you were five years old. If the video doesn&#39;t load, click on this link. . fastText cbow 300 dimensions from Facebook . Here&#39;re the simple steps for loading the Chinese model released by Facebook, abbreviated here as ft. . !pip install fasttext import fasttext . Collecting fasttext Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB) |████████████████████████████████| 71kB 4.4MB/s Requirement already satisfied: pybind11&gt;=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1) Requirement already satisfied: setuptools&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (51.1.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.19.5) Building wheels for collected packages: fasttext Building wheel for fasttext (setup.py) ... done Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3039122 sha256=5aa81e1045293ebc74315d2013c28cd0018ec96b8868502d535b71438f1faa0c Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592 Successfully built fasttext Installing collected packages: fasttext Successfully installed fasttext-0.9.2 . import fasttext.util fasttext.util.download_model(&#39;zh&#39;, if_exists=&#39;ignore&#39;) # zh = Chinese . Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.bin.gz . &#39;cc.zh.300.bin&#39; . ft = fasttext.load_model(&#39;cc.zh.300.bin&#39;) . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . The ft model covers a whopping great number of words, 2000000 to be exact, because it&#39;s trained on a HUGE corpus. . len(ft.words) . 2000000 . Let&#39;s check out the top 10 words most similar to &quot;疫情&quot; (meaning &quot;pandemic situation&quot;) according to the ft model. The numbers indicate the degree of similarity. The larger the number, the greater the similarity. . ft.get_nearest_neighbors(&quot;疫情&quot;) . [(0.7571706771850586, &#39;禽流感&#39;), (0.6940484046936035, &#39;甲流&#39;), (0.6807129383087158, &#39;流感&#39;), (0.6670429706573486, &#39;疫病&#39;), (0.6640030741691589, &#39;防疫&#39;), (0.6531218886375427, &#39;萨斯病&#39;), (0.6506668329238892, &#39;H1N1&#39;), (0.6495682001113892, &#39;疫症&#39;), (0.6432098150253296, &#39;ＳＡＲＳ&#39;), (0.642063319683075, &#39;疫区&#39;)] . . The results are pretty good. But the downside is that the ft model is huge in size. After being unzipped, the model file is about 6.74G. . fastText cbow 300 dimensions from ToastyNews in Cantonese . This article is what inpired me to write this post. The author trained a fastText model on articles written in Cantonese, which uses traditional characters. Here&#39;re the simple steps for loading his model, abbreviated here as hk. . Since his model is stored on GDrive, I find it more convenient to use the gdown library to download the model. . import gdown . url = &#39;https://drive.google.com/u/0/uc?export=download&amp;confirm=4g-b&amp;id=1kmZ8NKYDngKtA_-1f3ZdmbLV0CDBy1xA&#39; output = &#39;toasty_news.bin.gz&#39; gdown.download(url, output, quiet=False) . Downloading... From: https://drive.google.com/u/0/uc?export=download&amp;confirm=4g-b&amp;id=1kmZ8NKYDngKtA_-1f3ZdmbLV0CDBy1xA To: /content/toasty_news.bin.gz 2.77GB [00:26, 106MB/s] . &#39;toasty_news.bin.gz&#39; . The file needs to be first unzipped to be loaded as a fastText model. An easy way to do that is the command !gunzip plus a file name. . !gunzip toasty_news.bin.gz . hk = fasttext.load_model(&#39;/content/toasty_news.bin&#39;) hk.get_dimension() . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . 300 . The hk model covers 222906 words in total. . len(hk.words) . 222906 . fastText cbow 100 dimensions from Taiwan news in traditional Chinese . I trained a fastText model on 5816 articles of Taiwan news in traditional Chinese, most of them related to health and diseases. . tw = fasttext.load_model(path) # &quot;path&quot; is where my model is stored. tw.get_dimension() . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . 100 . The tw model covers only 11089 words in total because it&#39;s trained on a much smaller corpus than the hk model. . len(tw.words) . 11089 . Comparison . My original plan was to compare all the three models and see what similar words they come up with given the same keyword. But the ft model is huge so I can&#39;t load all of them into RAM. The RAM limit on Colab is about 12G. So we&#39;ll just compare the tw and hk model. . Since we&#39;re not concerned with the degree of similarity, let&#39;s write a simple function to show just similar words. . def similar_words(keyword, model): top_10 = model.get_nearest_neighbors(keyword) top_10 = [w[1] for w in top_10] return top_10 . Then, calling the function similar_word(), with a keyword and a fastText model as the required arguments, shows the top ten words most similar to the keyword. . similar_words(&quot;疫情&quot;, hk) . [&#39;疫症&#39;, &#39;病疫情&#39;, &#39;武漢肺炎&#39;, &#39;疫潮&#39;, &#39;疫&#39;, &#39;新冠肺炎&#39;, &#39;疫調&#39;, &#39;疫市&#39;, &#39;新型冠狀病毒&#39;, &#39;疫病&#39;] . Now let&#39;s write a function to show the results of the two models side by side in a dataframe. . import pandas as pd models = {&#39;hk&#39;: hk, &#39;tw&#39;: tw} def compare_models(keyword, **models): hk_results = similar_words(keyword, models[&#39;hk&#39;]) tw_results = similar_words(keyword, models[&#39;tw&#39;]) data = {&#39;HKNews_&#39;+keyword: hk_results, &#39;TWNews_&#39;+keyword: tw_results} df = pd.DataFrame(data) return df . Let&#39;s test it out with the keyword &quot;疫情&quot;. . test = compare_models(&quot;疫情&quot;, **models) test . HKNews_疫情 TWNews_疫情 . 0 疫症 | 疫情國 | . 1 病疫情 | 因應 | . 2 武漢肺炎 | 防堵 | . 3 疫潮 | 切記 | . 4 疫 | 擴散 | . 5 新冠肺炎 | 屬地 | . 6 疫調 | 疫情處 | . 7 疫市 | 升溫 | . 8 新型冠狀病毒 | 警訊 | . 9 疫病 | 嚴峻 | . It&#39;s interesting that similar words of &quot;總統&quot; (meaning &quot;the president&quot;) include &quot;蔡總統&quot; (meaning &quot;President Tsai&quot;, referring to Taiwan&#39;s incumbent president) according to the hk model but not the tw model. I&#39;d expect the opposite. . test = compare_models(&quot;總統&quot;, **models) test . HKNews_總統 TWNews_總統 . 0 代總統 | 主持 | . 1 美國總統 | 總統府 | . 2 前總統 | 部長 | . 3 民選總統 | 親臨 | . 4 李總統 | 局長 | . 5 副總統 | 蘇益仁 | . 6 下任總統 | 幹事長 | . 7 總理 | 副院長 | . 8 首相 | 李明亮 | . 9 蔡總統 | 座談會 | . Again, it is the hk model, not the tw model, that knows &quot;蔡英文&quot; (meaning &quot;Tsai Ing-wen&quot;) is most similar to &quot;蔡總統&quot; (meaning &quot;President Tsai&quot;). The two linguistic terms have the same reference. . test = compare_models(&quot;蔡總統&quot;, **models) test . HKNews_蔡總統 TWNews_蔡總統 . 0 蔡英文 | 總統 | . 1 賴清德 | 主持 | . 2 馬英九 | 部長 | . 3 李總統 | 親臨 | . 4 林全 | 局長 | . 5 民進黨 | 陳建仁 | . 6 柯文哲 | 座談會 | . 7 總統 | 吳 | . 8 川普 | 副院長 | . 9 總統府 | 總統府 | . Finally, let&#39;s write a function to quickly compare a list of keywords. . def concat_dfs(keyword_list): dfs = [] for word in keyword_list: df = compare_models(word, **models) dfs.append(df) results = pd.concat(dfs, axis=1) return results . keywords = &quot;疫情 疫苗 病毒 肺炎 檢疫 流感 台灣&quot; key_list = keywords.split() concat_dfs(key_list) . HKNews_疫情 TWNews_疫情 HKNews_疫苗 TWNews_疫苗 HKNews_病毒 TWNews_病毒 HKNews_肺炎 TWNews_肺炎 HKNews_檢疫 TWNews_檢疫 HKNews_流感 TWNews_流感 HKNews_台灣 TWNews_台灣 . 0 疫症 | 疫情國 | 流感疫苗 | 接種 | 輪狀病毒 | 病毒型 | 武漢肺炎 | 豬鏈球菌 | 檢疫所 | 檢疫官 | 流感病毒 | 新流感 | 臺灣 | 臺灣 | . 1 病疫情 | 因應 | 免疫針 | 接種地 | 含病毒 | 腺病毒 | 武肺 | 鏈球菌 | 檢疫中心 | 檢疫站 | 流行性感冒 | 防流感 | 台灣國 | 根除 | . 2 武漢肺炎 | 防堵 | 抗體 | 接種為 | 冠狀病毒 | 病毒株 | 新冠肺炎 | 疾患 | 檢疫局 | 檢疫局 | 禽流感 | 打流感 | 台灣政府 | 歷史 | . 3 疫潮 | 切記 | 藥物 | 接種點 | 新病毒 | 病毒學 | 病疫 | 雙球菌 | 檢疫站 | 航機 | 流行病 | 對流感 | 中國大陸 | 一直 | . 4 疫 | 擴散 | 卡介苗 | 接種卡 | 腺病毒 | 型別 | 病疫情 | 心包膜炎 | 隔離 | 機場 | 疫症 | 豬流感 | 中國 | 亞太 | . 5 新冠肺炎 | 屬地 | 抗生素 | 疫苗量 | 殺病毒 | 流行株 | 疫症 | 特殊 | 自我隔離 | 入境 | 病疫情 | 抗流感 | 台灣人 | 諸多 | . 6 疫調 | 疫情處 | 輪狀病毒 | 卡介苗 | 麻疹病毒 | 株型別 | 非典型肺炎 | 侵襲性 | 隔離者 | 調查表 | 麻疹 | 流感疫 | 中國台灣 | 世紀 | . 7 疫市 | 升溫 | 接種 | 價 | 防病毒 | 腸 | 疫情 | 冠狀動脈 | 病毒檢測 | 港口 | 流行性腮腺炎 | 季節性 | 台灣獨立 | 跨國性 | . 8 新型冠狀病毒 | 警訊 | 預防接種 | 多合一 | 冠状病毒 | 重組 | 廢炎 | 症候群 | 健康申報 | 登機 | 流感疫苗 | 流感病 | 台灣社 | 面臨 | . 9 疫病 | 嚴峻 | 麻疹 | 廠牌 | 病原體 | 毒株 | 疫病 | 冠狀病毒 | 檢測 | 聲明卡 | 登革熱 | 新型 | 中華民國 | 之中 | . . keywords = &quot;頭痛 發燒 流鼻水 &quot; key_list = keywords.split() concat_dfs(key_list) . HKNews_頭痛 TWNews_頭痛 HKNews_發燒 TWNews_發燒 HKNews_流鼻水 TWNews_流鼻水 . 0 偏頭痛 | 肌肉痛 | 咳嗽 | 出現 | 鼻水 | 鼻水 | . 1 頭疼 | 骨頭痛 | 病徵 | 症狀 | 流鼻涕 | 流 | . 2 胃痛 | 噁心 | 發高燒 | 喉嚨痛 | 咳嗽 | 鼻塞 | . 3 痠痛 | 骨頭 | 發病 | 嗅覺 | 喉嚨痛 | 喉嚨 | . 4 酸痛 | 肌肉 | 喉嚨痛 | 味覺 | 出疹 | 喉嚨癢 | . 5 絞痛 | 後眼 | 流鼻水 | 鼻水 | 發燒 | 喉嚨痛 | . 6 腫痛 | 畏寒 | 症狀 | 咳嗽 | 皮疹 | 嗅覺 | . 7 頭暈 | 倦怠 | 徵狀 | 喉嚨 | 流鼻血 | 味覺 | . 8 心絞痛 | 窩痛 | 出疹 | 疲倦 | 肚瀉 | 紅疹 | . 9 腰背痛 | 結膜 | 呼吸困難 | 流 | 咳血 | 倦怠 | . Recap . You can easily find out words most similar to a keyword that you&#39;re interested in just by loading a fastText model. And for it to work pretty well, you don&#39;t even need to have a huge corpus at hand. Even if you don&#39;t know how to train a model from scratch, you can still make good use of fastText by loading pretrained models, like those released by Facebook. In total, 157 languages are covered, including even Malay and Malayalam (Btw, check out this Malayalam grammar that I wrote and is now archived on Semantic Scholar)! . Note: This is my first post written in Jupyter notebook. After I uploaded the .ipynb file to GitHub, the post didn&#8217;t show up automatically and I got a CI failing warning in my repo. Listed in the tip section below is what I did to fix the problem, though I&#8217;m not sure which of them was the key. . Tip: 1. requested an automatic update by following the instructions in the troubleshooting guide 2. deleted the backtick symbol in the summary section of the front matter 3. uploaded the .ipynb file straight from Colab to GitHub instead of doing this manually .",
            "url": "https://howard-haowen.github.io/blog.ai/fasttext/embeddings/similar-words/chinese/2021/01/22/fastText-embeddings-for-traditional-Chinese.html",
            "relUrl": "/fasttext/embeddings/similar-words/chinese/2021/01/22/fastText-embeddings-for-traditional-Chinese.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "My First Post",
            "content": "My First Post: Setting tone for this blog . . There’s always an awesome list for X. . Don&#39;t reinvent the wheel is something you’ll hear a lot in the field of technology. That means, whenever possible, try to use existing tools out there instead of building from scratch. If you use GitHub long enough, you probably already know this: for almost everything you’d like to learn, there’s an awesome list for that, which collects all sorts of awesome resources, including tools and tutorials. All you need to do is search for awesome X on GitHub. Or better yet, there’s even a meta list of awesome lists on various topics, like sindresorhus/awesome. Included in it is an awesome list for linguistics, theimpossibleastronaut/awesome-linguistics, which is a good place to start learning about natural language processing (NLP) if you are a humanities major who knows nothing about programming, as I was about one year ago. To take one more example, I wish I had discovered keon/awesome-nlp much earlier, which could’ve saved me lots of time when I was still fumbling around and trying to wrap my head around how a tool, say gensim, fits into the broader picture of NLP. But sometimes the name of an awesome list doesn’t have the keyword awesome in it, such as this gem ivan-bilan/The-NLP-Pandect. In my opinion, although an awesome list by any other name would be as awesome, the The-NLP-Pandect repo would have got much more stars if awesome were in its name. . But the hardest part is to get the ball rolling. . However, awesome as they are, awesome lists can be quite intimidating to go through and easy to get lost in. And even when you are lucky enough to finally come across an awesome tool that you wanna try out, it sometimes takes lots of trials and errors to figure out the right way to get the ball rolling, especially when you are a fresh programmer. So on this blog, I plan to add my personal touch to various tools, documenting not only what I did right to get the ball rolling, but also what I did wrong to save you (or even future me) from abysmal frustration. For almost every tool, there is already a wide specturum of instructional documents available online, ranging from hardcore official documentations to professional posts on platforms like Medium. And this blog is meant to be a friendly complement to those. I’ll be writing in plain language because I was not trained for computer science or programming anyway. . The fast stack . I’d like to start with a series of tools that I dub the fast stack, including fastpages, fast.ai, fastText, and fastAPI. First of all, fastpages , designed by the awesome fast.ai team, is basically a template for building blogs (like this one!) within seconds. It does lots of awesome things for you hehind the scenes. Features that I like about it include: . automatically converts .md and .ipynb files on GitHub to posts on your blog | automatically adds links to Colab and GitHub | shows interactive visualizations of your data with the help of Altair | supports comments, tags, and fast search (super fast at that!) | is free from end to end | . Truth be told that I actually failed twice before I successfully set up this blog. The lesson learned is this: do exactly what’s said in the instructions! Humanities majors like me are often taught to be creative, but be sure to leave your creativity at the door when you set up computer programs. This then concludes my first post. Nothing is super technical here since it’s just a warm-up. I’ll save other tools in the fast stack for another day. . . I was clueless when I read PR in the instructions. It turns out to mean &#39;pull requests&#39;. Click on the tab that says &#39;pull requests&#39; when you are done forking the original repo. Then you&#39;re good to go by following the instructions there.",
            "url": "https://howard-haowen.github.io/blog.ai/awesome-list/fastpages/2020/01/17/my-first-post-dont-reinvent-the-wheel.html",
            "relUrl": "/awesome-list/fastpages/2020/01/17/my-first-post-dont-reinvent-the-wheel.html",
            "date": " • Jan 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Haowen Jiang. I got a PhD of linguistics from Rice University and spent most of my early career in academia doing theoretical research on indigenous languages of Taiwan, all of them related to languages such as Tagalog and Malay/Indonesian. Then along came COVID-19, which disrupted most people’s lives, including mine. Somehow I became hooked on machine learning and Python programming, and now I work as an AI engineer, focusing on Natural Language Processing (NLP) and other AI-related applicational stuff. .",
          "url": "https://howard-haowen.github.io/blog.ai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://howard-haowen.github.io/blog.ai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}