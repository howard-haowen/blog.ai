{
  
    
        "post0": {
            "title": "freeCodeCamp project Demographic Data Analyzer",
            "content": ". . Intro . To earn the freeCodeCamp certification on Data Analysis with Python, one has to complete five projects. This post goes through the steps for completing the one named Demographic Data Analyzer with the pandas library. For more background info, read the first post of this series. . Import . For this project, we need nothing but the pandas üêº library, which is preinstalled on Colab. This following graph summarizes its major use cases. . . We&#39;ll be primarily dealing with the following three types of objects: . Series | DataFrame | GroupBy | . import pandas as pd . Dataset . Let&#39;s first clone the project repo and change the current directory to boilerplate-demographic-data-analyzer. In it, you&#39;ll find the adult.data.csv file, which is the dataset. . repo_url = &quot;https://github.com/freeCodeCamp/boilerplate-demographic-data-analyzer&quot; !git clone {repo_url} %cd boilerplate-demographic-data-analyzer . Cloning into &#39;boilerplate-demographic-data-analyzer&#39;... remote: Enumerating objects: 22, done. remote: Counting objects: 100% (22/22), done. remote: Compressing objects: 100% (19/19), done. remote: Total 22 (delta 7), reused 12 (delta 2), pack-reused 0 Unpacking objects: 100% (22/22), done. /content/boilerplate-demographic-data-analyzer . . Since we&#39;re dealing with a csv file, we&#39;ll load the dataset with the read_csv function and save it as the variable df, which is pretty much the default name for DataFrame objects. A sample of 10 records is shown below. . fname = &quot;adult.data.csv&quot; df = pd.read_csv(fname) df.sample(10) . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 21634 38 | Private | 179668 | HS-grad | 9 | Married-civ-spouse | Protective-serv | Husband | White | Male | 0 | 0 | 40 | Scotland | &lt;=50K | . 4268 36 | Private | 281021 | HS-grad | 9 | Never-married | Transport-moving | Not-in-family | White | Male | 0 | 0 | 45 | United-States | &lt;=50K | . 1863 25 | Private | 176520 | HS-grad | 9 | Never-married | Craft-repair | Other-relative | White | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 10570 21 | ? | 188535 | Some-college | 10 | Never-married | ? | Own-child | White | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 22607 19 | Private | 243373 | 12th | 8 | Never-married | Sales | Other-relative | White | Male | 1055 | 0 | 40 | United-States | &lt;=50K | . 13134 41 | State-gov | 180272 | Masters | 14 | Never-married | Prof-specialty | Own-child | White | Female | 0 | 0 | 35 | United-States | &lt;=50K | . 14015 49 | Self-emp-inc | 119565 | Masters | 14 | Married-civ-spouse | Sales | Husband | White | Male | 0 | 0 | 40 | United-States | &gt;50K | . 32054 23 | Private | 335067 | Bachelors | 13 | Never-married | Sales | Not-in-family | White | Male | 0 | 0 | 50 | United-States | &lt;=50K | . 14950 51 | Private | 74660 | 10th | 6 | Married-civ-spouse | Craft-repair | Husband | White | Male | 0 | 0 | 40 | United-States | &gt;50K | . 28137 49 | Private | 209146 | HS-grad | 9 | Married-civ-spouse | Transport-moving | Husband | White | Male | 0 | 0 | 40 | United-States | &lt;=50K | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . The dataset has in total 32561 rows and 15 columns, with each row representing a person and each column indicating an attribute of that person. . df.shape . (32561, 15) . Task instructions . As in my previous post, I use the rich library to render the README.md file for task instructions. . !pip install rich from rich.console import Console from rich.markdown import Markdown def show_readme(): console = Console() with open(&quot;README.md&quot;) as readme: markdown = Markdown(readme.read()) console.print(markdown) . Collecting rich Downloading rich-11.2.0-py3-none-any.whl (217 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217 kB 3.2 MB/s Collecting colorama&lt;0.5.0,&gt;=0.4.0 Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB) Collecting commonmark&lt;0.10.0,&gt;=0.9.0 Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51 kB 5.8 MB/s Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich) (2.6.1) Requirement already satisfied: typing-extensions&lt;5.0,&gt;=3.7.4 in /usr/local/lib/python3.7/dist-packages (from rich) (3.10.0.2) Installing collected packages: commonmark, colorama, rich Successfully installed colorama-0.4.4 commonmark-0.9.1 rich-11.2.0 . . Assignment . show_readme() . Assignment ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó ‚ïë Demographic Data Analyzer ‚ïë ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù In this challenge you must analyze demographic data using Pandas. You are given a dataset of demographic data that was extracted from the 1994 Census database. Here is a sample of what the data looks like: | | age | workclass | fnlwgt | education | education-num | marital-status | occupation | relationship | race | sex | capital-gain | capital-loss | hours-per-week | native-country | salary | |:|:|:--|:|:- --|-:|:-|:|:|:- |:-|:|:|--:|:--|:- --| | 0 | 39 | State-gov | 77516 | Bachelors | 13 | Never-married | Adm-clerical | Not-in-family | White | Male | 2174 | 0 | 40 | United-States | &lt;=50K | | 1 | 50 | Self-emp-not-inc | 83311 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 13 | United-States | &lt;=50K | | 2 | 38 | Private | 215646 | HS-grad | 9 | Divorced | Handlers-cleaners | Not-in-family | White | Male | 0 | 0 | 40 | United-States | &lt;=50K | | 3 | 53 | Private | 234721 | 11th | 7 | Married-civ-spouse | Handlers-cleaners | Husband | Black | Male | 0 | 0 | 40 | United-States | &lt;=50K | | 4 | 28 | Private | 338409 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Wife | Black | Female | 0 | 0 | 40 | Cuba | &lt;=50K | You must use Pandas to answer the following questions: ‚Ä¢ How many people of each race are represented in this dataset? This should be a Pandas series with race names as the index labels. (race column) ‚Ä¢ What is the average age of men? ‚Ä¢ What is the percentage of people who have a Bachelor&#39;s degree? ‚Ä¢ What percentage of people with advanced education (Bachelors, Masters, or Doctorate) make more than 50K? ‚Ä¢ What percentage of people without advanced education make more than 50K? ‚Ä¢ What is the minimum number of hours a person works per week? ‚Ä¢ What percentage of the people who work the minimum number of hours per week have a salary of more than 50K? ‚Ä¢ What country has the highest percentage of people that earn &gt;50K and what is that percentage? ‚Ä¢ Identify the most popular occupation for those who earn &gt;50K in India. Use the starter code in the file demographic_data_analyzer. Update the code so all variables set to &quot;None&quot; are set to the appropriate calculation or code. Round all decimals to the nearest tenth. Unit tests are written for you under test_module.py. Development For development, you can use main.py to test your functions. Click the &quot;run&quot; button and main.py will run. Testing We imported the tests from test_module.py to main.py for your convenience. The tests will run automatically whenever you hit the &quot;run&quot; button. Submitting Copy your project&#39;s URL and submit it to freeCodeCamp. Dataset Source Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. . . Steps . The assignment lists 9 questions, and I&#39;ll just follow the order in which they occur. . The pandas syntax is quite powerful, and a lot of things could happen with just one line of code. When I first started learning pandas, I wish there were some kind of visual aids that could show me the transformation of a dataset from one state to another. Luckily üòô, I came across the Pandas Tutor, which can be very helpful if you are a visual learner like me. With its help, I&#39;ll be providing links to various visualizations in due course. But bear in mind that for the purpose of visualization I only took 20 random records rather than the whole dataset. The point is to visualize a tranformation process, not to actually do data analysis on Pandas Tutor. . Q1: Race counts . To get the counts of each race, we group the dataset by race. Calling the groupby method on df returns a GroupBy object, which cannot be displayed. . df.groupby(&quot;race&quot;) . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f528fe884d0&gt; . In order to show some results, we&#39;ll have to call an additional method on the Groupby object. For instance, the size method returns the numbers we&#39;re looking for. Follow this link to see a visualization of this process. . race_count = df.groupby(&quot;race&quot;).size() race_count . race Amer-Indian-Eskimo 311 Asian-Pac-Islander 1039 Black 3124 Other 271 White 27816 dtype: int64 . I thought race_count would be the answer to the first question, but it didn&#39;t pass the unittest. If you look at the test_module.py file, you&#39;ll see that the expected values of race_count are in ascending order, which is not made clear in the instructions. So, we&#39;ll have to sort the race counts by calling the sort_values method on race_count and setting the value of ascending to be False. . race_count = df.groupby(&quot;race&quot;).size().sort_values(ascending=False) race_count . race White 27816 Black 3124 Asian-Pac-Islander 1039 Amer-Indian-Eskimo 311 Other 271 dtype: int64 . Q2: Average age of men . To get the average age of men, we first group the dataset by sex, then take the age column, and finally call the mean method. Many steps are going on here, follow this link to visualize each of them. . average_age = df.groupby(&quot;sex&quot;)[&quot;age&quot;].mean() average_age . sex Female 36.858230 Male 39.433547 Name: age, dtype: float64 . At this point, average_age is a Series object with two keys Female and Male. So we just need to specify the Male key to get the average age of men. Then we round it to the tenth by calling the builtin function round. . average_age_men = average_age[&quot;Male&quot;] print(f&quot;Before rounding: {average_age_men}&quot;) average_age_men = round(average_age_men, 1) print(f&quot;After rounding: {average_age_men}&quot;) . Before rounding: 39.43354749885268 After rounding: 39.4 . Q3: Percentage of people with a Bachelor degree . To answer this question, we&#39;ll first need the fraction of the number of people with a Bachelor degree over the total number of people. To get the numerator, let&#39;s first create a Boolean filter named edu_filt, which will return True if the value of the education column for a given row is Bachelors or False if otherwise. Then we can just use the bracket notation df[edu_filt] to create a view of the dataset that satisfies the education criterion. Follow this link to see a visualization. . While the original dataset has 32561 records, the filtered one has only 5355 records. Since each record represents a person, these two numbers are precisely what we&#39;re looking for. . edu_filt = df[&quot;education&quot;] == &quot;Bachelors&quot; df[edu_filt] . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 0 39 | State-gov | 77516 | Bachelors | 13 | Never-married | Adm-clerical | Not-in-family | White | Male | 2174 | 0 | 40 | United-States | &lt;=50K | . 1 50 | Self-emp-not-inc | 83311 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 13 | United-States | &lt;=50K | . 4 28 | Private | 338409 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Wife | Black | Female | 0 | 0 | 40 | Cuba | &lt;=50K | . 9 42 | Private | 159449 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 5178 | 0 | 40 | United-States | &gt;50K | . 11 30 | State-gov | 141297 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 32530 35 | ? | 320084 | Bachelors | 13 | Married-civ-spouse | ? | Wife | White | Female | 0 | 0 | 55 | United-States | &gt;50K | . 32531 30 | ? | 33811 | Bachelors | 13 | Never-married | ? | Not-in-family | Asian-Pac-Islander | Female | 0 | 0 | 99 | United-States | &lt;=50K | . 32533 54 | Private | 337992 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | Asian-Pac-Islander | Male | 0 | 0 | 50 | Japan | &gt;50K | . 32536 34 | Private | 160216 | Bachelors | 13 | Never-married | Exec-managerial | Not-in-family | White | Female | 0 | 0 | 55 | United-States | &gt;50K | . 32538 38 | Private | 139180 | Bachelors | 13 | Divorced | Prof-specialty | Unmarried | Black | Female | 15020 | 0 | 45 | United-States | &gt;50K | . 5355 rows √ó 15 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . The number of records in a DataFrame object is accessible via the attribute shape, which returns a tuple of (number of records, number of columns). So here&#39;s how we arrive at the percentage we&#39;re after. . percentage_bachelors = (df[edu_filt].shape[0]/df.shape[0]) * 100 print(f&quot;Before rounding: {percentage_bachelors}&quot;) percentage_bachelors = round(percentage_bachelors, 1) print(f&quot;After rounding: {percentage_bachelors}&quot;) . Before rounding: 16.44605509658794 After rounding: 16.4 . Q4: Percentage of people with advanced education who make more than 50k . As in the previous question, we need to filter the dataset by the education column. But this time around, we&#39;re filtering three values (Bachelors, Masters, and Doctorate for higher degrees) rather than just one. So, it&#39;d be easier to create a Boolean filter by calling the isin method on df while providing a list of higher degrees as its argument. Let&#39;s save the filtered dataset as higher_education. Follow this link to visualize the filtering process. . higher_degrees = [&quot;Bachelors&quot;, &quot;Masters&quot;, &quot;Doctorate&quot;] higher_education_filt = df[&quot;education&quot;].isin(higher_degrees) higher_education = df[higher_education_filt] higher_education . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 0 39 | State-gov | 77516 | Bachelors | 13 | Never-married | Adm-clerical | Not-in-family | White | Male | 2174 | 0 | 40 | United-States | &lt;=50K | . 1 50 | Self-emp-not-inc | 83311 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 13 | United-States | &lt;=50K | . 4 28 | Private | 338409 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Wife | Black | Female | 0 | 0 | 40 | Cuba | &lt;=50K | . 5 37 | Private | 284582 | Masters | 14 | Married-civ-spouse | Exec-managerial | Wife | White | Female | 0 | 0 | 40 | United-States | &lt;=50K | . 8 31 | Private | 45781 | Masters | 14 | Never-married | Prof-specialty | Not-in-family | White | Female | 14084 | 0 | 50 | United-States | &gt;50K | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 32538 38 | Private | 139180 | Bachelors | 13 | Divorced | Prof-specialty | Unmarried | Black | Female | 15020 | 0 | 45 | United-States | &gt;50K | . 32539 71 | ? | 287372 | Doctorate | 16 | Married-civ-spouse | ? | Husband | White | Male | 0 | 0 | 10 | United-States | &gt;50K | . 32544 31 | Private | 199655 | Masters | 14 | Divorced | Other-service | Not-in-family | Other | Female | 0 | 0 | 30 | United-States | &lt;=50K | . 32553 32 | Private | 116138 | Masters | 14 | Never-married | Tech-support | Not-in-family | Asian-Pac-Islander | Male | 0 | 0 | 11 | Taiwan | &lt;=50K | . 32554 53 | Private | 321865 | Masters | 14 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 40 | United-States | &gt;50K | . 7491 rows √ó 15 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . The number of records in higher_education is then the denominator of the fraction we&#39;re looking for. For the numerator, it&#39;ll be the number of records in higher_education that is additionally filtered by the salary column. So my first attempted solution was as follows. . salary_filt = df[&quot;salary&quot;] == &quot;&gt;50K&quot; higher_education_rich = (higher_education[salary_filt].shape[0]/higher_education.shape[0]) * 100 print(f&quot;Before rounding: {higher_education_rich}&quot;) higher_education_rich = round(higher_education_rich, 1) print(f&quot;After rounding: {higher_education_rich}&quot;) . Before rounding: 46.535843011613935 After rounding: 46.5 . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index. . Though I did get the correct percentage, a warning popped up saying that &quot;Boolean Series key will be reindexed to match DataFrame index.&quot; I thought I could just ignore the warning, but the solution ended up failing the unittest üòû. I later figured out what might have caused the warning. It&#39;s probably because the keys of the salary_filt Series don&#39;t match up with the index of the higher_education DataFrame, as shown below. . salary_filt = df[&quot;salary&quot;] == &quot;&gt;50K&quot; print(&quot;Keys of salary_filt &gt;&gt;&gt;&quot;) print(salary_filt.keys()) print(&quot;Index of higher_education &gt;&gt;&gt;&quot;) print(higher_education.index) . Keys of salary_filt &gt;&gt;&gt; RangeIndex(start=0, stop=32561, step=1) Index of higher_education &gt;&gt;&gt; Int64Index([ 0, 1, 4, 5, 8, 9, 11, 12, 19, 20, ... 32530, 32531, 32532, 32533, 32536, 32538, 32539, 32544, 32553, 32554], dtype=&#39;int64&#39;, length=7491) . To fix the problem, I redefined salary_filt based on the filtered higher_education rather than the original df. This way, all the keys of salary_filt would be equal to the index of higher_education. . salary_filt = higher_education[&quot;salary&quot;] == &quot;&gt;50K&quot; print(&quot;Keys of salary_filt match index of higher_education?&quot;) all(salary_filt.keys() == higher_education.index) . Keys of salary_filt match index of higher_education? . True . Once the salary filter is fixed, the warning is gone üòé! . salary_filt = higher_education[&quot;salary&quot;] == &quot;&gt;50K&quot; higher_education_rich = (higher_education[salary_filt].shape[0]/higher_education.shape[0]) * 100 print(f&quot;Before rounding: {higher_education_rich}&quot;) higher_education_rich = round(higher_education_rich, 1) print(f&quot;After rounding: {higher_education_rich}&quot;) . Before rounding: 46.535843011613935 After rounding: 46.5 . Q5: Percentage of people without advanced education who make more than 50k . The logic for this question is pretty much like that for the previous one, except that we&#39;ll first filter out people without higher degrees rather than those with them. To negate the Boolean filter higher_education_filt, we just need to add the ~ symbol in front of it. To see a visualization of the filtering process, follow this link. . lower_education = df[~ higher_education_filt] lower_education . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 2 38 | Private | 215646 | HS-grad | 9 | Divorced | Handlers-cleaners | Not-in-family | White | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 3 53 | Private | 234721 | 11th | 7 | Married-civ-spouse | Handlers-cleaners | Husband | Black | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 6 49 | Private | 160187 | 9th | 5 | Married-spouse-absent | Other-service | Not-in-family | Black | Female | 0 | 0 | 16 | Jamaica | &lt;=50K | . 7 52 | Self-emp-not-inc | 209642 | HS-grad | 9 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 45 | United-States | &gt;50K | . 10 37 | Private | 280464 | Some-college | 10 | Married-civ-spouse | Exec-managerial | Husband | Black | Male | 0 | 0 | 80 | United-States | &gt;50K | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 32556 27 | Private | 257302 | Assoc-acdm | 12 | Married-civ-spouse | Tech-support | Wife | White | Female | 0 | 0 | 38 | United-States | &lt;=50K | . 32557 40 | Private | 154374 | HS-grad | 9 | Married-civ-spouse | Machine-op-inspct | Husband | White | Male | 0 | 0 | 40 | United-States | &gt;50K | . 32558 58 | Private | 151910 | HS-grad | 9 | Widowed | Adm-clerical | Unmarried | White | Female | 0 | 0 | 40 | United-States | &lt;=50K | . 32559 22 | Private | 201490 | HS-grad | 9 | Never-married | Adm-clerical | Own-child | White | Male | 0 | 0 | 20 | United-States | &lt;=50K | . 32560 52 | Self-emp-inc | 287927 | HS-grad | 9 | Married-civ-spouse | Exec-managerial | Wife | White | Female | 15024 | 0 | 40 | United-States | &gt;50K | . 25070 rows √ó 15 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . Now that we&#39;ve got lower_education, we can use a salary filter as before to get a subset of lower_education, which, when divided by the total set, gives us the percentage we&#39;re looking for. . salary_filt = lower_education[&quot;salary&quot;] == &quot;&gt;50K&quot; lower_education_rich = (lower_education[salary_filt].shape[0]/lower_education.shape[0]) * 100 print(f&quot;Before rounding: {lower_education_rich}&quot;) lower_education_rich = round(lower_education_rich, 1) print(f&quot;After rounding: {lower_education_rich}&quot;) . Before rounding: 17.3713601914639 After rounding: 17.4 . Q6: Minimum hours per week . This question is the easiest of all. We simply grab the hours-per-week Series, and find out the minimal value in it by calling the methond min on it. The value is saved as min_work_hours for later use. . min_work_hours = df[&quot;hours-per-week&quot;].min() min_work_hours . 1 . Q7: Percentage of people working minimum hours per week who make more than 50k . Given min_work_hours, we can use a filter, named hour_filt, to filter out people who work minimum hours per week, a subset saved as min_hour_workers. This link shows how the filtered subset is created. . hour_filt = df[&quot;hours-per-week&quot;] == min_work_hours min_hour_workers = df[hour_filt] min_hour_workers . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 189 58 | State-gov | 109567 | Doctorate | 16 | Married-civ-spouse | Prof-specialty | Husband | White | Male | 0 | 0 | 1 | United-States | &gt;50K | . 1036 66 | Self-emp-inc | 150726 | 9th | 5 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 1409 | 0 | 1 | ? | &lt;=50K | . 1262 69 | ? | 195779 | Assoc-voc | 11 | Widowed | ? | Not-in-family | White | Female | 0 | 0 | 1 | United-States | &lt;=50K | . 5590 78 | ? | 363134 | HS-grad | 9 | Widowed | ? | Not-in-family | White | Female | 0 | 0 | 1 | United-States | &lt;=50K | . 5632 45 | ? | 189564 | Masters | 14 | Married-civ-spouse | ? | Wife | White | Female | 0 | 0 | 1 | United-States | &lt;=50K | . 5766 62 | ? | 97231 | Some-college | 10 | Married-civ-spouse | ? | Wife | White | Female | 0 | 0 | 1 | United-States | &lt;=50K | . 5808 76 | ? | 211574 | 10th | 6 | Married-civ-spouse | ? | Husband | White | Male | 0 | 0 | 1 | United-States | &lt;=50K | . 8447 67 | ? | 244122 | Assoc-voc | 11 | Widowed | ? | Not-in-family | White | Female | 0 | 0 | 1 | United-States | &lt;=50K | . 9147 75 | ? | 260543 | 10th | 6 | Widowed | ? | Other-relative | Asian-Pac-Islander | Female | 0 | 0 | 1 | China | &lt;=50K | . 11451 27 | Private | 147951 | HS-grad | 9 | Never-married | Machine-op-inspct | Other-relative | White | Male | 0 | 0 | 1 | United-States | &lt;=50K | . 19337 72 | ? | 76860 | HS-grad | 9 | Married-civ-spouse | ? | Husband | Asian-Pac-Islander | Male | 0 | 0 | 1 | United-States | &lt;=50K | . 19750 23 | Private | 72887 | HS-grad | 9 | Never-married | Craft-repair | Own-child | Asian-Pac-Islander | Male | 0 | 0 | 1 | Vietnam | &lt;=50K | . 20072 65 | ? | 76043 | HS-grad | 9 | Married-civ-spouse | ? | Husband | White | Male | 0 | 0 | 1 | United-States | &gt;50K | . 20909 77 | Self-emp-not-inc | 71676 | Some-college | 10 | Widowed | Adm-clerical | Not-in-family | White | Female | 0 | 1944 | 1 | United-States | &lt;=50K | . 22960 21 | Private | 184135 | HS-grad | 9 | Never-married | Machine-op-inspct | Own-child | Black | Male | 0 | 0 | 1 | United-States | &lt;=50K | . 23536 69 | ? | 320280 | Some-college | 10 | Never-married | ? | Not-in-family | White | Male | 1848 | 0 | 1 | United-States | &lt;=50K | . 24284 57 | Self-emp-not-inc | 56480 | HS-grad | 9 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 1 | United-States | &lt;=50K | . 25078 74 | Private | 260669 | 10th | 6 | Divorced | Other-service | Not-in-family | White | Female | 0 | 0 | 1 | United-States | &lt;=50K | . 29752 69 | ? | 117525 | Assoc-acdm | 12 | Divorced | ? | Unmarried | White | Female | 0 | 0 | 1 | United-States | &lt;=50K | . 32525 81 | ? | 120478 | Assoc-voc | 11 | Divorced | ? | Unmarried | White | Female | 0 | 0 | 1 | ? | &lt;=50K | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . Now, let&#39;s create a salary filter based on min_hour_workers. The number of records in min_hour_workers with the filter divided by the number without it times 100 produces the answer, which is saved as rich_percentage. . salary_filt = min_hour_workers[&quot;salary&quot;] == &quot;&gt;50K&quot; rich_percentage = (min_hour_workers[salary_filt].shape[0]/min_hour_workers.shape[0]) * 100 print(f&quot;Before rounding: {rich_percentage}&quot;) rich_percentage = round(rich_percentage, 1) print(f&quot;After rounding: {rich_percentage}&quot;) . Before rounding: 10.0 After rounding: 10.0 . Q8: Country with the highest percentage of people who make more than 50k . This question is perhaps the most challenging of all. My initial move was to create a GroupBy object first selected by by native-country then by salary, and call the size method on it. This would produce a Series of counts for &lt;=50K and &gt;50K in each country, as follows. Go to this page to see this process in action. . df.groupby([&quot;native-country&quot;, &quot;salary&quot;]).size() . native-country salary ? &lt;=50K 437 &gt;50K 146 Cambodia &lt;=50K 12 &gt;50K 7 Canada &lt;=50K 82 ... United-States &gt;50K 7171 Vietnam &lt;=50K 62 &gt;50K 5 Yugoslavia &lt;=50K 10 &gt;50K 6 Length: 82, dtype: int64 . But then it&#39;s cubersome to get the percentage of the counts for &gt;50K over total counts in each country. The solution I submited for the first time worked, but it was not neat enough. After tackling the problem on a different occasion, I came up with a cleaner solution, to be explained below. . Let&#39;s forget about the breakdown of percentages by countries for the time being, and work out a way to get the percentage for all the dataset. Calling the value_counts method on the salary Series creates another Series with unique values in it as keys and their counts as the corresponding values. Let&#39;s name it salary_counts. . salary_counts = df[&quot;salary&quot;].value_counts() salary_counts . &lt;=50K 24720 &gt;50K 7841 Name: salary, dtype: int64 . Now we can retrieve the number of people who make more than 50K by using the bracket notation. . salary_counts[&quot;&gt;50K&quot;] . 7841 . For the total number of people, we can just call the sum method on the salary_counts Series. . salary_counts.sum() . 32561 . With the two numbers in place, we&#39;re now ready to calculate the percentage, which is named rich_percentage. . rich_percentage = (salary_counts[&quot;&gt;50K&quot;]/salary_counts.sum()) * 100 rich_percentage . 24.080955744602438 . Next, to replicate what we just did, let&#39;s create a function named get_rich_percentage, which takes the salary Series as its argument and returns the percentage of people who make more than 50K. . def get_rich_percentage(salary_series): salary_counts = salary_series.value_counts() res = (salary_counts[&quot;&gt;50K&quot;]/salary_counts.sum()) * 100 return res . A test on the function gives us the expected number. . get_rich_percentage(df[&quot;salary&quot;]) . 24.080955744602438 . Then, we apply the get_rich_percentage function to each group of a GroupBy object named by_country, which is selected by the native-country column. The outcome, saved as rich_percentage_by_country, would have been a Series of percentages across countries, were it not for the KeyError I got üòû. . by_country = df.groupby(&quot;native-country&quot;) rich_percentage_by_country = by_country.apply(lambda group: get_rich_percentage(group[&quot;salary&quot;])) rich_percentage_by_country . KeyError Traceback (most recent call last) /usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance) 3360 try: -&gt; 3361 return self._engine.get_loc(casted_key) 3362 except KeyError as err: /usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc() /usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item() KeyError: &#39;&gt;50K&#39; The above exception was the direct cause of the following exception: KeyError Traceback (most recent call last) &lt;ipython-input-44-34911dd4f740&gt; in &lt;module&gt;() -&gt; 1 df.groupby(&#34;native-country&#34;).apply(lambda group: get_rich_percentage(group[&#34;salary&#34;])) /usr/local/lib/python3.7/dist-packages/pandas/core/groupby/groupby.py in apply(self, func, *args, **kwargs) 1273 with option_context(&#34;mode.chained_assignment&#34;, None): 1274 try: -&gt; 1275 result = self._python_apply_general(f, self._selected_obj) 1276 except TypeError: 1277 # gh-20949 /usr/local/lib/python3.7/dist-packages/pandas/core/groupby/groupby.py in _python_apply_general(self, f, data) 1307 data after applying f 1308 &#34;&#34;&#34; -&gt; 1309 keys, values, mutated = self.grouper.apply(f, data, self.axis) 1310 1311 return self._wrap_applied_output( /usr/local/lib/python3.7/dist-packages/pandas/core/groupby/ops.py in apply(self, f, data, axis) 813 try: 814 sdata = splitter.sorted_data --&gt; 815 result_values, mutated = splitter.fast_apply(f, sdata, group_keys) 816 817 except IndexError: /usr/local/lib/python3.7/dist-packages/pandas/core/groupby/ops.py in fast_apply(self, f, sdata, names) 1358 # must return keys::list, values::list, mutated::bool 1359 starts, ends = lib.generate_slices(self.slabels, self.ngroups) -&gt; 1360 return libreduction.apply_frame_axis0(sdata, f, names, starts, ends) 1361 1362 def _chop(self, sdata: DataFrame, slice_obj: slice) -&gt; DataFrame: /usr/local/lib/python3.7/dist-packages/pandas/_libs/reduction.pyx in pandas._libs.reduction.apply_frame_axis0() &lt;ipython-input-44-34911dd4f740&gt; in &lt;lambda&gt;(group) -&gt; 1 df.groupby(&#34;native-country&#34;).apply(lambda group: get_rich_percentage(group[&#34;salary&#34;])) &lt;ipython-input-42-95f3068a1aa3&gt; in get_rich_percentage(salary_series) 1 def get_rich_percentage(salary_series): 2 salary_counts = salary_series.value_counts() -&gt; 3 res = (salary_counts[&#34;&gt;50K&#34;]/salary_counts.sum()) * 100 4 return res /usr/local/lib/python3.7/dist-packages/pandas/core/series.py in __getitem__(self, key) 940 941 elif key_is_scalar: --&gt; 942 return self._get_value(key) 943 944 if is_hashable(key): /usr/local/lib/python3.7/dist-packages/pandas/core/series.py in _get_value(self, label, takeable) 1049 1050 # Similar to Index.get_value, but we do not fall back to positional -&gt; 1051 loc = self.index.get_loc(label) 1052 return self.index._get_values_for_loc(self, loc, label) 1053 /usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance) 3361 return self._engine.get_loc(casted_key) 3362 except KeyError as err: -&gt; 3363 raise KeyError(key) from err 3364 3365 if is_scalar(key) and isna(key) and not self.hasnans: KeyError: &#39;&gt;50K&#39; . . Apparently, the key &gt;50K is missing from some countries, so let&#39;s revise the get_rich_percentage function to handle the key error. . def get_rich_percentage(salary_series): salary_counts = salary_series.value_counts() if salary_counts.get(&quot;&gt;50K&quot;): res = (salary_counts.get(&quot;&gt;50K&quot;)/salary_counts.sum()) * 100 else: res = 0 return res . Having taken care of the key error, we now got the Series we&#39;re after üôå. The only thing modified this time is to additionally call the sort_values method on the Series so that the numbers are in descending order. And I&#39;m surprised to see Taiwan üáπ among the top five on the list! . by_country = df.groupby(&quot;native-country&quot;) rich_percentage_by_country = by_country.apply(lambda group: get_rich_percentage(group[&quot;salary&quot;])).sort_values(ascending=False) rich_percentage_by_country . native-country Iran 41.860465 France 41.379310 India 40.000000 Taiwan 39.215686 Japan 38.709677 Yugoslavia 37.500000 Cambodia 36.842105 Italy 34.246575 England 33.333333 Canada 32.231405 Germany 32.116788 Philippines 30.808081 Hong 30.000000 Greece 27.586207 China 26.666667 Cuba 26.315789 ? 25.042882 Scotland 25.000000 United-States 24.583476 Hungary 23.076923 Ireland 20.833333 South 20.000000 Poland 20.000000 Thailand 16.666667 Ecuador 14.285714 Jamaica 12.345679 Laos 11.111111 Portugal 10.810811 Trinadad&amp;Tobago 10.526316 Puerto-Rico 10.526316 Haiti 9.090909 El-Salvador 8.490566 Honduras 7.692308 Vietnam 7.462687 Peru 6.451613 Nicaragua 5.882353 Mexico 5.132193 Guatemala 4.687500 Columbia 3.389831 Dominican-Republic 2.857143 Outlying-US(Guam-USVI-etc) 0.000000 Holand-Netherlands 0.000000 dtype: float64 . Finally, we call the idxmax method on rich_percentage_by_country to get the country with the highest percentage, which turns out to be Iran. . highest_earning_country = rich_percentage_by_country.idxmax() highest_earning_country . &#39;Iran&#39; . And with the key Iran, we can easily get the percentage for that country. . highest_earning_country_percentage = rich_percentage_by_country[highest_earning_country] print(f&quot;Before rounding: {highest_earning_country_percentage}&quot;) highest_earning_country_percentage = round(highest_earning_country_percentage, 1) print(f&quot;After rounding: {highest_earning_country_percentage}&quot;) . Before rounding: 41.86046511627907 After rounding: 41.9 . Q9: Most popular occupation for those who make more than 50k in India . This final question is specifically asking about people who make more than 50K in India, so we need two Boolean filters, one based on salary and the other on native-country. Adding the symbol &amp; between the two filters creates the and condition, which filters out the subset we want, named rich_indians üí∞üáÆ . . salary_filt = df[&quot;salary&quot;] == &quot;&gt;50K&quot; indian_filt = df[&quot;native-country&quot;] == &quot;India&quot; rich_indians = df[(indian_filt)&amp;(salary_filt)] rich_indians . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 11 30 | State-gov | 141297 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 968 48 | Private | 164966 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 1327 52 | Private | 168381 | HS-grad | 9 | Widowed | Other-service | Unmarried | Asian-Pac-Islander | Female | 0 | 0 | 40 | India | &gt;50K | . 7258 42 | State-gov | 102343 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 72 | India | &gt;50K | . 7285 54 | State-gov | 93449 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 8124 36 | Private | 172104 | Prof-school | 15 | Never-married | Prof-specialty | Not-in-family | Other | Male | 0 | 0 | 40 | India | &gt;50K | . 9939 43 | Federal-gov | 325706 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 50 | India | &gt;50K | . 10590 35 | Private | 98283 | Prof-school | 15 | Never-married | Prof-specialty | Not-in-family | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 10661 59 | Private | 122283 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 99999 | 0 | 40 | India | &gt;50K | . 10736 30 | Private | 243190 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 20 | India | &gt;50K | . 11260 54 | Private | 225599 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 7298 | 0 | 40 | India | &gt;50K | . 11384 34 | Private | 98283 | Prof-school | 15 | Never-married | Tech-support | Not-in-family | Asian-Pac-Islander | Male | 0 | 1564 | 40 | India | &gt;50K | . 13422 53 | Private | 366957 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | Asian-Pac-Islander | Male | 99999 | 0 | 50 | India | &gt;50K | . 13551 40 | Private | 220977 | Doctorate | 16 | Married-civ-spouse | Exec-managerial | Husband | Asian-Pac-Islander | Male | 3103 | 0 | 40 | India | &gt;50K | . 13862 45 | Private | 209912 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 16017 41 | Private | 207578 | Assoc-acdm | 12 | Married-civ-spouse | Exec-managerial | Husband | Black | Male | 0 | 0 | 50 | India | &gt;50K | . 16778 43 | Private | 242968 | Masters | 14 | Married-civ-spouse | Exec-managerial | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 16923 41 | Private | 143003 | Assoc-voc | 11 | Married-civ-spouse | Other-service | Husband | Asian-Pac-Islander | Male | 7298 | 0 | 60 | India | &gt;50K | . 17259 57 | Self-emp-inc | 123053 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 15024 | 0 | 50 | India | &gt;50K | . 17834 29 | Self-emp-not-inc | 341672 | HS-grad | 9 | Married-spouse-absent | Transport-moving | Other-relative | Asian-Pac-Islander | Male | 0 | 1564 | 50 | India | &gt;50K | . 20417 42 | Self-emp-inc | 23510 | Masters | 14 | Divorced | Exec-managerial | Unmarried | Asian-Pac-Islander | Male | 0 | 2201 | 60 | India | &gt;50K | . 20465 39 | Private | 198654 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 2415 | 67 | India | &gt;50K | . 21128 30 | Private | 122889 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 50 | India | &gt;50K | . 23474 55 | State-gov | 120781 | Doctorate | 16 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 24154 46 | Private | 229737 | Bachelors | 13 | Married-civ-spouse | Sales | Husband | White | Male | 0 | 0 | 50 | India | &gt;50K | . 25739 35 | Self-emp-inc | 79586 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 26305 27 | Private | 207352 | Bachelors | 13 | Married-civ-spouse | Tech-support | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 26356 34 | Private | 99872 | Masters | 14 | Married-civ-spouse | Exec-managerial | Husband | Asian-Pac-Islander | Male | 3103 | 0 | 40 | India | &gt;50K | . 27670 61 | Private | 80896 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 45 | India | &gt;50K | . 28264 51 | Self-emp-not-inc | 120781 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | Other | Male | 99999 | 0 | 70 | India | &gt;50K | . 28433 42 | Private | 198341 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 1902 | 55 | India | &gt;50K | . 28452 53 | Private | 70387 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 4386 | 0 | 40 | India | &gt;50K | . 28557 34 | Private | 165737 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 43 | India | &gt;50K | . 28651 45 | Self-emp-not-inc | 216402 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 50 | India | &gt;50K | . 28798 30 | Self-emp-not-inc | 116666 | Masters | 14 | Divorced | Prof-specialty | Not-in-family | Asian-Pac-Islander | Male | 0 | 0 | 50 | India | &gt;50K | . 30111 41 | Federal-gov | 219155 | Prof-school | 15 | Married-civ-spouse | Prof-specialty | Husband | White | Male | 0 | 0 | 50 | India | &gt;50K | . 30152 48 | Private | 119471 | Doctorate | 16 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | India | &gt;50K | . 30833 25 | Private | 110978 | Assoc-acdm | 12 | Married-civ-spouse | Adm-clerical | Wife | Asian-Pac-Islander | Female | 0 | 0 | 37 | India | &gt;50K | . 31327 38 | State-gov | 125499 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 7688 | 0 | 60 | India | &gt;50K | . 31357 23 | Private | 143003 | Masters | 14 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 1887 | 50 | India | &gt;50K | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . Next, since we&#39;re trying to find out the most popular job among rich Indians, we need a count for each unique job. One of the neatest way to go about this is to call the groupby method on rich_indians and then the size method on the resulting GroupBy object. . ind_occupations = rich_indians.groupby(&quot;occupation&quot;).size() ind_occupations . occupation Adm-clerical 1 Exec-managerial 8 Other-service 2 Prof-specialty 25 Sales 1 Tech-support 2 Transport-moving 1 dtype: int64 . Finally, we call the idxmax method on the ind_occupations Series to get the most popular occupation, which is Prof-specialty. . top_IN_occupation = ind_occupations.idxmax() top_IN_occupation . &#39;Prof-specialty&#39; . Put it together . Here&#39;s the complete code for this project, including the main function calculate_demographic_data and the helper function get_rich_percentage for Q8. . import pandas as pd fpath = &quot;adult.data.csv&quot; def get_rich_percentage(salary_series): salary_counts = salary_series.value_counts() if salary_counts.get(&quot;&gt;50K&quot;): res = (salary_counts.get(&quot;&gt;50K&quot;)/salary_counts.sum()) * 100 else: res = 0 return res def calculate_demographic_data(print_data=True): # Read data from file df = pd.read_csv(fpath) # Q1 How many of each race are represented in this dataset? This should be a Pandas series with race names as the index labels. race_count = df.groupby(&quot;race&quot;).size().sort_values(ascending=False) # Q2 What is the average age of men? average_age_men = df.groupby(&quot;sex&quot;)[&quot;age&quot;].mean()[&quot;Male&quot;] average_age_men = round(average_age_men, 1) # Q3 What is the percentage of people who have a Bachelor&#39;s degree? edu_filt = df[&quot;education&quot;] == &quot;Bachelors&quot; percentage_bachelors = (df[edu_filt].shape[0]/df.shape[0]) * 100 percentage_bachelors = round(percentage_bachelors, 1) # Q4 What percentage of people with advanced education (`Bachelors`, `Masters`, or `Doctorate`) make more than 50K? higher_degrees = [&quot;Bachelors&quot;, &quot;Masters&quot;, &quot;Doctorate&quot;] higher_education_filt = df[&quot;education&quot;].isin(higher_degrees) higher_education = df[higher_education_filt] salary_filt = higher_education[&quot;salary&quot;] == &quot;&gt;50K&quot; higher_education_rich = (higher_education[salary_filt].shape[0]/higher_education.shape[0]) * 100 higher_education_rich = round(higher_education_rich, 1) # Q5 What percentage of people without advanced education make more than 50K? lower_education = df[~ higher_education_filt] salary_filt = lower_education[&quot;salary&quot;] == &quot;&gt;50K&quot; lower_education_rich = (lower_education[salary_filt].shape[0]/lower_education.shape[0]) * 100 lower_education_rich = round(lower_education_rich, 1) # Q6 What is the minimum number of hours a person works per week (hours-per-week feature)? min_work_hours = df[&quot;hours-per-week&quot;].min() # Q7 What percentage of the people who work the minimum number of hours per week have a salary of &gt;50K? hour_filt = df[&quot;hours-per-week&quot;] == min_work_hours min_hour_workers = df[hour_filt] salary_filt = min_hour_workers[&quot;salary&quot;] == &quot;&gt;50K&quot; rich_percentage = (min_hour_workers[salary_filt].shape[0]/min_hour_workers.shape[0]) * 100 rich_percentage = round(rich_percentage, 1) # Q8 What country has the highest percentage of people that earn &gt;50K and what is that percentage? by_country = df.groupby(&quot;native-country&quot;) rich_percentage_by_country = by_country.apply(lambda group: get_rich_percentage(group[&quot;salary&quot;])).sort_values(ascending=False) highest_earning_country = rich_percentage_by_country.idxmax() highest_earning_country_percentage = rich_percentage_by_country[highest_earning_country] highest_earning_country_percentage = round(highest_earning_country_percentage, 1) # Q9 Identify the most popular occupation for those who earn &gt;50K in India. salary_filt = df[&quot;salary&quot;] == &quot;&gt;50K&quot; indian_filt = df[&quot;native-country&quot;] == &quot;India&quot; rich_indians = df[(indian_filt)&amp;(salary_filt)] ind_occupations = rich_indians.groupby(&quot;occupation&quot;).size() top_IN_occupation = ind_occupations.idxmax() # DO NOT MODIFY BELOW THIS LINE if print_data: print(&quot;Number of each race: n&quot;, race_count) print(&quot;Average age of men:&quot;, average_age_men) print(f&quot;Percentage with Bachelors degrees: {percentage_bachelors}%&quot;) print( f&quot;Percentage with higher education that earn &gt;50K: {higher_education_rich}%&quot; ) print( f&quot;Percentage without higher education that earn &gt;50K: {lower_education_rich}%&quot; ) print(f&quot;Min work time: {min_work_hours} hours/week&quot;) print( f&quot;Percentage of rich among those who work fewest hours: {rich_percentage}%&quot; ) print(&quot;Country with highest percentage of rich:&quot;, highest_earning_country) print( f&quot;Highest percentage of rich people in country: {highest_earning_country_percentage}%&quot; ) print(&quot;Top occupations in India:&quot;, top_IN_occupation) return { &#39;race_count&#39;: race_count, &#39;average_age_men&#39;: average_age_men, &#39;percentage_bachelors&#39;: percentage_bachelors, &#39;higher_education_rich&#39;: higher_education_rich, &#39;lower_education_rich&#39;: lower_education_rich, &#39;min_work_hours&#39;: min_work_hours, &#39;rich_percentage&#39;: rich_percentage, &#39;highest_earning_country&#39;: highest_earning_country, &#39;highest_earning_country_percentage&#39;: highest_earning_country_percentage, &#39;top_IN_occupation&#39;: top_IN_occupation } . Now it&#39;s time to harvest the fruits of our labor by calling the calculate_demographic_data function! And it worked as expected! üéä„äó . calculate_demographic_data() . Number of each race: race White 27816 Black 3124 Asian-Pac-Islander 1039 Amer-Indian-Eskimo 311 Other 271 dtype: int64 Average age of men: 39.4 Percentage with Bachelors degrees: 16.4% Percentage with higher education that earn &gt;50K: 46.5% Percentage without higher education that earn &gt;50K: 17.4% Min work time: 1 hours/week Percentage of rich among those who work fewest hours: 10.0% Country with highest percentage of rich: Iran Highest percentage of rich people in country: 41.9% Top occupations in India: Prof-specialty . {&#39;average_age_men&#39;: 39.4, &#39;higher_education_rich&#39;: 46.5, &#39;highest_earning_country&#39;: &#39;Iran&#39;, &#39;highest_earning_country_percentage&#39;: 41.9, &#39;lower_education_rich&#39;: 17.4, &#39;min_work_hours&#39;: 1, &#39;percentage_bachelors&#39;: 16.4, &#39;race_count&#39;: race White 27816 Black 3124 Asian-Pac-Islander 1039 Amer-Indian-Eskimo 311 Other 271 dtype: int64, &#39;rich_percentage&#39;: 10.0, &#39;top_IN_occupation&#39;: &#39;Prof-specialty&#39;} . Recap . Once you finish the freeCodeCamp project Demographic Data Analyzer, you&#39;ll become comfortable doing data analysis with the pandas library while manipulating pandas objects like Series, DataFrame, and GroupBy. Starting from the next project, we&#39;ll enter the realm of visualization, drawing figures with the help of libraries like matplotlib and seaborn. Until then üëã! .",
            "url": "https://howard-haowen.github.io/blog.ai/pandas/freecodecamp/certificate/2022/03/09/freeCodeCamp-project-Demographic-Data-Analyzer.html",
            "relUrl": "/pandas/freecodecamp/certificate/2022/03/09/freeCodeCamp-project-Demographic-Data-Analyzer.html",
            "date": " ‚Ä¢ Mar 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "freeCodeCamp project Mean-Variance-Standard Deviation Calculator",
            "content": ". . Intro . freeCodeCamp is a website where one can learn coding for free. Once you log in, you&#39;ll have free access to more than 8,000 tutorials üòá. What I like best about it is that you can earn free certifications by completing courses on various topics. These courses are pretty intense, and it&#39;ll take you at least 300 hours to finish any of them. For instance, I started Data Analysis with Python, Scientific Computing with Python, and Machine Learning with Python back in 2020, but didn&#39;t actually get any of them done until recently. . The major challenge is that all these courses involve some coding projects, which require much more time and attention than just watch tutorial videos and do multiple-choice quizzes. But I finally nailed Data Analysis with Python and earned this certification üôå! . . This particular course covers the following five projects: . Mean-Variance-Standard Deviation Calculator | Demographic Data Analyzer | Medical Data Visualizer | Page View Time Series Visualizer | Sea Level Predictor | . The tools you&#39;ll be using are numpy and pandas for data wrangling and matplotlib and seaborn for visualization. I&#39;ve learned a lot By working on these projects, which are really worth my time. So I plan to document the learning process and write a post on each of the projects. . I&#39;m sure there are already some solutions somewhere on the internet, but I&#39;ll write down mine anyway and focus more on the thinking process than final codes. The only help I turned to was official documentations of Python libraries and some snippets on stackoverFlow. . With that, let&#39;s start with Mean-Variance-Standard Deviation Calculator, which is the easiest one of the five. . Import . For this project, we need nothing but the numpy library, which is preinstalled on Colab. This graph that I found on techvidvan.com nicely summarizes 10 common uses of numpy. . . import numpy as np . Task instructions . Let&#39;s first clone the project repo and change the current directory to boilerplate-mean-variance-standard-deviation-calculator, where the README.md file contains the task instructions. . !git clone https://github.com/freeCodeCamp/boilerplate-mean-variance-standard-deviation-calculator %cd boilerplate-mean-variance-standard-deviation-calculator readme = !cat README.md print(readme) . Cloning into &#39;boilerplate-mean-variance-standard-deviation-calculator&#39;... remote: Enumerating objects: 14, done. remote: Counting objects: 100% (14/14), done. remote: Compressing objects: 100% (12/12), done. remote: Total 14 (delta 3), reused 10 (delta 1), pack-reused 0 Unpacking objects: 100% (14/14), done. /content/boilerplate-mean-variance-standard-deviation-calculator [&#39;### Assignment&#39;, &#39;&#39;, &#39;Create a function named `calculate()` in `mean_var_std.py` that uses Numpy to output the mean, variance, standard deviation, max, min, and sum of the rows, columns, and elements in a 3 x 3 matrix. &#39;, &#39;&#39;, &#39;The input of the function should be a list containing 9 digits. The function should convert the list into a 3 x 3 Numpy array, and then return a dictionary containing the mean, variance, standard deviation, max, min, and sum along both axes and for the flattened matrix. &#39;, &#39;&#39;, &#39;The returned dictionary should follow this format:&#39;, &#39;py&#39;, &#39;{&#39;, &#34; &#39;mean&#39;: [axis1, axis2, flattened],&#34;, &#34; &#39;variance&#39;: [axis1, axis2, flattened],&#34;, &#34; &#39;standard deviation&#39;: [axis1, axis2, flattened],&#34;, &#34; &#39;max&#39;: [axis1, axis2, flattened],&#34;, &#34; &#39;min&#39;: [axis1, axis2, flattened],&#34;, &#34; &#39;sum&#39;: [axis1, axis2, flattened]&#34;, &#39;}&#39;, &#39;&#39;, &#39;&#39;, &#39;If a list containing less than 9 elements is passed into the function, it should raise a `ValueError` exception with the message: &#34;List must contain nine numbers.&#34; The values in the returned dictionary should be lists and not Numpy arrays.&#39;, &#39;&#39;, &#39;For example, `calculate([0,1,2,3,4,5,6,7,8])` should return:&#39;, &#39;py&#39;, &#39;{&#39;, &#34; &#39;mean&#39;: [[3.0, 4.0, 5.0], [1.0, 4.0, 7.0], 4.0], &#34;, &#34; &#39;variance&#39;: [[6.0, 6.0, 6.0], [0.6666666666666666, 0.6666666666666666, 0.6666666666666666], 6.666666666666667], &#34;, &#34; &#39;standard deviation&#39;: [[2.449489742783178, 2.449489742783178, 2.449489742783178], [0.816496580927726, 0.816496580927726, 0.816496580927726], 2.581988897471611],&#34;, &#34; &#39;max&#39;: [[6, 7, 8], [2, 5, 8], 8],&#34;, &#34; &#39;min&#39;: [[0, 1, 2], [0, 3, 6], 0],&#34;, &#34; &#39;sum&#39;: [[9, 12, 15], [3, 12, 21], 36]&#34;, &#39;}&#39;, &#39;&#39;, &#39;&#39;, &#39;The unit tests for this project are in `test_module.py`.&#39;, &#39;&#39;, &#39;### Development&#39;, &#39;&#39;, &#39;For development, you can use `main.py` to test your `calculate()` function. Click the &#34;run&#34; button and `main.py` will run.&#39;, &#39;&#39;, &#39;### Testing &#39;, &#39;&#39;, &#39;We imported the tests from `test_module.py` to `main.py` for your convenience. The tests will run automatically whenever you hit the &#34;run&#34; button.&#39;, &#39;&#39;, &#39;### Submitting&#39;, &#39;&#39;, &#34;Copy your project&#39;s URL and submit it to freeCodeCamp.&#34;] . . But the plain text isn&#39;t pretty and hard to read. So I&#39;ll use the rich library to make it look rich üí∞. . !pip install rich . Collecting rich Downloading rich-11.2.0-py3-none-any.whl (217 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217 kB 22.8 MB/s Collecting commonmark&lt;0.10.0,&gt;=0.9.0 Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51 kB 4.6 MB/s Collecting colorama&lt;0.5.0,&gt;=0.4.0 Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB) Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich) (2.6.1) Requirement already satisfied: typing-extensions&lt;5.0,&gt;=3.7.4 in /usr/local/lib/python3.7/dist-packages (from rich) (3.10.0.2) Installing collected packages: commonmark, colorama, rich Successfully installed colorama-0.4.4 commonmark-0.9.1 rich-11.2.0 . . Now with the helper function show_readme, we can render the instructions in a beautiful format. . from rich.console import Console from rich.markdown import Markdown def show_readme(): console = Console() with open(&quot;README.md&quot;) as readme: markdown = Markdown(readme.read()) console.print(markdown) . Assignment . Here&#39;s original text for the assignment. . show_readme() . Assignment Create a function named calculate() in mean_var_std.py that uses Numpy to output the mean, variance, standard deviation, max, min, and sum of the rows, columns, and elements in a 3 x 3 matrix. The input of the function should be a list containing 9 digits. The function should convert the list into a 3 x 3 Numpy array, and then return a dictionary containing the mean, variance, standard deviation, max, min, and sum along both axes and for the flattened matrix. The returned dictionary should follow this format: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ { ‚îÇ ‚îÇ &#39;mean&#39;: [axis1, axis2, flattened], ‚îÇ ‚îÇ &#39;variance&#39;: [axis1, axis2, flattened], ‚îÇ ‚îÇ &#39;standard deviation&#39;: [axis1, axis2, flattened], ‚îÇ ‚îÇ &#39;max&#39;: [axis1, axis2, flattened], ‚îÇ ‚îÇ &#39;min&#39;: [axis1, axis2, flattened], ‚îÇ ‚îÇ &#39;sum&#39;: [axis1, axis2, flattened] ‚îÇ ‚îÇ } ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò If a list containing less than 9 elements is passed into the function, it should raise a ValueError exception with the message: &quot;List must contain nine numbers.&quot; The values in the returned dictionary should be lists and not Numpy arrays. For example, calculate([0,1,2,3,4,5,6,7,8]) should return: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ { ‚îÇ ‚îÇ &#39;mean&#39;: [[3.0, 4.0, 5.0], [1.0, 4.0, 7.0], 4.0], ‚îÇ ‚îÇ &#39;variance&#39;: [[6.0, 6.0, 6.0], [0.6666666666666666, 0.6666666666666666, ‚îÇ ‚îÇ 0.6666666666666666], 6.666666666666667], ‚îÇ ‚îÇ &#39;standard deviation&#39;: [[2.449489742783178, 2.449489742783178, 2.449489742783178], ‚îÇ ‚îÇ [0.816496580927726, 0.816496580927726, 0.816496580927726], 2.581988897471611], ‚îÇ ‚îÇ &#39;max&#39;: [[6, 7, 8], [2, 5, 8], 8], ‚îÇ ‚îÇ &#39;min&#39;: [[0, 1, 2], [0, 3, 6], 0], ‚îÇ ‚îÇ &#39;sum&#39;: [[9, 12, 15], [3, 12, 21], 36] ‚îÇ ‚îÇ } ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò The unit tests for this project are in test_module.py. Development For development, you can use main.py to test your calculate() function. Click the &quot;run&quot; button and main.py will run. Testing We imported the tests from test_module.py to main.py for your convenience. The tests will run automatically whenever you hit the &quot;run&quot; button. Submitting Copy your project&#39;s URL and submit it to freeCodeCamp. . Steps . Here&#39;re the steps that I&#39;m gonna go through: . I&#39;ll create a variable named matrix, which holds 9 numbers in a 3x3 matrix. | I&#39;ll calculate the values for summation on the matrix to see if everything works as expected. | I&#39;ll wrap the calculating process in a general function named get_results. | Create a matrix . One of the easiest ways to create a matrix is to call the np.array function with a list of numbers, which is turned into a NumPy one-dimentional array. Then you just call the reshape method on it to make it two-dimentional. . input = [num for num in range(9)] arr = np.array(input) matrix = arr.reshape(3, 3) . print(f&quot;arr: n{arr}&quot;) print(f&quot;matrix: n{matrix}&quot;) . arr: [0 1 2 3 4 5 6 7 8] matrix: [[0 1 2] [3 4 5] [6 7 8]] . Calculate the sum . To get the sum of all the numbers in matrix, just call the sum method on it. . flattened_sum = matrix.sum() flattened_sum . 36 . That was easy, but what comes next can be confusing. We&#39;re supposed to do summation along two axes, so let&#39;s first figure out whether axis1 contains results calcuated along columns or rows. It turns out that we&#39;ll get the sum of all the numbers along a column when we call the sum method on matrix with the axis argument specified as 0 and along a row when the axis value is 1. . axis1_sum = matrix.sum(axis=0) axis2_sum = matrix.sum(axis=1) print(f&quot;axis1_sum: n{axis1_sum}&quot;) print(f&quot;axis2_sum: n{axis2_sum}&quot;) . axis1_sum: [ 9 12 15] axis2_sum: [ 3 12 21] . The following graph illustrates the idea better than what I just said. . . Notice that axis1_sum is a NumPy array, but we&#39;re supposed to return it as a list. So we call the tolist method on axis1_sum to do the conversion. . old_type = type(axis1_sum) axis1_sum = axis1_sum.tolist() new_type = type(axis1_sum) print(f&quot;Old type: {old_type}&quot;) print(f&quot;New type: {new_type}&quot;) . Old type: &lt;class &#39;numpy.ndarray&#39;&gt; New type: &lt;class &#39;list&#39;&gt; . Wrap calculations in a function . Now that we&#39;ve figured out how to get the values of axis1, axis2, and flattened for summation, we can just repeat the same process for the other 5 mathematical operations. So let&#39;s wrap the calculating process in a general function called get_results. It takes two arguments, one being op for the name of the mathematical operation to be done, and the other matrix. Luckily üòÄ, the numpy API is quite consistent across the 6 operations, so for each mathematical operation we just need to change the method name accordingly. . def get_results(op, matrix): if op == &quot;sum&quot;: axis1 = matrix.sum(axis=0).tolist() axis2 = matrix.sum(axis=1).tolist() flattened = matrix.sum() elif op == &quot;min&quot;: axis1 = matrix.min(axis=0).tolist() axis2 = matrix.min(axis=1).tolist() flattened = matrix.min() elif op == &quot;max&quot;: axis1 = matrix.max(axis=0).tolist() axis2 = matrix.max(axis=1).tolist() flattened = matrix.max() elif op == &quot;std&quot;: axis1 = matrix.std(axis=0).tolist() axis2 = matrix.std(axis=1).tolist() flattened = matrix.std() elif op == &quot;var&quot;: axis1 = matrix.var(axis=0).tolist() axis2 = matrix.var(axis=1).tolist() flattened = matrix.var() elif op == &quot;mean&quot;: axis1 = matrix.mean(axis=0).tolist() axis2 = matrix.mean(axis=1).tolist() flattened = matrix.mean() return [axis1, axis2, flattened] . . Now let&#39;s test the function with sum and var, for the sum and variance respectively. . get_results(&quot;sum&quot;, matrix) . [[9, 12, 15], [3, 12, 21], 36] . get_results(&quot;var&quot;, matrix) . [[6.0, 6.0, 6.0], [0.6666666666666666, 0.6666666666666666, 0.6666666666666666], 6.666666666666667] . The last thing that needs to be taken care of is error handling. We just need to raise ValueError with a specific message when the length of the input list is not 9. . def calculate(mylist): if len(mylist) != 9: raise ValueError(&quot;List must contain nine numbers.&quot;) else: pass . mylist = [1, 2, 3] calculate(mylist) . ValueError Traceback (most recent call last) &lt;ipython-input-3-6ee8fb3f03c3&gt; in &lt;module&gt;() 6 7 mylist = [1, 2, 3] -&gt; 8 calculate(mylist) &lt;ipython-input-3-6ee8fb3f03c3&gt; in calculate(mylist) 1 def calculate(mylist): 2 if len(mylist) != 9: -&gt; 3 raise ValueError(&#34;List must contain nine numbers.&#34;) 4 else: 5 pass ValueError: List must contain nine numbers. . Put it together . Finally, our final codes should be saved as mean_var_std.py and look like this: . import numpy as np def get_results(op, matrix): if op == &quot;sum&quot;: axis1 = matrix.sum(axis=0).tolist() axis2 = matrix.sum(axis=1).tolist() flattened = matrix.sum() elif op == &quot;min&quot;: axis1 = matrix.min(axis=0).tolist() axis2 = matrix.min(axis=1).tolist() flattened = matrix.min() elif op == &quot;max&quot;: axis1 = matrix.max(axis=0).tolist() axis2 = matrix.max(axis=1).tolist() flattened = matrix.max() elif op == &quot;std&quot;: axis1 = matrix.std(axis=0).tolist() axis2 = matrix.std(axis=1).tolist() flattened = matrix.std() elif op == &quot;var&quot;: axis1 = matrix.var(axis=0).tolist() axis2 = matrix.var(axis=1).tolist() flattened = matrix.var() elif op == &quot;mean&quot;: axis1 = matrix.mean(axis=0).tolist() axis2 = matrix.mean(axis=1).tolist() flattened = matrix.mean() return [axis1, axis2, flattened] def calculate(mylist): if len(mylist) != 9: raise ValueError(&quot;List must contain nine numbers.&quot;) else: matrix = np.array(mylist).reshape(3, 3) calculations = { &#39;mean&#39;: get_results(&#39;mean&#39;, matrix), &#39;variance&#39;: get_results(&#39;var&#39;, matrix), &#39;standard deviation&#39;: get_results(&#39;std&#39;, matrix), &#39;max&#39;: get_results(&#39;max&#39;, matrix), &#39;min&#39;: get_results(&#39;min&#39;, matrix), &#39;sum&#39;: get_results(&#39;sum&#39;, matrix), } return calculations . . Let&#39;s test the calculate function to ensure it works just as expected. And it did üôå! . input = [0,1,2,3,4,5,6,7,8] calculate(input) . {&#39;max&#39;: [[6, 7, 8], [2, 5, 8], 8], &#39;mean&#39;: [[3.0, 4.0, 5.0], [1.0, 4.0, 7.0], 4.0], &#39;min&#39;: [[0, 1, 2], [0, 3, 6], 0], &#39;standard deviation&#39;: [[2.449489742783178, 2.449489742783178, 2.449489742783178], [0.816496580927726, 0.816496580927726, 0.816496580927726], 2.581988897471611], &#39;sum&#39;: [[9, 12, 15], [3, 12, 21], 36], &#39;variance&#39;: [[6.0, 6.0, 6.0], [0.6666666666666666, 0.6666666666666666, 0.6666666666666666], 6.666666666666667]} . Recap . After you finish the freeCodeCamp project Mean-Variance-Standard Deviation Calculator, you&#39;ll be able to do some basic math using the numpy library. This first project is really just a warm-up, and it gets more challenging as we move on to the other four, which I&#39;ll blog (definitely not brag) about soon üëº. .",
            "url": "https://howard-haowen.github.io/blog.ai/numpy/freecodecamp/certificate/2022/02/25/freeCodeCamp-project-Mean-Variance-Standard-Deviation-Calculator.html",
            "relUrl": "/numpy/freecodecamp/certificate/2022/02/25/freeCodeCamp-project-Mean-Variance-Standard-Deviation-Calculator.html",
            "date": " ‚Ä¢ Feb 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Scraping Dcard with cloudscraper",
            "content": ". Intro . Dcard is a popular social networking platform in Taiwan, and as such offers great resources for text mining and NLP model building. Our goals in this post is to scrape data from Dcard at regular intervals and persist it to a SQL database without duplicating the same records. We&#39;ll be leveraging the Dcard API v2 as well as the following libraries, which are not included in Python&#39;s standard library: . cloudscraper: for bypassing Cloudflare&#39;s anti-bot page | pandas: for organizing the scraped data into a tabular format, which can then be easily saved as a SQL table | schedule: for scheduling tasks | . Installing dependencies . Since pandas is preinstalled on Colab, we only need to install cloudscraper and schedule. . !pip install cloudscraper !pip install schedule #!pip pandas # uncomment this if pandas is not installed in your environment . Collecting cloudscraper Downloading cloudscraper-1.2.58-py2.py3-none-any.whl (96 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96 kB 4.5 MB/s Requirement already satisfied: requests&gt;=2.9.2 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.23.0) Requirement already satisfied: pyparsing&gt;=2.4.7 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.4.7) Collecting requests-toolbelt&gt;=0.9.1 Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54 kB 2.4 MB/s Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.9.2-&gt;cloudscraper) (2021.5.30) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.9.2-&gt;cloudscraper) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.9.2-&gt;cloudscraper) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.9.2-&gt;cloudscraper) (1.24.3) Installing collected packages: requests-toolbelt, cloudscraper Successfully installed cloudscraper-1.2.58 requests-toolbelt-0.9.1 Collecting schedule Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB) Installing collected packages: schedule Successfully installed schedule-1.1.0 . . Let&#39;s check out the Python version on Colab. . !python --version . Python 3.7.11 . Testing the Dcard API . Let&#39;s first test the Dcard API v2 with cloudscraper, the syntax of which is much like that of requests. The only difference is that we&#39;ll have to first create a scraper instance with cloudscraper.create_scraper(). For each HTTP request, we&#39;ll get a batch of 30 posts. The forum variable in the URL is the English name of a forum, and we&#39;ll first test the one named stock. . import cloudscraper forum = &quot;stock&quot; URL = f&quot;https://www.dcard.tw/service/api/v2/forums/{forum}/posts&quot; scraper = cloudscraper.create_scraper() batch = scraper.get(URL).json() len(batch) . 30 . Here&#39;s what one single post looks like. . import pprint pprint.pprint(batch[0]) . {&#39;activityAvatar&#39;: &#39;&#39;, &#39;anonymousDepartment&#39;: True, &#39;anonymousSchool&#39;: False, &#39;categories&#39;: [&#39;Ë´ãÁõä&#39;], &#39;commentCount&#39;: 0, &#39;createdAt&#39;: &#39;2021-09-12T14:22:22.276Z&#39;, &#39;customStyle&#39;: None, &#39;elapsedTime&#39;: 515, &#39;excerpt&#39;: &#39;Â¶ÇÈ°åÔºåÊàëÁü•ÈÅìÂÆòËÇ°Â≠òËÇ°È¶ñÊé®ÂÖÜË±êÈáëÔºåÈÖçÊÅØÂèàÈÖçËÇ°ÔºåÂèØÊòØÊúÉÈÖçËÇ°ÁöÑËÇ°Á•®‰ª£Ë°®ËÇ°Êú¨Ë¶ÅÂæàÂ§ßÔºå‰∏îÊØèÂπ¥Áç≤Âà©Â¶ÇÊûúÊ≤íË∑ü‰∏äÁöÑË©±ÔºåEPSÊúÉÊéâÔºåËÇ°ÂÉπ‰πüÊúÉË∑üËëóÊéâÔºåÂÆòËÇ°Áç≤Âà©Ë∑üÂçÅÂπ¥ÂâçÊØîÊúâÂ¶Ç‰∏ÄÊî§Ê≠ªÊ∞¥ÔºåÂ∞èÂºüÊàëÁúüÁöÑ‰∏çÊõâÂæóÁÇ∫‰ªÄÈ∫ºÂ≠òËÇ°ÊØîËµ∑Â≠òÊ∞ëÁáüÈáë&#39;, &#39;excerptComments&#39;: [], &#39;forumAlias&#39;: &#39;stock&#39;, &#39;forumId&#39;: &#39;2fb88b62-aa28-4b18-af51-dda08dd037a9&#39;, &#39;forumName&#39;: &#39;ËÇ°Á•®&#39;, &#39;gender&#39;: &#39;M&#39;, &#39;hidden&#39;: False, &#39;id&#39;: 236953704, &#39;isModerator&#39;: False, &#39;isSuspiciousAccount&#39;: False, &#39;layout&#39;: &#39;classic&#39;, &#39;likeCount&#39;: 0, &#39;media&#39;: [], &#39;mediaMeta&#39;: [], &#39;memberType&#39;: &#39;&#39;, &#39;meta&#39;: {&#39;layout&#39;: &#39;classic&#39;}, &#39;nsfw&#39;: False, &#39;pinned&#39;: False, &#39;postAvatar&#39;: &#39;&#39;, &#39;reactions&#39;: [], &#39;replyId&#39;: None, &#39;replyTitle&#39;: None, &#39;reportReason&#39;: &#39;&#39;, &#39;reportReasonText&#39;: &#39;&#39;, &#39;school&#39;: &#39;ÂºòÂÖâÁßëÊäÄÂ§ßÂ≠∏&#39;, &#39;spoilerAlert&#39;: False, &#39;tags&#39;: [], &#39;title&#39;: &#39;#Ë´ãÁõä #Ë´ãÁõä ÈáëËûçËÇ°Â≠òËÇ°ÁñëÂïè ÂÆòËÇ° Ê∞ëËÇ°&#39;, &#39;topics&#39;: [&#39;Ë´ãÁõä&#39;, &#39;ÈáëËûç&#39;, &#39;ÊäïË≥á&#39;, &#39;ÂÆòËÇ°&#39;, &#39;Ê∞ëÁáüÈáëÊéß&#39;], &#39;totalCommentCount&#39;: 0, &#39;updatedAt&#39;: &#39;2021-09-12T14:22:22.276Z&#39;, &#39;verifiedBadge&#39;: False, &#39;withImages&#39;: False, &#39;withNickname&#39;: False, &#39;withVideos&#39;: False} . . Parsing the JSON response . Then we&#39;ll parse the JSON response to get the data we&#39;re interested in, including title, createdAt, categories, excerpt, and topics. . cols = [&#39;title&#39;, &#39;createdAt&#39;, &#39;categories&#39;, &#39;excerpt&#39;, &#39;topics&#39;] title = [item.get(cols[0]) for item in batch] createdAt = [item.get(cols[1]) for item in batch] categories = [item.get(cols[2]) for item in batch] excerpt = [item.get(cols[3]) for item in batch] topics = [item.get(cols[4]) for item in batch] . For instance, the topics column contains a list of topic terms for each post, but the list may be empty. . topics . [[&#39;Ë´ãÁõä&#39;, &#39;ÈáëËûç&#39;, &#39;ÊäïË≥á&#39;, &#39;ÂÆòËÇ°&#39;, &#39;Ê∞ëÁáüÈáëÊéß&#39;], [&#39;ÂàÜÊûê&#39;, &#39;Âè∞ËÇ°&#39;, &#39;Áï∂Ê≤ñ&#39;, &#39;Ê≥¢ÊÆµ&#39;, &#39;ÊäÄË°ìÂàÜÊûê&#39;], [&#39;ÊäïË≥á&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁêÜË≤°&#39;, &#39;Âè∞ËÇ°&#39;, &#39;ËÇ°Â∏Ç&#39;], [&#39;Ë´ãÁõä&#39;], [&#39;ÂÖá&#39;, &#39;Èü≠Ëèú&#39;], [&#39;ÊäïË≥á&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁæéËÇ°&#39;, &#39;ETF&#39;, &#39;Êñ∞ËÅû&#39;], [&#39;ÊäïË≥á&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁæéËÇ°&#39;, &#39;ËÇ°Â∏Ç&#39;, &#39;Êñ∞Êâã&#39;], [&#39;ËÇ°Á•®&#39;, &#39;ÊäïË≥á&#39;, &#39;ÁêÜË≤°&#39;, &#39;Âè∞ËÇ°&#39;, &#39;Áï∂Ê≤ñ&#39;], [&#39;ÊäïË≥á&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁêÜË≤°&#39;, &#39;Âè∞ËÇ°&#39;, &#39;ÁîüÊ¥ª&#39;], [&#39;ËÇ°Á•®&#39;, &#39;ÁæéËÇ°&#39;, &#39;ÊäÄË°ìÂàÜÊûê&#39;, &#39;ÁãºÁéã&#39;], [&#39;Ë´ãÁõä&#39;, &#39;Êñ∞ËÅû&#39;, &#39;ÂΩ±Èüø&#39;, &#39;ÊäïË≥á&#39;, &#39;ËÇ°Á•®&#39;], [&#39;ÊäïË≥á&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁêÜË≤°&#39;, &#39;Âè∞ËÇ°&#39;, &#39;ËÇ°Â∏Ç&#39;], [&#39;ÊôÇ‰∫ã&#39;, &#39;ÂàÜ‰∫´&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁêÜË≤°&#39;, &#39;Êñ∞ËÅû&#39;], [&#39;ËÄÅÂ∏´&#39;, &#39;Áõ¥Êí≠&#39;, &#39;Á≠ÜË®ò&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÊäïË≥á&#39;], [&#39;Êµ∑Â§ñ&#39;, &#39;Âà∏ÂïÜ&#39;, &#39;Ê≥ïÂæã&#39;], [&#39;ÂàÜ‰∫´&#39;, &#39;ÈáëËûç&#39;, &#39;ÊäïË≥á&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁêÜË≤°&#39;], [&#39;ÊäïË≥á&#39;, &#39;app&#39;], [&#39;ËÇ°Á•®&#39;], [&#39;ËÇ°Á•®&#39;, &#39;ÂàÜ‰∫´&#39;], [&#39;ÂàÜ‰∫´&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÊäïË≥á&#39;, &#39;Áï∂Ê≤ñ&#39;, &#39;ÂàÜÊûê&#39;], [&#39;ËÇ°ÊÅØ&#39;, &#39;ÂàÜ‰∫´&#39;, &#39;ËÇ°Âà©&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÊäïË≥á&#39;], [&#39;ÊäïË≥á&#39;, &#39;ÁêÜË≤°&#39;, &#39;Âè∞ËÇ°&#39;, &#39;ËÇ°Á•®&#39;, &#39;ËÇ°Â∏Ç&#39;], [&#39;ËÇ°Á•®&#39;, &#39;Âè∞ËÇ°&#39;], [&#39;ÊäïË≥á&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁêÜË≤°&#39;, &#39;Âè∞ËÇ°&#39;, &#39;ËÇ°Â∏Ç&#39;], [&#39;ËÇ°Â∏Ç&#39;, &#39;Áï∂Ê≤ñ&#39;, &#39;Ê≥¢ÊÆµ&#39;, &#39;Â§ßÁõ§&#39;], [&#39;etf&#39;, &#39;ÊäïË≥á&#39;], [&#39;ÁæéËÇ°&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÊäÄË°ìÂàÜÊûê&#39;, &#39;ÁãºÁéã&#39;], [&#39;ÁæéËÇ°&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÁêÜË≤°&#39;, &#39;NVIDIA&#39;, &#39;ÊäïË≥á&#39;], [&#39;ËÇ°Á•®&#39;, &#39;ÊäïË≥á&#39;], [&#39;Ë´ãÁõä&#39;, &#39;Âà∏ÂïÜ&#39;, &#39;ËÇ°Á•®&#39;, &#39;ÊäïË≥á&#39;, &#39;Âè∞ËÇ°&#39;]] . . Creating DataFrame from JSON . Now let&#39;s define a function called parse_batch() that takes the JSON response as input and returns a DataFrame instance. . import pandas as pd def parse_batch(batch): createdAt = [item.get(&#39;createdAt&#39;, &#39;None&#39;) for item in batch] title = [item.get(&#39;title&#39;, &#39;None&#39;) for item in batch] excerpt = [item.get(&#39;excerpt&#39;, &#39;None&#39;) for item in batch] dummy = [] categories = [item.get(&#39;categories&#39;, dummy) for item in batch] # every element is a list topics = [item.get(&#39;topics&#39;, dummy) for item in batch] # every element is a list data = { &#39;createdAt&#39;: createdAt, &#39;title&#39;: title, &#39;excerpt&#39;: excerpt, &#39;categories&#39;: categories, &#39;topics&#39;: topics, } df = pd.DataFrame(data) df.loc [:, &#39;categories&#39;] = df[&#39;categories&#39;].apply(lambda x: &quot; | &quot;.join(x)) df.loc [:, &#39;topics&#39;] = df[&#39;topics&#39;].apply(lambda x: &quot; | &quot;.join(x)) return df . Here&#39;s the first five rows of our scraped data. . stock = parse_batch(batch) stock.head() . createdAt title excerpt categories topics . 0 2021-09-12T14:22:22.276Z | #Ë´ãÁõä #Ë´ãÁõä ÈáëËûçËÇ°Â≠òËÇ°ÁñëÂïè ÂÆòËÇ° Ê∞ëËÇ° | Â¶ÇÈ°åÔºåÊàëÁü•ÈÅìÂÆòËÇ°Â≠òËÇ°È¶ñÊé®ÂÖÜË±êÈáëÔºåÈÖçÊÅØÂèàÈÖçËÇ°ÔºåÂèØÊòØÊúÉÈÖçËÇ°ÁöÑËÇ°Á•®‰ª£Ë°®ËÇ°Êú¨Ë¶ÅÂæàÂ§ßÔºå‰∏îÊØèÂπ¥Áç≤Âà©Â¶ÇÊûúÊ≤í... | Ë´ãÁõä | Ë´ãÁõä | ÈáëËûç | ÊäïË≥á | ÂÆòËÇ° | Ê∞ëÁáüÈáëÊéß | . 1 2021-09-12T13:59:36.831Z | #ÂàÜ‰∫´ 9/12ÈöîÊó•Áï∂Ê≤ñ+Ê≥¢ÊÆµÂàÜÊûê | **ÁÑ°Êé®Ëñ¶Ë∑üÂñÆ‰πãÊÑè**Ôºå**Á¥îÂÄã‰∫∫Êìç‰ΩúÂàÜ‰∫´**Ôºå**ÊêçÁõäËá™Ë≤†**ÔºåÊú¨‰∫∫Áï∂Ê≤ñÁÜ±ÊÑõTickÊµÅÁé©Ê≥ï... | ÂàÜ‰∫´ | ÂàÜÊûê | Âè∞ËÇ° | Áï∂Ê≤ñ | Ê≥¢ÊÆµ | ÊäÄË°ìÂàÜÊûê | . 2 2021-09-12T13:02:01.859Z | #ÂàÜ‰∫´ 09/12È°ûËÇ°ÂàÜ‰∫´-ÊäÄË°ì„ÄÅÁ±åÁ¢ºÂàÜÊûê | ‰ª•‰∏ãÁÇ∫ÂÄã‰∫∫ÊäÄË°ìÂèäÁ±åÁ¢ºÈù¢ÂàÜÊûêÔºåÂÉÖ‰æõÂèÉËÄÉÔºåÈÄ≤Âá∫Â†¥Ë´ã‰æùÁÖßÂÄã‰∫∫ÁúãÊ≥ïÂÅöÊ±∫ÂÆö„ÄÇÊØèÊó•ÊúÉÊúâ‰∏ÄÁØáÊõ¥Ë©≥Á¥∞ÁöÑÈ°ûËÇ°ÂàÜ... | ÂàÜ‰∫´ | ÊäïË≥á | ËÇ°Á•® | ÁêÜË≤° | Âè∞ËÇ° | ËÇ°Â∏Ç | . 3 2021-09-12T11:55:08.194Z | #Ë´ãÁõä Èï∑Ê¶ÆÊàêÊú¨17ÂÖÉ„ÄÇ11Âπ¥ÂâçË≤∑ÁöÑ | Ë´ãÂïè‰∏Ä‰∏ã„ÄÇÈï∑Ê¶ÆÊµ∑ÈÅãËÇ°Á•®10Âºµ„ÄÇÊàêÊú¨17ÂÖÉ‚Ä¶11Âπ¥ÂâçË≤∑ÁöÑÔºåÔºåÔºåÂøòË®òËá™Â∑±Êúâ ÈÄôÊ™îËÇ°Á•®„ÄÇ‰ΩïÊôÇË©≤Âá∫Â†¥Ë≥£ÊéâÔºü | Ë´ãÁõä | Ë´ãÁõä | . 4 2021-09-12T11:25:08.604Z | #ÂÖ∂‰ªñ Á∞°Ë®äË∂ä‰æÜË∂äÂÖá‰∫ÜÂï¶ | ÁèæÂú®Â†±ÊòéÁâåÁöÑÁ∞°Ë®äË∂ä‰æÜË∂äÂÖá‰∫Ü~Â§ßÂÆ∂ÊúâÁôºÁèæÂóéÔºüÂïäÊØèÂ§©ÈÇ£È∫ºÂ§öÂ∞ÅÁ∞°Ë®äÔΩû‰∏Ä‰∏ã„ÑüÂí™ÔΩû‰∏Ä‰∏ãcandyÔºåÈÉΩ‰∏ç... | ÂÖ∂‰ªñ | ÂÖá | Èü≠Ëèú | . Getting forum names . As of Sep 11, 2021, there are in total 527 forums on Dcard. . import cloudscraper URL = &quot;https://www.dcard.tw/service/api/v2/forums&quot; scraper = cloudscraper.create_scraper() result = scraper.get(URL).json() len(result) . 527 . For each forum, we can get its English name, Chinese name, and the number of users who subscribe to it, as shown in the following dataframe. . import pandas as pd alias = [item.get(&#39;alias&#39;) for item in result] name = [item.get(&#39;name&#39;) for item in result] subscriptionCount = [item.get(&#39;subscriptionCount&#39;) for item in result] df = pd.DataFrame({&quot;name&quot;: name, &quot;alias&quot;: alias, &quot;subs&quot;: subscriptionCount}) df . name alias subs . 0 ÂçàÂ§úÂØ¶È©óÂÆ§ | midnightlab | 1711 | . 1 ÊôÇÂÖâËÜ†Âõä | timecapsule | 4284 | . 2 ÊØçË¶™ÁØÄ | mother | 373 | . 3 ËÅñË™ïCiaoCiao | merryxmas | 16807 | . 4 Áà∂Ë¶™ÁØÄ | father | 363 | . ... ... | ... | ... | . 522 „Çπ„Éù„Éº„ÉÑ | jp_sport | 110 | . 523 „Éü„Éº„É† | jp_meme | 48 | . 524 MAMAMOO | mamamoo | 4316 | . 525 ÁÑ°ÊÄßÊàÄ | asexuality | 769 | . 526 Â≠∏Â£´Âæå | post_bachelor | 592 | . 527 rows √ó 3 columns . Let&#39;s just focus on the top 20 forums in terms of subscriptions. . df.sort_values(by=[&#39;subs&#39;], ascending=False, inplace=True) top20 = df.head(20) top20 . name alias subs . 373 Ë•øÊñØ | sex | 639112 | . 224 Á©øÊê≠ | dressup | 586341 | . 228 ÊÑüÊÉÖ | relationship | 583232 | . 217 ÁæéÂ¶ù | makeup | 487542 | . 233 Ê¢óÂúñ | meme | 476599 | . 273 ÁæéÈ£ü | food | 413792 | . 230 ÈñíËÅä | talk | 375398 | . 270 ÊòüÂ∫ß | horoscopes | 364226 | . 346 ÊôÇ‰∫ã | trending | 358119 | . 340 ÁêÜË≤° | money | 323464 | . 231 ÊúâË∂£ | funny | 295991 | . 287 Netflix | netflix | 295088 | . 234 Â•≥Â≠© | girl | 289326 | . 212 YouTuber | youtuber | 283766 | . 229 ÂøÉÊÉÖ | mood | 281588 | . 328 Ê∏õËÇ• | weight_loss | 255195 | . 261 ÂØµÁâ© | pet | 248843 | . 447 ËÇ°Á•® | stock | 239823 | . 327 ÂÅ•Ë∫´ | fitness | 239310 | . 347 Â∑•‰Ωú | job | 231564 | . To get a better visual representation, let&#39;s plot out top20 with plotly, which has better support for Chinese characters than matplotlib. . import plotly.express as px fig = px.bar( top20, # df object x=&quot;name&quot;, y=&quot;subs&quot;, color=&quot;subs&quot;, title=&quot;DcardÂêÑÁâàË®ÇÈñ±Êï∏&quot;, barmode=&quot;group&quot;, height=300, ) fig.show() . . . Persisting data to SQL . We&#39;ll use the sqlite3 module to interact with a SQL database. First, the sqlite3.connect() function creates and then connects to a .db file, which we name Dcard.db. The next important thing to do is to create a table in the database. The create_table variable contains SQL syntax for creating a table named Posts with five columns, including createdAt, title, excerpt, categories, and topics. Crucially, we make the createdAt column the primary key so that posts with the same primary key will be ignored. The assumption here is that posts with the same timestamp are duplicates, though this might not be always the case. But in lack of info like post IDs, we&#39;ll just make do with timestamps. . import sqlite3 conn = sqlite3.connect(&#39;Dcard.db&#39;) cursor = conn.cursor() create_table = &quot;&quot;&quot; CREATE TABLE IF NOT EXISTS Posts ( createdAt TIMESTAMP PRIMARY KEY ON CONFLICT IGNORE, title, excerpt, categories, topics); &quot;&quot;&quot; cursor.execute(create_table) conn.commit() . Then we save the stock dataframe to the table we just created. . stock.to_sql(&#39;Posts&#39;, conn, if_exists=&#39;append&#39;, index=False) conn.commit() . To make sure the data is properly saved, let&#39;s load back the dataframe from the database. . new_stock = pd.read_sql(&quot;SELECT * FROM Posts;&quot;, conn) new_stock . createdAt title excerpt categories topics . 0 2021-09-12T14:22:22.276Z | #Ë´ãÁõä #Ë´ãÁõä ÈáëËûçËÇ°Â≠òËÇ°ÁñëÂïè ÂÆòËÇ° Ê∞ëËÇ° | Â¶ÇÈ°åÔºåÊàëÁü•ÈÅìÂÆòËÇ°Â≠òËÇ°È¶ñÊé®ÂÖÜË±êÈáëÔºåÈÖçÊÅØÂèàÈÖçËÇ°ÔºåÂèØÊòØÊúÉÈÖçËÇ°ÁöÑËÇ°Á•®‰ª£Ë°®ËÇ°Êú¨Ë¶ÅÂæàÂ§ßÔºå‰∏îÊØèÂπ¥Áç≤Âà©Â¶ÇÊûúÊ≤í... | Ë´ãÁõä | Ë´ãÁõä | ÈáëËûç | ÊäïË≥á | ÂÆòËÇ° | Ê∞ëÁáüÈáëÊéß | . 1 2021-09-12T13:59:36.831Z | #ÂàÜ‰∫´ 9/12ÈöîÊó•Áï∂Ê≤ñ+Ê≥¢ÊÆµÂàÜÊûê | **ÁÑ°Êé®Ëñ¶Ë∑üÂñÆ‰πãÊÑè**Ôºå**Á¥îÂÄã‰∫∫Êìç‰ΩúÂàÜ‰∫´**Ôºå**ÊêçÁõäËá™Ë≤†**ÔºåÊú¨‰∫∫Áï∂Ê≤ñÁÜ±ÊÑõTickÊµÅÁé©Ê≥ï... | ÂàÜ‰∫´ | ÂàÜÊûê | Âè∞ËÇ° | Áï∂Ê≤ñ | Ê≥¢ÊÆµ | ÊäÄË°ìÂàÜÊûê | . 2 2021-09-12T13:02:01.859Z | #ÂàÜ‰∫´ 09/12È°ûËÇ°ÂàÜ‰∫´-ÊäÄË°ì„ÄÅÁ±åÁ¢ºÂàÜÊûê | ‰ª•‰∏ãÁÇ∫ÂÄã‰∫∫ÊäÄË°ìÂèäÁ±åÁ¢ºÈù¢ÂàÜÊûêÔºåÂÉÖ‰æõÂèÉËÄÉÔºåÈÄ≤Âá∫Â†¥Ë´ã‰æùÁÖßÂÄã‰∫∫ÁúãÊ≥ïÂÅöÊ±∫ÂÆö„ÄÇÊØèÊó•ÊúÉÊúâ‰∏ÄÁØáÊõ¥Ë©≥Á¥∞ÁöÑÈ°ûËÇ°ÂàÜ... | ÂàÜ‰∫´ | ÊäïË≥á | ËÇ°Á•® | ÁêÜË≤° | Âè∞ËÇ° | ËÇ°Â∏Ç | . 3 2021-09-12T11:55:08.194Z | #Ë´ãÁõä Èï∑Ê¶ÆÊàêÊú¨17ÂÖÉ„ÄÇ11Âπ¥ÂâçË≤∑ÁöÑ | Ë´ãÂïè‰∏Ä‰∏ã„ÄÇÈï∑Ê¶ÆÊµ∑ÈÅãËÇ°Á•®10Âºµ„ÄÇÊàêÊú¨17ÂÖÉ‚Ä¶11Âπ¥ÂâçË≤∑ÁöÑÔºåÔºåÔºåÂøòË®òËá™Â∑±Êúâ ÈÄôÊ™îËÇ°Á•®„ÄÇ‰ΩïÊôÇË©≤Âá∫Â†¥Ë≥£ÊéâÔºü | Ë´ãÁõä | Ë´ãÁõä | . 4 2021-09-12T11:25:08.604Z | #ÂÖ∂‰ªñ Á∞°Ë®äË∂ä‰æÜË∂äÂÖá‰∫ÜÂï¶ | ÁèæÂú®Â†±ÊòéÁâåÁöÑÁ∞°Ë®äË∂ä‰æÜË∂äÂÖá‰∫Ü~Â§ßÂÆ∂ÊúâÁôºÁèæÂóéÔºüÂïäÊØèÂ§©ÈÇ£È∫ºÂ§öÂ∞ÅÁ∞°Ë®äÔΩû‰∏Ä‰∏ã„ÑüÂí™ÔΩû‰∏Ä‰∏ãcandyÔºåÈÉΩ‰∏ç... | ÂÖ∂‰ªñ | ÂÖá | Èü≠Ëèú | . 5 2021-09-12T10:23:31.827Z | #ÂàÜ‰∫´ ÈÄôÈÄ±ÊñπËàüÊ©üÊßãARKÊåÅËÇ°ËÆäÂåñ | ÂàÜ‰∫´ÈÄôÈÄ±Ôºà9/6 ~9/10ÔºâARKÊåÅËÇ°ËÆäÂåñÔºåËÇ°Á•®‰ª£Á¢º-ARKQ ÊâÄÂ±¨ETF ARKQÔºåPA... | ÂàÜ‰∫´ | ÊäïË≥á | ËÇ°Á•® | ÁæéËÇ° | ETF | Êñ∞ËÅû | . 6 2021-09-12T10:10:13.080Z | #ÂàÜ‰∫´ ÁæéËÇ°Â±ëË≤°Â†±-Êú¨ÈÄ±Ë≤°Â†±ËàáÈáçÈªû‰∫ã‰ª∂ | .ÔºåIG ÔºöÁæéËÇ°È§Ö‰πæÂ±ëÔºåÔºàÈÄ±Â†±Âõ∫ÂÆöÊØèÈÄ±Êó•Êôö‰∏ä6ÈªûÊõ¥Êñ∞ÔºâÔºå.ÔºåÊú¨ÈÄ±Ë≤°Â†±ÁúüÁöÑÊòØÊúâ‰∫õÁÑ°ËÅäÔºå‰ΩÜÔºÅË¶ÅÁôº... | ÂàÜ‰∫´ | ÊäïË≥á | ËÇ°Á•® | ÁæéËÇ° | ËÇ°Â∏Ç | Êñ∞Êâã | . 7 2021-09-12T09:55:47.721Z | #ÂàÜ‰∫´ ÊòéÊó•Áï∂Ê≤ñËßÄÁõ§ÈáçÈªû | Ê≠°ËøéÂ§ßÂÆ∂ËøΩËπ§Êàë‰∏ÄËµ∑Â≠∏ÁøíÂì¶ÔºÅ | ÂàÜ‰∫´ | ËÇ°Á•® | ÊäïË≥á | ÁêÜË≤° | Âè∞ËÇ° | Áï∂Ê≤ñ | . 8 2021-09-12T09:47:05.404Z | #Ê®ôÁöÑ ËÅØÈõª‰ª•ÂèäÊô∫ÂéüÂÄãËÇ°ÂàÜÊûê | ÈÄôÁ¶ÆÊãúÊúÄÂæå‰∏ÄÊ¨°ÂØ´ËÅØÈõªÁôºÁèæÂ§ßÂÆ∂ÁúüÁöÑÂ∞çËÅØÈõªÂæàÊúâËààË∂£ü§£ÈÇ£ÊàëÂÄëÂª¢Ë©±‰∏çÂ§öË™™ÔºåÈ¶¨‰∏äÈñãÂßãÂêßÔΩûËÅØÈõªÔºà2303... | Ê®ôÁöÑ | ÊäïË≥á | ËÇ°Á•® | ÁêÜË≤° | Âè∞ËÇ° | ÁîüÊ¥ª | . 9 2021-09-12T08:03:31.864Z | #ÂàÜ‰∫´ ÁãºÁéã9Êúà11Êó•Âë®ÂÖ≠ÁâπËºØ | Á≤âÁµ≤ÂÄãËÇ°ÊäïÁ•®ÊôÇÈñì‰ª•ÂèäÈÇ£‰∫õÂèØ‰ª• ‰∏≠Á∑ö‰ΩàÂ±ÄÁöÑËÇ°Á•®ÂÄë ROKU ADSK CHWY SAVA SA... | ÂàÜ‰∫´ | ËÇ°Á•® | ÁæéËÇ° | ÊäÄË°ìÂàÜÊûê | ÁãºÁéã | . 10 2021-09-12T05:46:00.967Z | #Ë´ãÁõä ÈóúÊñºÈÄôÊñ∞ËÅûÔºåÂêÑ‰ΩçÊÄéÈ∫ºÁúãÔºüÁúüÁöÑÊúÉÂΩ±ÈüøÂóéÔºü | ÂÉèÈÄôÁ®ÆÊñ∞ËÅûÔºåÂ∞çÊñºËÇ°Â∏ÇÁöÑÂΩ±ÈüøÁ®ãÂ∫¶ÊúÉÊúâÂΩ±ÈüøÂóéÔºüËÇ°Â∏ÇÂ∏∏Â∏∏Ëµ∑Ëµ∑‰ºè‰ºèÔºåÁúüÁöÑÂæàÊÄïË¢´ÈÄôÁ®ÆÊñ∞ËÅûÁµ¶ÁãôÊìä | Ë´ãÁõä | Ë´ãÁõä | Êñ∞ËÅû | ÂΩ±Èüø | ÊäïË≥á | ËÇ°Á•® | . 11 2021-09-12T05:26:03.419Z | #ÂàÜ‰∫´ ÂàÜ‰∫´ÂÄãËÇ°ÁúãÊ≥ï 2390 3450 | Êú¨Ê¨°Ëß£Êûê‰∏Ä‰∏ã‰∫ëËæ∞ÂíåËÅØÈàûÔºåÊ≠°Ëøé‰∏ãÊñπÁïôË®ÄËôïË®éË´ñÔºåÊúâ‰ªª‰ΩïÂú∞ÊñπÊ®ôÁ§∫ÈåØË™§Ë´ãÊåáÊïô„ÄÇÊàëÊòØËÇ°Êµ∑‰∏ÄÊª¥Ê∞¥ÔºåÊàëÂÄë‰∏ã... | ÂàÜ‰∫´ | ÊäïË≥á | ËÇ°Á•® | ÁêÜË≤° | Âè∞ËÇ° | ËÇ°Â∏Ç | . 12 2021-09-12T04:44:13.512Z | #ÂàÜ‰∫´ ÊôÇ‰∫ãÂàÜ‰∫´‚Äî‰ª•ËÇ°ÂàÜ‰∫§Êèõ‰ΩúÁÇ∫‰ºÅÊ•≠‰ΩàÂ±ÄÊâãÊÆµ | Â§ßÈØ®È≠öÂêÉÂ∞èËù¶Á±≥ÁöÑÊïÖ‰∫ãÔºåÂú®Ë≥áÊú¨Â∏ÇÂ†¥ÂÖ∂ÂØ¶ÂæàÂ∏∏ÁôºÁîüÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê≠êÁæéÂ∏ÇÂ†¥ÂæàÊµÅË°å‰ª•‰ΩµË≥ºÁöÑÊñπÂºèÔºå‰æÜÂ£ØÂ§ßÂÖ¨Âè∏... | ÂàÜ‰∫´ | ÊôÇ‰∫ã | ÂàÜ‰∫´ | ËÇ°Á•® | ÁêÜË≤° | Êñ∞ËÅû | . 13 2021-09-12T03:57:18.993Z | #ÂàÜ‰∫´ AshinËÄÅÂ∏´9/10Áõ¥Êí≠Á≠ÜË®ò | | ÂàÜ‰∫´ | ËÄÅÂ∏´ | Áõ¥Êí≠ | Á≠ÜË®ò | ËÇ°Á•® | ÊäïË≥á | . 14 2021-09-12T03:33:35.076Z | #Ë´ãÁõä Êµ∑Â§ñÂà∏ÂïÜÈñíÁΩÆÂ§™‰πÖÊúÉÊúâ‰ªÄÈ∫ºÊ≥ïÂæã‰∏äÁöÑÂïèÈ°åÂóé | Â¶ÇÈ°åÁõÆ ÊúÄËøëÊÉ≥ÈñãÂßãÂ≠òÁæéËÇ°‰∫Ü Âì™ÂÖ∂ÂØ¶‰∏äÁ∂≤Êü•Âà∞ÂæàÂ§öÊúâÁî®ÁöÑË≥áË®ä‰∫Ü Ââ©‰∏ãÈÄôÂÄãÂïèÈ°å Â¶ÇÊûúÈñíÁΩÆÂ§™‰πÖÊúÉÊúâÊ≥ï... | Ë´ãÁõä | Êµ∑Â§ñ | Âà∏ÂïÜ | Ê≥ïÂæã | . 15 2021-09-12T02:57:04.070Z | #ÂàÜ‰∫´ Êê≠‰∏äËΩâÂûãÂàóËªä-ÈáëËûçÊï∏‰ΩçÂåñ | ÈáëËûçËΩâÂûãÂã¢ÂøÖÁÇ∫Êú™‰æÜË∂®Âã¢ÔºåÈáëËûçÊï∏‰ΩçÂåñÔºå‰πüÊòØÁèæÂú®Âè∞ÁÅ£ÈáëËûçÁî¢Ê•≠Ê≠£Âú®ËëóÊâãÈÄ≤Ë°åÁöÑ‰∫ãÊÉÖÔºåËÄåÈñãÁôºÈáë‰πü‰∏çÈõ£Áúã... | ÂàÜ‰∫´ | ÂàÜ‰∫´ | ÈáëËûç | ÊäïË≥á | ËÇ°Á•® | ÁêÜË≤° | . 16 2021-09-12T02:38:43.280Z | #Ë´ãÁõä „ÄêË´ãÊïô„ÄëXQÂÖ®ÁêÉË¥èÂÆ∂APP Êñ∞ÂäüËÉΩ-Ë¥èÂÆ∂ÈÅ∏ËÇ°‰ΩøÁî®ÂøÉÂæó | ÊúÄËøëÂú®XQÂÖ®ÁêÉË¥èÂÆ∂ÁöÑFBÁ≤âÂ∞à‰∏äÔºåÁúãÂà∞‰ªñÂÄëÁöÑÊâãÊ©üAPPÊúâÊé®Âá∫‰∏ÄÂÄãÂÖ®Êñ∞ÁöÑÂäüËÉΩ‚ÄîË¥èÂÆ∂ÈÅ∏ËÇ°ÔºåËÅΩË™™ÊúâÈ´ò... | Ë´ãÁõä | ÊäïË≥á | app | . 17 2021-09-12T02:00:14.107Z | #ÂÖ∂‰ªñ Êú¨Âë®Á¥ÄÈåÑ | ÂâçÂπæÂ§©ÁôºÊñáÊ≤íÈñãÂà∞Âç°Á®±ÔºåÂÜçÁôº‰∏ÄÊ¨°Á¥ÄÈåÑÔºåÈÄôÈÇäÂè™ÁïôÁü≠Á∑öÊìç‰ΩúÁ¥ÄÈåÑÔºåÈï∑Á∑öÊòØÂÆöÊúüÂÆöÈ°çETFÔºå‰∏ÄË©ÆÔºåÊ¨£ËààÂ∞è... | ÂÖ∂‰ªñ | ËÇ°Á•® | . 18 2021-09-11T18:23:49.445Z | #ÂàÜ‰∫´ ÂêÑ‰ΩçÂà∞Â∫ïÁü•ÈÅìËÇ°Á•®ÁÇ∫‰ªÄÈ∫ºÊúÉÊº≤ÂóéÔºü | ÊàëË≥áÊ≠∑Â§ßÁ¥Ñ12Âπ¥ÔºåÂæûÈ´ò‰∏≠ÊôÇÂ∞±Áü•ÈÅìÔºå‰∏ÄÂÆöË¶ÅÂ≠∏ËÇ°Á•®ÔºåÂõ†ÁÇ∫Â§öÊï∏ÁöÑÂü∫Èáë‰πüÊòØÈù†Ë≤∑ËÇ°Á•®Ë≥∫ÔºåÈÇ£‰Ωï‰∏çËá™Â∑±Â≠∏Ëµ∑... | ÂàÜ‰∫´ | ËÇ°Á•® | ÂàÜ‰∫´ | . 19 2021-09-11T14:55:53.481Z | #ÂàÜ‰∫´ Áï∂Ê≤ñTickÊµÅÂàÜ‰∫´ | ÈÄôÂÖ©Â§©ÂæàÂ§ö‰∫∫ÂïèÊàëÈÄôÂÄãÂïèÈ°åÔºåÈÄôÈÇä‰æÜÂàÜ‰∫´‰∏Ä‰∏ãÁï∂Ê≤ñTickÁé©Ê≥ïÔºåÂñÆÁ¥îÂàÜ‰∫´Ëá™Ë∫´Á∂ìÈ©ó„ÄÅÊìç‰ΩúÊ®°ÂºèÔºåÊ≤íÊúâ‰ªª... | ÂàÜ‰∫´ | ÂàÜ‰∫´ | ËÇ°Á•® | ÊäïË≥á | Áï∂Ê≤ñ | ÂàÜÊûê | . 20 2021-09-11T12:21:23.991Z | #ÂàÜ‰∫´ ËøëÊúüÈ´òÊÆñÂà©ÁéáÊ®ôÁöÑ | ‰∏ãÈÄ±‰∏Ä‰∏ÄÂºµÂúãÊèöÔºà2505ÔºâÂèØ‰ª•È†ò1.5Ôºå‰∏ãÈÄ±‰∫å‰∏ÄÂºµËÅ≤ÂØ∂Ôºà1604ÔºâÂèØ‰ª•È†ò2.5ÔºåÁõÆÂâçÈÄôÂÖ©Ê™îÈÉΩ... | ÂàÜ‰∫´ | ËÇ°ÊÅØ | ÂàÜ‰∫´ | ËÇ°Âà© | ËÇ°Á•® | ÊäïË≥á | . 21 2021-09-11T12:17:40.308Z | #ÂàÜ‰∫´ Á∑®ÂäáÁµ¶ÊàëÊâæÂá∫‰æÜ | ‰∏çÂõâÂó¶‰∏äÂúñÔºå‰Ω†ÂÄëÂ∞±ÁúãÁúãÈ∫óÁè†ÔºåÊó©Âú®ÂπæÂπ¥ÂâçÂ∞±Â∑≤Á∂ìÁúãÂ•ΩËà™ÈÅã‰∫ÜÔºå‰Ω†ÂêÑ‰ΩçÁèæÂú®ÊâçËøΩÔºüÈôÑ‰∏ä‰∫∫Ê¨äÂï¶ÔºåË±™ÂÜ∑Ôºå2... | ÂàÜ‰∫´ | ÊäïË≥á | ÁêÜË≤° | Âè∞ËÇ° | ËÇ°Á•® | ËÇ°Â∏Ç | . 22 2021-09-11T04:46:19.407Z | #ÂàÜ‰∫´ È´ò‰ºØÁ≤æÈÅ∏ËÇ°-2481Â§ö | ÂñÆÁ¥îÁúãKÊ£í‰æÜË™™ ËÇ°ÂÉπ‰æÜÊâìÂ∑¶ÈªûÁ¥ÖÊ£í‰ΩéÈªû ÂÅúÊêçÂ∞èÔºåÁèæÂú®ÈÄ≤Â†¥Â§ßÁ¥ÑÁõÆÊ®ôÂ∞±ÊòØ110ÁöÑ‰ΩçÁΩÆ ÊàëËá™Â∑±ÂÅúÊêçÂ§ß... | ÂàÜ‰∫´ | ËÇ°Á•® | Âè∞ËÇ° | . 23 2021-09-11T02:42:41.606Z | #ÂàÜ‰∫´ Êù±Êµ∑ÂΩºÂæó - 9/10Áõ§ÂæåÂàÜÊûê | 9/10 Áõ§ÂæåÂàÜÊûêÔºåËøëÊúüÁæéËÇ°ÁöÑËµ∞Âã¢Áõ∏Â∞çÂπ≥Á©©ÔºåÁõ∏ËºÉÊñº‰∏ÄÂÄãÊúàÂâçÁöÑÊ≥¢ÂãïÔºåÂèØ‰ª•Ë™™ÂèçÂ∑ÆÈùûÂ∏∏ÂäáÁÉàÔºåÊúÄ‰∏ªË¶Å... | ÂàÜ‰∫´ | ÊäïË≥á | ËÇ°Á•® | ÁêÜË≤° | Âè∞ËÇ° | ËÇ°Â∏Ç | . 24 2021-09-11T01:38:21.750Z | #ÂàÜ‰∫´ Â¶ÇÊàë‰∏äÈÄ±ÊâÄÂà§Êñ∑ÔºåÈÇ£ËÇ°Â∏Ç‰∏ãÈÄ±Ë©≤Â¶Ç‰Ωï.. | ‰∏äÈÄ±ÂàÜ‰∫´ÁöÑ..‰πüË¢´ÊàëË™™‰∏≠‰∫Ü..Ôºå‰ª•‰∏ãÊòØ‰∏äÈÄ±ÂàÜ‰∫´ÁöÑËßÄÈªûÔºåÈÇ£Êú¨ÈÄ±ÁöÑÁãÄÊ≥Å..ÔºåÊúâÈóúÊ≥®ÁöÑÂ∞±ÊúÉÁôºÁèæÂ§ßÁõ§... | ÂàÜ‰∫´ | ËÇ°Â∏Ç | Áï∂Ê≤ñ | Ê≥¢ÊÆµ | Â§ßÁõ§ | . 25 2021-09-11T01:09:59.029Z | #Ë´ãÁõä ÂÆöÊúüÂÆöÈ°çË≤∑etf | ÊÉ≥Ë´ãÊïôÂ§ßÂÆ∂ÔºåÊàëÊòØËÇ°Â∏ÇÊñ∞ÊâãÔºåÂêåÊôÇ‰πüÊòØÁ§æÊúÉÊñ∞ÈÆÆ‰∫∫ÔºåÁî±ÊñºÁèæÂú®Âè∞ËÇ°‰∏ÄÁõ¥ÈÉΩÂú®1w7Â∑¶Âè≥ÔºåÊáâË©≤ÊòØÂè∞ËÇ°ÊúÄÊó∫... | Ë´ãÁõä | etf | ÊäïË≥á | . 26 2021-09-11T01:04:15.684Z | #ÂàÜ‰∫´ ÁãºÁéã9Êúà10Êó•ÁæéËÇ°Âæ©Áõ§ | ÈúáËï©ÁöÑ‰∏ÄÂë® Ë≤å‰ºº‰∏äÈÄ±Êé®ÊºîÂ∞±ÊèêÈÜíÂç±Èö™‰∫Ü~Ôºü‰ªäÂ§©ÊàëÂá∫ÊâãË≤∑Ë≤®‰∫ÜÂì¶~ MA TSLA AAPL NF... | ÂàÜ‰∫´ | ÁæéËÇ° | ËÇ°Á•® | ÊäÄË°ìÂàÜÊûê | ÁãºÁéã | . 27 2021-09-10T16:19:52.902Z | #Ê®ôÁöÑ ÁæéËÇ° Nvidia ËºùÈÅîÔºà$NVDAÔºâ ÁúãÂ§ö | ‰ªäÂ§©Ë¶Å‰æÜË´áÁöÑÊ®ôÁöÑÊòØ $NVDA„ÄÇÂ¶ÇÊûú‰Ω†ÊúâÂú®ÊâìÈÅäÊà≤ÔºåÂ∞±‰∏ÄÂÆöÊúÉÁü•ÈÅì‰ªñÂÄëÁöÑË∂ÖÂº∑È°ØÂç°„ÄÇ‰ªñÂÄëÁöÑÈ°ØÂç°‰∏çÂè™... | Ê®ôÁöÑ | ÁæéËÇ° | ËÇ°Á•® | ÁêÜË≤° | NVIDIA | ÊäïË≥á | . 28 2021-09-10T15:55:10.143Z | #Ë´ãÁõä Ë´ãÂïèÈÄôÊòØ600Ëê¨Ë≥†Âà∞300Ëê¨Âóé | ÂæàË¨ùË¨ùÂ§ßÂÆ∂ÁöÑÂõûË¶Ü‰ΩÜÂéüÊú¨ÁöÑË≥áË®äÂ•ΩÂÉè‰∏çÂ§™Ê∏ÖÊ•öÔºåÈÄôÊòØÂÆåÊï¥ÂÖßÂÆπÂÜçÈ∫ªÁÖ©Â§ßÂÆ∂Ëß£ÊÉë‰∫ÜË¨ùË¨ùÔºå‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî... | Ë´ãÁõä | ËÇ°Á•® | ÊäïË≥á | . 29 2021-09-10T15:51:03.972Z | #Ë´ãÁõä #Ë´ãÁõä Ë´ãÂïèÈÄôÊòØÂì™ÂÆ∂Âà∏ÂïÜÁöÑ‰ªãÈù¢ | Ë´ãÂïèÈÄôÊòØÂì™ÂÆ∂Âà∏ÂïÜÁöÑ‰∏ãÂñÆÁ≥ªÁµ±ÔºåÂèØ‰ª•È°ØÁ§∫ÁôæÂàÜÊØîÔºÅÔºåË∑™Ê±ÇÂêÑ‰ΩçÂ§ßÁ•û‚ÄçÔ∏è‚ÄçÔ∏è‚ÄçÔ∏è | Ë´ãÁõä | Ë´ãÁõä | Âà∏ÂïÜ | ËÇ°Á•® | ÊäïË≥á | Âè∞ËÇ° | . . Testing the logger . We&#39;ll use the logging module to create a log file named logging.txt, which can be configured by the logging.basicConfig() function. I&#39;d like the logging format to be [{timestamp}] {logging level} | {logging message}, so the value of the format argument is [%(asctime)s] %(levelname)s | %(message)s. In addition, the format of the timestamp can be set up by the datefmt argument. . import logging logging.basicConfig( filename=&#39;logging.txt&#39;, filemode=&quot;a&quot;, level=logging.INFO, format=&quot;[%(asctime)s] %(levelname)s | %(message)s&quot;, datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;, ) . Let&#39;s test out three types of logs and check out the logging file. . logging.info(&quot;This is an info.&quot;) logging.error(&quot;This an error!&quot;) logging.warning(&quot;This is a warning!&quot;) !head logging.txt . [2021-09-12 08:31:31] INFO | This is an info. [2021-09-12 08:31:31] ERROR | This an error! [2021-09-12 08:31:31] WARNING | This is a warning! . Testing the scheduler . We&#39;ll use the schedule library to activate our Dcard scraper at regular intervals. As a test for the scheduling function, the scheduler.py simply logs the current time to logging.txt every three seconds. The first step for scheduling a job is to define a function, which is named job() in this case. Then the job can be put on schedule by simply calling the schedule.every({num}).{unit}.do({job}) function, where {num} is an integer, {unit} a string for unit of time like seconds, minutes or hours, and finally {job} the function scheduled to run. Finally, if we call the schedule.run_pending() function within a while loop, the program will run indefinitely. Run the following cell to create scheduler.py. . %%writefile scheduler.py import schedule import time from datetime import datetime import logging logging.basicConfig( filename=&#39;logging.txt&#39;, filemode=&quot;a&quot;, level=logging.INFO, format=&quot;[%(asctime)s] %(levelname)s | %(message)s&quot;, datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;, ) def job(): now = datetime.now() message = f&quot;Hello, the current time is {now}.&quot; logging.info(message) schedule.every(3).seconds.do(job) schedule.run_all() #Without this line, the job will start in 3 seconds rather than immediately. while True: schedule.run_pending() time.sleep(1) . . Now run python scheduler.py in the terminal to test the scheduler, which will keep running unless stopped! If you run it for a while and then stop it, logging.txt will look something like this: . !tail logging.txt . [2021-09-12 08:31:31] ERROR | This an error! [2021-09-12 08:31:31] WARNING | This is a warning! [2021-09-12 08:37:40] INFO | Hello, the current time is 2021-09-12 08:37:40.643431. [2021-09-12 08:37:43] INFO | Hello, the current time is 2021-09-12 08:37:43.647108. [2021-09-12 08:37:46] INFO | Hello, the current time is 2021-09-12 08:37:46.651045. [2021-09-12 08:37:49] INFO | Hello, the current time is 2021-09-12 08:37:49.655009. [2021-09-12 08:37:52] INFO | Hello, the current time is 2021-09-12 08:37:52.658558. [2021-09-12 08:37:55] INFO | Hello, the current time is 2021-09-12 08:37:55.662393. [2021-09-12 08:37:58] INFO | Hello, the current time is 2021-09-12 08:37:58.666367. [2021-09-12 08:38:01] INFO | Hello, the current time is 2021-09-12 08:38:01.669397. . Now that we&#39;ve covered all the components we need, let&#39;s remove logging.txt and Dcard.db to start afresh and put everything together. To do that, just run rm logging.txt Dcard.db in the terminal. . Putting everything together . Finally, it&#39;s time to put everything together. Run the following cell to create Dcard_scraper.py. The only thing new here is that this time around we are going to scrape multiple forums rather than just one. So we first create a dictionary called forums, where the keys are forum names in English and the values their equivalents in Chinese. We&#39;ll need the English forum names to get the API full URLs. Plus, we add two more columns in the Posts tabel of Dcard.db (i.e. forum_en and forum_zh) to store the forum names. The main() function takes care of iteration over every forum stored in the forums variable as well as some basic exception handling. . %%writefile Dcard_scraper.py import cloudscraper import logging import pandas as pd from random import randint import schedule import sqlite3 import time # Configuring the logging.txt file logging.basicConfig( filename=&#39;logging.txt&#39;, filemode=&quot;a&quot;, level=logging.INFO, format=&quot;[%(asctime)s] %(levelname)s | %(message)s&quot;, datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;, ) # Dcard API base URL baseURL = &quot;https://www.dcard.tw/service/api/v2/forums/&quot; # List of forums. Add as many as you want. Here I&#39;m just picking 18 forums. forums = { &quot;dressup&quot;: &quot;Á©øÊê≠&quot;, &quot;relationship&quot;: &quot;ÊÑüÊÉÖ&quot;, &quot;makeup&quot;: &quot;ÁæéÂ¶ù&quot;, &quot;food&quot;: &quot;ÁæéÈ£ü&quot;, &quot;horoscopes&quot;: &quot;ÊòüÂ∫ß&quot;, &quot;talk&quot;: &quot;ÈñíËÅä&quot;, &quot;trending&quot;: &quot;ÊôÇ‰∫ã&quot;, &quot;money&quot;: &quot;ÁêÜË≤°&quot;, &quot;funny&quot;: &quot;ÊúâË∂£&quot;, &quot;girl&quot;: &quot;Â•≥Â≠©&quot;, &quot;netflix&quot;: &quot;Netflix&quot;, &quot;youtuber&quot;: &quot;YouTuber&quot;, &quot;mood&quot;: &quot;ÂøÉÊÉÖ&quot;, &quot;pet&quot;: &quot;ÂØµÁâ©&quot;, &quot;weight_loss&quot;: &quot;Ê∏õËÇ•&quot;, &quot;fitness&quot;: &quot;ÂÅ•Ë∫´&quot;, &quot;stock&quot;: &quot;ËÇ°Á•®&quot;, &quot;job&quot;: &quot;Â∑•‰Ωú&quot;, } # Creating a SQLite database and a table named Posts conn = sqlite3.connect(&#39;Dcard.db&#39;) cursor = conn.cursor() create_table = &quot;&quot;&quot; CREATE TABLE IF NOT EXISTS Posts ( createdAt TIMESTAMP PRIMARY KEY ON CONFLICT IGNORE, title, excerpt, categories, topics, forum_en, forum_zh); &quot;&quot;&quot; cursor.execute(create_table) conn.commit() # Parsing a batch of JSON response and creating a dataframe out of it def parse_batch(batch): createdAt = [item.get(&#39;createdAt&#39;, &#39;None&#39;) for item in batch] title = [item.get(&#39;title&#39;, &#39;None&#39;) for item in batch] excerpt = [item.get(&#39;excerpt&#39;, &#39;None&#39;) for item in batch] dummy = [] categories = [item.get(&#39;categories&#39;, dummy) for item in batch] # every element is a list topics = [item.get(&#39;topics&#39;, dummy) for item in batch] # every element is a list data = { &#39;createdAt&#39;: createdAt, &#39;title&#39;: title, &#39;excerpt&#39;: excerpt, &#39;categories&#39;: categories, &#39;topics&#39;: topics, } df = pd.DataFrame(data) df.loc [:, &#39;categories&#39;] = df[&#39;categories&#39;].apply(lambda x: &quot; | &quot;.join(x)) df.loc [:, &#39;topics&#39;] = df[&#39;topics&#39;].apply(lambda x: &quot; | &quot;.join(x)) return df # Main scraper def main(): scraper = cloudscraper.create_scraper() sec = randint(1, 15) for forum_en, forum_zh in forums.items(): result = scraper.get(baseURL + forum_en + &quot;/posts&quot;) if result.status_code == 200: batch = result.json() try: df = parse_batch(batch) df[&quot;forum_en&quot;] = forum_en df[&quot;forum_zh&quot;] = forum_zh logging.info(f&quot;{df.shape[0]} posts on {forum_en} have been scraped.&quot;) df.to_sql(&quot;Posts&quot;, conn, if_exists=&quot;append&quot;, index=False) conn.commit() cursor.execute(f&quot;SELECT COUNT(*) from Posts;&quot;) rows = cursor.fetchone()[0] logging.info(f&quot;There are in total {rows} posts in the DB.&quot;) except Exception as argument: logging.error(argument) else: logging.error(f&quot;The request on {forum_en} was unsuccessful.&quot;) time.sleep(sec) # Setting the scraping interval schedule.every(30).minutes.do(main) schedule.run_all() while True: schedule.run_pending() time.sleep(1) . . Now it&#39;s harvest time! Run python Dcard_scraper.py in the terminal to start the scraper, which will run every 30 minutes unless stopped. If everything goes well, the logging.txt file will look like this: . !tail logging.txt . [2021-09-12 14:44:21] INFO | 30 posts on pet have been scraped. [2021-09-12 14:44:21] INFO | There are in total 420 posts in the DB. [2021-09-12 14:44:33] INFO | 30 posts on weight_loss have been scraped. [2021-09-12 14:44:33] INFO | There are in total 450 posts in the DB. [2021-09-12 14:44:46] INFO | 30 posts on fitness have been scraped. [2021-09-12 14:44:46] INFO | There are in total 480 posts in the DB. [2021-09-12 14:44:58] INFO | 30 posts on stock have been scraped. [2021-09-12 14:44:58] INFO | There are in total 510 posts in the DB. [2021-09-12 14:45:10] INFO | 30 posts on job have been scraped. [2021-09-12 14:45:10] INFO | There are in total 540 posts in the DB. . And here&#39;s the result of our hard work! In my case, I ran the scraper for around 3 minutes and got 540 posts. . conn = sqlite3.connect(&#39;Dcard.db&#39;) data = pd.read_sql(&quot;SELECT * FROM Posts;&quot;, conn) data . createdAt title excerpt categories topics forum_en forum_zh . 0 2021-09-12T14:35:07.314Z | Âïèair forceÁúüÂÅá | Ô∏èÁ¨¨‰∏ÄÊ¨°ÁôºÊñáÔºå‰∏çÁü•ÈÅìÁôºÂú®Á©øÊê≠ÁâàÂèØ‰∏çÂèØ‰ª•ÔºåÊéíÁâà‰∏çÂ•ΩË´ãË¶ãË´íËã•ÊúâÈÅïÂèçË¶èÂÆöÊúÉÂà™ÊñáÔºåÂâçÈô£Â≠êÂú®Ëù¶ÁöÆË≥ºË≤∑‰∏Ä... | | Âïè | force | ÁúüÂÅá | Á©øÊê≠ | Ëù¶ÁöÆ | dressup | Á©øÊê≠ | . 1 2021-09-12T14:18:12.598Z | Áñ´ÊÉÖË≤∑ÁöÑË°£ÊúçÂàÜ‰∫´üôåÊ∑òÂØ∂Â±ÖÂ§ö | ÊàëÊòØÂ•≥ÁîüÔºÅÔºåÊú¨‰∫∫156/45ÔºåË°£ÊúçÈÉΩË†ªÂπ≥ÂÉπÁöÑÔΩû1‚É£Ô∏èÔºåÊ¥ãË£ùÔºöÊ∑òÂØ∂ÔºåÂåÖÔºöToaeÔºåÈûãÂ≠êÔºöÊ∑òÂØ∂Ôºå... | | Áñ´ÊÉÖ | Ë°£Êúç | ÂàÜ‰∫´ | dressup | Á©øÊê≠ | . 2 2021-09-12T13:56:35.559Z | ÊàëËàáÂÆ§ÂèãÁöÑÁ©øÊê≠ÂàÜ‰∫´ | Ë∂ÅÈ¢±È¢®Â§©Ê≤í‰∫ã‰æÜÂàÜ‰∫´ÊàëË∑üÂÆ§ÂèãÁöÑÁ©øÊê≠ÔΩûÔºàÊ≤íÊà¥Âè£ÁΩ©ÁöÑÊòØÁñ´ÊÉÖÂâçÊãçÁöÑÂë¶ÔºâÔºåÂÖàÂàÜ‰∫´ÂÆ§ÂèãÁöÑÔºå1. ÂñÆËªäË§≤Á©ø... | | Á©øÊê≠ | Â•≥ÁîüÁ©øÊê≠ | dressup | Á©øÊê≠ | . 3 2021-09-12T13:54:26.217Z | #Âïè Ê±ÇÂåÖÂåÖÁöÑÈóúÈçµÂ≠ó | ÊÉ≥Ë´ãÂïè‰øû‰∏ÅËÉåÁöÑÈÄôÁ®ÆÂåÖÂè´‰ªÄÈ∫ºÂêçÂ≠óÔºåÊúâÈªûÂÉèÈÄÅÂ≠êÈ≥•ÂåÖÔºåÂèØÊòØÊàëÂú®Ëù¶ÁöÆÈÉΩÊâæ‰∏çÂà∞È°û‰ººÁöÑÔºåÊàñÊòØÊúâ‰∫∫Âú®Âì™‰∫õÁ∂≤... | | ÂåÖÂåÖ | ÈóúÈçµÂ≠ó | dressup | Á©øÊê≠ | . 4 2021-09-12T13:36:51.407Z | #Âïè ÂåóËáâÂåÖÂåÖ‰ª£Ë≥º | Â∞èÂ¶πÊÉ≥Ë≤∑ÈÄôÂÄãÂåÖÂåÖÂæà‰πÖ‰∫ÜÔºå‰ΩÜÂåóËáâÁöÑÂåÖÂåÖÊòØÁ¨¨‰∏ÄÊ¨°Ë≥ºË≤∑ÔºåÊÄïË≤∑Âà∞‰ªøÂÜíÂìÅÔºåÊÉ≥Ë´ãÊïôÂêÑ‰ΩçÁâàÂèãÔºåÊúâÊé®Ëñ¶ÁöÑË≥£ÂÆ∂... | | Âïè | ÂåóËáâ | ÂåÖÂåÖ | ÁúüÂÅá | dressup | Á©øÊê≠ | . ... ... | ... | ... | ... | ... | ... | ... | . 535 2021-09-12T12:37:12.067Z | ËΩâËÅ∑ ÈÄöÂã§orÁßüÂ±ãË´ãÁõä | Â§ßÂÆ∂Â•ΩÔºåÂ∞èÂ¶πÈ†êË®à10ÊúàÂàùÂà∞Êñ∞ÂÖ¨Âè∏ÔºàÊûóÂè£ÔºâÂ†±Âà∞ÔºåÂÆ∂‰ΩèÊñ∞ÂåóÊ±êÊ≠¢ÔºåÁõÆÂâçÁÖ©ÊÉ±Ë¶ÅÈñãËªäÈÄöÂã§Ôºà40-50ÂàÜ... | | ÈÄöÂã§ | ÁßüÂ±ã | job | Â∑•‰Ωú | . 536 2021-09-12T12:24:17.122Z | Êó©ÂÖ´Êôö‰∫îÂ∑•‰Ωú | Ë´ãÂïèÊúâ‰ªÄÈ∫ºÂ∑•‰ΩúÊòØÊó©ÂÖ´Êôö‰∫îÔºåÔºàÊ≠£ËÅ∑ÔºâÔºå‰ΩÜÊòØÊîæÂÅá‰∏çÊòØË¶ãÁ¥ÖÂ∞±‰ºëÔºüËÄåÊòØÊéí‰ºëÁöÑÔºü | | Â∑•‰Ωú | Â∑•‰ΩúÁ∂ìÈ©ó | job | Â∑•‰Ωú | . 537 2021-09-12T12:20:05.422Z | #Âïè ÊúÉÂïèÁîüÊ¥ªÈ´îÈ©óÊòØÂ∏åÊúõÂæóÂà∞‰ªÄÈ∫ºÁ≠îÊ°à | Â¶ÇÈ°åÔºåÈù¢Ë©¶ÊôÇÂÖ¨Âè∏Áµ¶‰∫Ü‰∏ÄÂºµÂü∫Êú¨Ë≥áÊñôË°®ÔºåÁ¨¨‰∏ÄÂÄãÂïèÊàëÂ∞ç‚ÄúÂ∑•‰Ωú‚ÄùÔºàÊ≤íÁ¢∫ÂàáË™™ÊòØÊáâÂæµËÅ∑‰ΩçÈÇÑÊòØÂ∑•‰ΩúÊú¨Ë∫´ÔºâÁöÑ... | | Â∑•‰Ωú | Ê±ÇËÅ∑ | job | Â∑•‰Ωú | . 538 2021-09-12T12:11:54.507Z | Â∑•‰ΩúÂπæÂπ¥ÂæåÈÇÑÊúÉÊÉ≥ÂõûÂ≠∏Ê†°ËÆÄÊõ∏ÂóéÔºü | ‰ª•ÂâçËÄÅÂ∏´Â∏∏Ë™™Ë¶ÅËÆÄÂ∞±‰∏ÄÂè£Ê∞£ËÆÄ‰∏çË¶Å‰∏≠Êñ∑‰∏çÁÑ∂ÂæàÈõ£Â∑•‰ΩúÂæåÂÜçÂõû‰æÜËÆÄÔºåÊàëËá™Â∑±ÊòØÂ∑•‰Ωú2Âπ¥ÂæåÈõ¢ËÅ∑Ê∫ñÂÇô‰∏ÄÂπ¥ËÄÉ‰∏ä... | | Â≠∏Ê†° | ËÆÄÊõ∏ | Â∑•‰Ωú | job | Â∑•‰Ωú | . 539 2021-09-12T12:10:44.789Z | #Âïè Â§ßÂ≠∏Á†îÁ©∂‰∏≠ÂøÉÂæµÊâç | Êú¨‰∫∫ÁÇ∫ÁßÅÁ´ãÊáâÂ±ÜÔºåÊúÄËøëÂú®Ê±ÇËÅ∑‰∏≠ÔºåÁúãÂà∞ÂæàÂ§öÂõõÂ§ßÂõõ‰∏≠ÁöÑÁ†îÁ©∂‰∏≠ÂøÉ‰πãÈ°ûÁöÑÂú®ÂæµÊâçÔºå‰πüÊúâÊäïÈÅûÂ±•Ê≠∑Á¥ÑÈù¢Ë©¶ÔºåËÅ∑... | | ÂæµÊâç | Âïè | ÊáâÂ±ÜÁï¢Ê•≠Áîü | job | Â∑•‰Ωú | . 540 rows √ó 7 columns . . Recap . In this post, we used cloudscraper to scrape data from Dcard and schedule to regularly run the scraper. Both are powerful and elegant libraries that can be applied to any other scraping project. As a side note, I was able to run the Dcard scraper for several days in a row without having any error! .",
            "url": "https://howard-haowen.github.io/blog.ai/cloudscraper/schedule/sqlite3/logging/2021/09/12/Scraping-Dcard-with-cloudscraper.html",
            "relUrl": "/cloudscraper/schedule/sqlite3/logging/2021/09/12/Scraping-Dcard-with-cloudscraper.html",
            "date": " ‚Ä¢ Sep 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Training lecture for the International Linguistics Olympiad",
            "content": ". Intro . Recently, I had the honor of being invited to give a training lecture to the Taiwan team for the International Linguistics Olympiad. It‚Äôs basically an annual competition where participants from countries all over the world get together to solve linguistics puzzles. You don‚Äôt need to have official training in linguistics in order to solve those puzzles, though knowing some linguistics does help. That‚Äôs why I was asked to give this lecture. . A past problem set . The whole point of the competition is not so much the scope of linguistic knowledge one has as the analytic ability of identifying the systematic patterns buried in the given data. The process is actually quite similar to solving programming problems on LeetCode. . Take this problem set from the 16th competition in 2018 for instance1. . . The problem set is from Ter√™na, a language spoken in Brazil. Given the information provided here, participants are asked to fill in the gaps indicated by numbers 1 through 14. NLP models might be able to solve a problem like this one day, but that would require way more training data than provided here. But we humans are pretty good at making generalizations with only a limited amount of data. I encourage those of you who know little about linguistics to work on this puzzle before checking out the answer2. . Lecture slides . The Ter√™na problem set happens to be relevant to one of my favorite research topics, which is why I chose this title for my lecture: verbal person marking &amp; pronominal clitics. It may sound like gibberish if you don‚Äôt know anything about linguistics. Although the technical terms may sound obscure, the linguistic patterns they describe are in fact quite straightforward. To find out, check out some examples in the lecture slides below. . A problem set from Taiwan . In addition to the slides, I created a problem set based on a language of Taiwan, which is called Kavalan. You‚Äôve probably heard of this name because of Kavalan Whisky. That‚Äôs right. This is the same Kavalan we‚Äôre talking about here. I wrote my MA thesis on the Kavalan language, but I‚Äôve never tried Kavalan Whisky. . Here‚Äôs the Kavalan problem set, most examples of which are drawn from this paper3. . SN Kavalan English . 01 | pukunankuisu. | I beat you. | . 02 | pukunansuiku. | You beat me. | . 03 | qaRatannaiku. | He bit me. | . 04 | maiiku pmukun wasusu. | I didn‚Äôt beat your dog. | . 05 | pmukuniku sunisku. | I‚Äôm beating my child. | . 06 | mai wasusu qmaRat wasuku ni? | Didn‚Äôt your dog bite mine? | . 07 | maipamaisu maynep ni? | Aren‚Äôt you asleep yet? | . 08 | mayneptiisu ni? | Are you already asleep? | . 09 | pmukuntiisu wasusu ni? | Did you already beat your dog? | . 10 | maiiku pukunanna. | He didn‚Äôt beat me. | . 11 | qaRatansuiku ni? | (1) | . 12 | maiiku qmaRat sunissu. | (2) | . 13 | (3) | I haven‚Äôt beaten my child yet. | . 14 | (4) | Didn‚Äôt you beat your child? | . The task is to fill in the gaps indicated by (1) through (4). If you go through the slides, you‚Äôll be in a better position to solve this puzzle. I encourage you to share your solution in the comments below. . Footnotes . Here‚Äôs the complete Ter√™na problem set.¬†&#8617; . | Here‚Äôs the complete solution to the Ter√™na problem set.¬†&#8617; . | The Cluster-internal Ordering of Clitics in Kavalan (East Formosan, Austronesian by Doris Ching-jung Yen and Loren Billings, presented at the Annual Meeting of the Berkeley Linguistics Society.¬†&#8617; . |",
            "url": "https://howard-haowen.github.io/blog.ai/linguistics-olympiad/google-slides/languages-of-taiwan/2021/07/18/International-Linguistics-Olympiad.html",
            "relUrl": "/linguistics-olympiad/google-slides/languages-of-taiwan/2021/07/18/International-Linguistics-Olympiad.html",
            "date": " ‚Ä¢ Jul 18, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "StackEdit makes blogging in Markdown easy-peasy",
            "content": ". Intro . FastPages supports blog posts in three file formats, including .ipynb for Jupyter, .md for Markdown and .docx for Microsoft Word. Each type of formats is supposed to be saved in a dedicated folder, respectively named _notebooks, _posts, and _word by FastPages. I‚Äôve been using Colab to write posts in .ipynb because publishing them is as easy as saving them to GitHub. This can be done on Colab by doing: . File &gt; Save a copy in GitHub . Then all you need to do is select your FastPages repo on GitHub and the _notebooks folder. . But as for posts in .md, my workflow was not as smooth until I discovered StackEdit, which is an in-browser Markdown editor. So this post shows how StackEdit makes blogging in Markdown easy-peasy. . Adding a workspace . Once you‚Äôre on StackEdit, clicking on START WRITING is all that needs to be done to, umm‚Ä¶, start writing in Markdown. But what comes next is not that straightforward. It took me a while to figure it out. To sync a StackEdit browser instance with a folder on GitHub, you first need to add a workspace. This is done by clicking on the hashtag icon at the upper right corner. Then you do: . Workspaces &gt; Add a GitHub workspace . Then enter your FastPages repo URL and the _posts folder path (or any other GitHub folder). Now if you click on the folder icon at the upper left corner, you‚Äôll see all your .md files in the the _posts folder like this: . . Now you can enjoy writing on StackEdit, and rest assured that all your local .md files (saved in the browser storage) will be synced with your GitHub folder. . By the way, I found a trick of quickly embedding an image on StackEdit. Suppose you have an image file stored on GitHub, you can just copy that file name in the tree view on GitHub and paste it on StackEdit. Take the image above for example, this is what you‚Äôll see: . [StackEdit-snapshot.png](https://github.com/howard-haowen/blog.ai/blob/master/images/StackEdit-snapshot.png &quot;StackEdit-snapshot.png&quot;) . Then to display the image, you just need to add an exclamation mark at the beginning and replace blob with raw, like this: . ![StackEdit-snapshot.png](https://github.com/howard-haowen/blog.ai/raw/master/images/StackEdit-snapshot.png &quot;StackEdit-snapshot.png&quot;) . This trick saves you the trouble of typing out the embedding syntax. . Testing Markdown extensions . In addition to default Markdown syntax, StackEdit also supports some additional extensions, including SmartyPants, KaTeX, and UML diagrams. To test if these extensions will be properly rendered by FastPages, I simply copied and pasted examples from StackEdit down below: . . SmartyPants . SmartyPants converts ASCII punctuation characters into ‚Äúsmart‚Äù typographic punctuation HTML entities. For example: . ¬† ASCII HTML . Single backticks | &#39;Isn&#39;t this fun?&#39; | ‚ÄòIsn‚Äôt this fun?‚Äô | . Quotes | &quot;Isn&#39;t this fun?&quot; | ‚ÄúIsn‚Äôt this fun?‚Äù | . Dashes | -- is en-dash, is em-dash | ‚Äì is en-dash, ‚Äî is em-dash | . KaTeX . You can render LaTeX mathematical expressions using KaTeX: . The Gamma function satisfying $ Gamma(n) = (n-1)! quad forall n in mathbb N$ is via the Euler integral . Œì(z)=‚à´0‚àûtz‚àí1e‚àítdt‚Äâ. Gamma(z) = int_0^ infty t^{z-1}e^{-t}dt ,.Œì(z)=‚à´0‚àû‚Äãtz‚àí1e‚àítdt. . You can find more information about LaTeX mathematical expressions here. .",
            "url": "https://howard-haowen.github.io/blog.ai/blogging/markdown/stackedit/colab/2021/07/17/StackEdit-makes-blogging-in-Markdown-easy-peasy.html",
            "relUrl": "/blogging/markdown/stackedit/colab/2021/07/17/StackEdit-makes-blogging-in-Markdown-easy-peasy.html",
            "date": " ‚Ä¢ Jul 17, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "How I learned Python from scratch",
            "content": ". Before the outbreak of Covid-19, I couldn‚Äôt write any single line of code. But by teaching myself Python and machine learning during the lockdown period, I ended up being an AI engineer in mid-September in 2020. So I was invited to give a mini-talk on how to learn Python from scratch. I came up with a 4B learning path, covering basics, badges, and bootstrapping, and blogging. For those interested in the details, click on the title page at the bottom to view the slides. At first, I didn‚Äôt think the slides are worth sharing at all, but one of my old-time colleagues at Rice University told me that he saw them posted on LinkedIn and found them quite helpful. He even considered changing his career path and getting into data science. This is why I‚Äôm sharing the slides here. .",
            "url": "https://howard-haowen.github.io/blog.ai/python/google-slides/2021/07/11/how-to-learn-python-from-scratch.html",
            "relUrl": "/python/google-slides/2021/07/11/how-to-learn-python-from-scratch.html",
            "date": " ‚Ä¢ Jul 11, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "List of blogs built with FastPages",
            "content": ". Last updated: 12 July, 2021 . This post aggregates other blogs built with FastPages that I came across. This list is constantly updated. . NLP . Inverse Entropy | . Backend . Red‚Äôs Digressions | . Visualization . COVID-19 Dashboards | .",
            "url": "https://howard-haowen.github.io/blog.ai/fastpages/nlp/visualization/backend/2021/07/11/FastPages-blogs.html",
            "relUrl": "/fastpages/nlp/visualization/backend/2021/07/11/FastPages-blogs.html",
            "date": " ‚Ä¢ Jul 11, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "What I did to pass the Microsoft Exam DP-100 for data scientists",
            "content": ". Intro . It‚Äôs been a while since I posted anything here. And it‚Äôs primarily because I was busy preparing for the Microsoft DP-100 Exam, which is an associate (i.e. intermediate) level for designing and implementing data science solutions on Azure. I spent about 2 months preparing for it and got my certificate today! Here‚Äôs what it looks like. . . The certificate also comes with a dedicated Credly webpage, where people can verify its validity. In this post, I‚Äôll share the resources that I perused for the prep work. They come from three sources: a book, two Udemy courses, and three kinds of official materials from Microsoft. . One book . . The book is entitled Data Science Solutions on Azure Tools and Techniques Using Databricks and MLOps, which is a good start if you don‚Äôt know anything about Azure or other cloud tools that Microsoft offers. One nice thing about this book is that it includes lots of snapshots of the Azure Machine Learning interface, which gives you a clear mental map of the workflow even if you don‚Äôt have an Azure account. . Two Udemy courses . Here‚Äôre the two Udemy courses that I took. . DP-100 Microsoft Azure Data Scientist Complete Exam Prep taught by Scott Duffy | DP-100: A-Z Machine Learning using Azure Machine Learning taught by Jitesh Khurkhuriya | . The first one is about 2.5 hours long and the second one has a staggering length of 32 hours, but they cost the same (i.e. NT$2,990 last time I checked)! The second one is long primarily because it covers a lot of the classic Azure ML Studio, which is gradually replaced by the updated Azure ML Designer. Also included is a primer on Python programming. I skipped this part, which is why I didn‚Äôt finish the second course, as indicated in my Udemy learning history. . . I don‚Äôt remember much from the first course while I find it hard to say the same to the second one. Its instructor, Jitesh, has a very personalized style of intonation, which makes even the dullest stuff sound interesting. He is not only pretty good at explaining complex concepts in simple and intuitive ways, but also meticulous about every bit of details regarding the exam. My only complaint about his course is the low speed at which he talks. I had to set the playback speed to x1.5 faster to stay awake. . Three kinds of official materials from Microsoft . Microsoft did an incredibly good job of making high-quality learning materials accessible to the public, which makes me feel that Microsoft really wants exam-takers to pass rather than fail. Here‚Äôre the three kinds of official materials that I went through. . 4 online learning paths Create machine learning models (5 Modules) | Use visual tools to create machine learning models with Azure Machine Learning (4 Modules) | Build and operate machine learning solutions with Azure Machine Learning (14 Modules) | Perform data science with Azure Databricks (12 Modules) | . | 17 ipynb files from the MicrosoftLearning/mslearn-dp100 repo | Documentation for Algorithm &amp; module reference for Azure Machine Learning designer | . The learning paths give you a high-level overview of what Azure ML products can do. The ipynb files cover pretty much all you need to know about Azure Machine Learning SDK for Python as far as the exam is concerned. They are so helpful that I forked the whole repo into this one, to which I added a note of crucial codes taken from all the 17 ipynb files. The last material is specfically about Azure ML Designer, and I made a summary note of the entire documentation. . . To navigate this long note, press the button with three bars and dots at the upper left corner of the markdown page. Some final notes about the exam. In hindsight, I believe I could still pass the exam even without taking the Udemy courses, considering the official materials from Microsoft already cover the great majority of questions. Next, the price for taking the exam varies depending on the country you choose even if you take it online. I did it online (who wouldn‚Äôt during the pandemic!), and the price for Taiwan is $125 USD (but $165 USD instead for the USA). Finally, you have a few options for the exam language, which can be English, Japanese, Chinese, or some others. But the exam proctor only speaks English or Japanese. I wonder why Microsoft offers Japanese as the only non-English language for proctors. .",
            "url": "https://howard-haowen.github.io/blog.ai/azure/certificate/2021/06/27/how-I-passed-Microsoft-DP100-Exam.html",
            "relUrl": "/azure/certificate/2021/06/27/how-I-passed-Microsoft-DP100-Exam.html",
            "date": " ‚Ä¢ Jun 27, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Classifying customer reviews with spaCy v3.0",
            "content": ". . Intro . Text classification is a very common NLP task. Given enough training data, it&#39;s relatively easy to build a model that can automatically classify previously unseen texts in a way that follows the logic of the training data. In this post, I&#39;ll go through the steps for building such a model. Specifically, I&#39;ll leverage the power of the recently released spaCy v3.0 to train two classification models, one for identifying the sentiment of customer reviews in Chinese as being positive or negative (i.e. binary classification) and the other for predicting their product categories in a list of five (i.e. multiclass classification). If you can&#39;t wait to see how spaCy v3.0 has made the training process an absolute breeze, feel free to jump to the training the textcat component with CLI section. If not, bear with me on this long journey. All the datasets and models created in this post are hosted in this repo of mine. . Preparing the dataset . Getting the dataset . I&#39;m hoping to build classification models that can take traditional Chinese texts as input, but I can&#39;t find any publicly available datasets of customer reviews in traditional Chinese. So I had to make do with reviews in simplified Chinese. Let&#39;s first download the dataset using !wget. . !wget https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip . --2021-03-07 14:08:42-- https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 4084428 (3.9M) [application/zip] Saving to: ‚Äòonline_shopping_10_cats.zip‚Äô online_shopping_10_ 100%[===================&gt;] 3.89M --.-KB/s in 0.1s 2021-03-07 14:08:43 (37.1 MB/s) - ‚Äòonline_shopping_10_cats.zip‚Äô saved [4084428/4084428] . . Then we unzip the downloaded file online_shopping_10_cats.zip with, surprisingly, !unzip. . !unzip online_shopping_10_cats.zip . Archive: online_shopping_10_cats.zip inflating: online_shopping_10_cats.csv . . The dataset has three columns: review for review texts, label for sentiment , and cat for product categories. Here&#39;s a random sample of five reviews. . import pandas as pd file_path = &#39;/content/online_shopping_10_cats.csv&#39; df = pd.read_csv(file_path) df.sample(5) . . cat label review . 35479 Ê¥óÂèëÊ∞¥ | 0 | ‰π∞‰∫Ü‰∏§Â•óËØ¥Â•ΩÁöÑËµ†ÂìÅÂêπÈ£éÊú∫Ê≤°ÁªôÔºÅ | . 35477 Ê¥óÂèëÊ∞¥ | 0 | Êä¢Ë¥≠Èôç‰ª∑‰∏ÄÂçäÔºüÂùëÔºåÁàπÔºüÊ≤°Ëµ∂‰∏äÊó∂ÂÄôÔºü | . 53299 ÈÖíÂ∫ó | 1 | Á¢∞‰∏äÈÖíÂ∫óÂÅöÊ¥ªÂä®ÔºåÂä†‰∫Ü40ÂÖÉÁªôÂçáÁ∫ßÂà∞Ë°åÊîøÊàø„ÄÇÊàøÈó¥Ëøò‰∏çÈîôÔºåÊØîËæÉÊñ∞„ÄÇÊúçÂä°ÂëòÊòØÂÆû‰π†ÁîüÔºå‰∏çÁÜüÁªÉ‰ΩÜÊÄÅÂ∫¶ËÆ§... | . 14367 ÊâãÊú∫ | 1 | 1ÔºâÂ§ñËßÇÊñ∞È¢ñ2ÔºâÊã•ÊúâÂº∫Â§ßÁöÑÂ§öÂ™í‰ΩìÂäüËÉΩÂíåÂçìË∂äÁöÑÊÄßËÉΩÔºåÂêåÊó∂Â∞ÜÁîµÊ±†ÁöÑÊ∂àËÄóÂáèÂà∞ÊúÄÂ∞èÔºåÊñπ‰æø‰∫ÜÊõ¥Â§öÁöÑÁî®Êà∑... | . 12549 Âπ≥Êùø | 0 | ÂàÜËæ®ÁéáÂ§™‰ΩéÔºå‰π∞ÁöÑÂêéÊÇî‰∫Ü. | . There&#39;re in total 62774 reviews. . df.shape . (62774, 3) . The label column has only two unique values, 1 for positive reviews and 0 for negative ones. . df.label.unique() . array([1, 0]) . The cat column has nine unique values. . df.cat.unique() . array([&#39;‰π¶Á±ç&#39;, &#39;Âπ≥Êùø&#39;, &#39;ÊâãÊú∫&#39;, &#39;Ê∞¥Êûú&#39;, &#39;Ê¥óÂèëÊ∞¥&#39;, &#39;ÁÉ≠Ê∞¥Âô®&#39;, &#39;ËíôÁâõ&#39;, &#39;Ë°£Êúç&#39;, &#39;ËÆ°ÁÆóÊú∫&#39;, &#39;ÈÖíÂ∫ó&#39;], dtype=object) . Before moving on, let&#39;s save the raw dataset to Google Drive. The dest variable can be any GDrive path you like. . dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/&quot; !cp {file_path} {dest} . Filtering the datases . Now let&#39;s do some data filtering. The groupby function from pandas is very useful, and here&#39;s how to get the counts of each of the unique values in the cat column. . df.groupby(by=&#39;cat&#39;).size() . cat ‰π¶Á±ç 3851 Âπ≥Êùø 10000 ÊâãÊú∫ 2323 Ê∞¥Êûú 10000 Ê¥óÂèëÊ∞¥ 10000 ÁÉ≠Ê∞¥Âô® 575 ËíôÁâõ 2033 Ë°£Êúç 10000 ËÆ°ÁÆóÊú∫ 3992 ÈÖíÂ∫ó 10000 dtype: int64 . To create a balanced dataset, I decided to keep categories whose counts are 10,000. So we&#39;re left with five product categories, Âπ≥Êùø for tablets, Ê∞¥Êûú for fruits, Ê¥óÂèëÊ∞¥ for shampoo, Ë°£Êúç for clothing, and finally ÈÖíÂ∫ó for hotels. . There&#39;re many ways to filter data in pandas, and my favorite is to first create a filt variable that holds a list of True and False, which in this particular case is whether the value in the cat volumn is in the cat_list variable for the categories to be kept. Then we can simply filter data with df[filt]. After filtering, the dataset is reduced to 50,000 reviews. . cat_list = [&#39;Âπ≥Êùø&#39;, &#39;Ê∞¥Êûú&#39;, &#39;Ê¥óÂèëÊ∞¥&#39;, &#39;Ë°£Êúç&#39;, &#39;ÈÖíÂ∫ó&#39;] filt = df[&#39;cat&#39;].isin(cat_list) df = df[filt] df.shape . (50000, 3) . Now, the dataset is balanced in terms of both the cat and label columnn. There&#39;re 10,000 reviews for each product category. . df.groupby(by=&#39;cat&#39;).size() . cat Âπ≥Êùø 10000 Ê∞¥Êûú 10000 Ê¥óÂèëÊ∞¥ 10000 Ë°£Êúç 10000 ÈÖíÂ∫ó 10000 dtype: int64 . And there&#39;re 25,000 for either of the two sentiments. . df.groupby(by=&#39;label&#39;).size() . label 0 25000 1 25000 dtype: int64 . Having made sure the filtered dataset is balanced, we can now reset the index, and save the dataset as online_shopping_5_cats_sim.csv. . df.reset_index(inplace=True, drop=True) df.to_csv(dest+&quot;online_shopping_5_cats_sim.csv&quot;, sep=&quot;,&quot;, index=False) . Converting the dataset to traditional Chinese . Let&#39;s load back the file we just saved to make sure the dataset is accessible for later use. . df = pd.read_csv(dest+&quot;online_shopping_5_cats_sim.csv&quot;) df.tail() . cat label review . 49995 ÈÖíÂ∫ó | 0 | Êàë‰ª¨ÂéªÁõêÂüéÁöÑÊó∂ÂÄôÈÇ£ÈáåÁöÑÊúÄ‰ΩéÊ∞îÊ∏©Âè™Êúâ4Â∫¶ÔºåÊôö‰∏äÂÜ∑ÂæóË¶ÅÊ≠ªÔºåÂ±ÖÁÑ∂Ëøò‰∏çÂºÄÁ©∫Ë∞ÉÔºåÊäïËØâÂà∞ÈÖíÂ∫óÂÆ¢ÊàøÈÉ®ÔºåÂæóÂà∞... | . 49996 ÈÖíÂ∫ó | 0 | ÊàøÈó¥ÂæàÂ∞èÔºåÊï¥‰ΩìËÆæÊñΩËÄÅÂåñÔºåÂíåÂõõÊòüÁöÑÂ∑ÆË∑ùÂæàÂ§ß„ÄÇÊØõÂ∑æÂ§™Á†¥Êóß‰∫Ü„ÄÇÊó©È§êÂæàÁÆÄÈôã„ÄÇÊàøÈó¥ÈöîÈü≥ÂæàÂ∑ÆÔºåÈöî‰∏§Èó¥ÊàøÈó¥... | . 49997 ÈÖíÂ∫ó | 0 | ÊàëÊÑüËßâ‰∏çË°å„ÄÇ„ÄÇ„ÄÇÊÄß‰ª∑ÊØîÂæàÂ∑Æ„ÄÇ‰∏çÁü•ÈÅìÊòØÈì∂Â∑ùÈÉΩËøôÊ†∑ËøòÊòØÊÄé‰πàÁöÑÔºÅ | . 49998 ÈÖíÂ∫ó | 0 | ÊàøÈó¥Êó∂Èó¥ÈïøÔºåËøõÂéªÊúâÁÇπÂºÇÂë≥ÔºÅÊúçÂä°ÂëòÊòØ‰∏çÊòØ‰∏çÂ§üÁî®ÂïäÔºÅÊàëÂú®‰∏ÄÊ•ºÊâæ‰∫ÜÂçä‰∏™Â∞èÊó∂‰ª•‰∏äÊâçÊâæÂà∞Ëá™Â∑±ÊàøÈó¥ÔºåÊÉ≥Êâæ... | . 49999 ÈÖíÂ∫ó | 0 | ËÄÅ‰∫∫Â∞èÂ≠©‰∏ÄÂ§ßÂÆ∂ÊóèËÅö‰ºöÔºåÈÄâÂú®Âê¥ÂÆ´Ê≥õÂ§™Âπ≥Ê¥ãÔºå‰ª•‰∏∫Êñ∞Âä†Âù°ÂìÅÁâå‰∏ÄÂÆöÂæà‰∏çÈîôÔºåÊ≤°ÊÉ≥Âà∞11ÁÇπ30ÂàÜÂà∞ÂâçÂè∞Ôºå... | . Next, I converted the reviews from simplified Chinese to traditional Chinese using the OpenCC library. . !pip install OpenCC . Collecting OpenCC Downloading https://files.pythonhosted.org/packages/d5/b4/24e677e135df130fc6989929dc3990a1ae19948daf28beb8f910b4f7b671/OpenCC-1.1.1.post1-py2.py3-none-manylinux1_x86_64.whl (1.3MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3MB 8.0MB/s Installing collected packages: OpenCC Successfully installed OpenCC-1.1.1.post1 . . OpenCC has many conversion methods. I specifically used s2twp, which converts simplified Chinese to traditional Chinese adpated to Taiwanese vocabulary. The adaptation is not optimal, but it&#39;s better than mechanic simplified-to-traditional conversion. Here&#39;s a random review in the two writing systems. . from opencc import OpenCC cc = OpenCC(&#39;s2twp&#39;) test = df.loc[49995, &#39;review&#39;] print(test) test_tra = cc.convert(test) print(test_tra) . Êàë‰ª¨ÂéªÁõêÂüéÁöÑÊó∂ÂÄôÈÇ£ÈáåÁöÑÊúÄ‰ΩéÊ∞îÊ∏©Âè™Êúâ4Â∫¶ÔºåÊôö‰∏äÂÜ∑ÂæóË¶ÅÊ≠ªÔºåÂ±ÖÁÑ∂Ëøò‰∏çÂºÄÁ©∫Ë∞ÉÔºåÊäïËØâÂà∞ÈÖíÂ∫óÂÆ¢ÊàøÈÉ®ÔºåÂæóÂà∞ÁöÑÁ≠îÂ§çÊòØÁé∞Âú®ËøòÊ≤°ÊúâÈ¢ÜÂØºÊåáÁ§∫ÈúÄË¶ÅÂºÄÊöñÊ∞îÔºåÂ¶ÇÊûúÂÜ∑Âà∞ËØùÂèØ‰ª•Â§öÁªô‰∏ÄÂ∫äË¢´Â≠êÔºåÂ§™ÂèØÊÄú‰∫Ü„ÄÇ„ÄÇ„ÄÇ ÊàëÂÄëÂéªÈπΩÂüéÁöÑÊôÇÂÄôÈÇ£Ë£°ÁöÑÊúÄ‰ΩéÊ∞£Ê∫´Âè™Êúâ4Â∫¶ÔºåÊôö‰∏äÂÜ∑ÂæóË¶ÅÊ≠ªÔºåÂ±ÖÁÑ∂ÈÇÑ‰∏çÈñãÁ©∫Ë™øÔºåÊäïË®¥Âà∞ÈÖíÂ∫óÂÆ¢ÊàøÈÉ®ÔºåÂæóÂà∞ÁöÑÁ≠îË¶ÜÊòØÁèæÂú®ÈÇÑÊ≤íÊúâÈ†òÂ∞éÊåáÁ§∫ÈúÄË¶ÅÈñãÊöñÊ∞£ÔºåÂ¶ÇÊûúÂÜ∑Âà∞Ë©±ÂèØ‰ª•Â§öÁµ¶‰∏ÄÂ∫äË¢´Â≠êÔºåÂ§™ÂèØÊÜê‰∫Ü„ÄÇ„ÄÇ„ÄÇ . Having made sure the conversion is correct, we can now go ahead and convert all reviews. . df.loc[ : , &#39;review&#39;] = df[&#39;review&#39;].apply(lambda x: cc.convert(x)) . Let&#39;s make the same change to the cat column. . df.loc[ : , &#39;cat&#39;] = df[&#39;cat&#39;].apply(lambda x: cc.convert(x)) . And then we save the converted dataset as online_shopping_5_cats_tra.csv. . df.to_csv(dest+&#39;online_shopping_5_cats_tra.csv&#39;, sep=&quot;,&quot;, index=False) . Inspecting the dataset . Let&#39;s load back the file just saved to make sure it&#39;s accessible in the future. . df = pd.read_csv(dest+&#39;online_shopping_5_cats_tra.csv&#39;) df.tail() . cat label review . 49995 ÈÖíÂ∫ó | 0 | ÊàëÂÄëÂéªÈπΩÂüéÁöÑÊôÇÂÄôÈÇ£Ë£°ÁöÑÊúÄ‰ΩéÊ∞£Ê∫´Âè™Êúâ4Â∫¶ÔºåÊôö‰∏äÂÜ∑ÂæóË¶ÅÊ≠ªÔºåÂ±ÖÁÑ∂ÈÇÑ‰∏çÈñãÁ©∫Ë™øÔºåÊäïË®¥Âà∞ÈÖíÂ∫óÂÆ¢ÊàøÈÉ®ÔºåÂæóÂà∞... | . 49996 ÈÖíÂ∫ó | 0 | ÊàøÈñìÂæàÂ∞èÔºåÊï¥È´îË®≠ÊñΩËÄÅÂåñÔºåÂíåÂõõÊòüÁöÑÂ∑ÆË∑ùÂæàÂ§ß„ÄÇÊØõÂ∑æÂ§™Á†¥Ëàä‰∫Ü„ÄÇÊó©È§êÂæàÁ∞°Èôã„ÄÇÊàøÈñìÈöîÈü≥ÂæàÂ∑ÆÔºåÈöîÂÖ©ÈñìÊàøÈñì... | . 49997 ÈÖíÂ∫ó | 0 | ÊàëÊÑüË¶∫‰∏çË°å„ÄÇ„ÄÇ„ÄÇÂÉπÊïàÊØîÂæàÂ∑Æ„ÄÇ‰∏çÁü•ÈÅìÊòØÈäÄÂ∑ùÈÉΩÈÄôÊ®£ÈÇÑÊòØÊÄéÈ∫ºÁöÑÔºÅ | . 49998 ÈÖíÂ∫ó | 0 | ÊàøÈñìÊôÇÈñìÈï∑ÔºåÈÄ≤ÂéªÊúâÈªûÁï∞Âë≥ÔºÅÊúçÂãôÂì°ÊòØ‰∏çÊòØ‰∏çÂ§†Áî®ÂïäÔºÅÊàëÂú®‰∏ÄÊ®ìÊâæ‰∫ÜÂçäÂÄãÂ∞èÊôÇ‰ª•‰∏äÊâçÊâæÂà∞Ëá™Â∑±ÊàøÈñìÔºåÊÉ≥Êâæ... | . 49999 ÈÖíÂ∫ó | 0 | ËÄÅ‰∫∫Â∞èÂ≠©‰∏ÄÂ§ßÂÆ∂ÊóèËÅöÊúÉÔºåÈÅ∏Âú®Âê≥ÂÆÆÊ≥õÂ§™Âπ≥Ê¥ãÔºå‰ª•ÁÇ∫Êñ∞Âä†Âù°ÂìÅÁâå‰∏ÄÂÆöÂæà‰∏çÈåØÔºåÊ≤íÊÉ≥Âà∞11Èªû30ÂàÜÂà∞ÂâçËá∫Ôºå... | . Before building models, I would normally inspect the dataset. There&#39;re many ways to do so. I recently learned that there&#39;s a trick on Colab which allows you to filter a dataset in an interactive manner. All it takes is three lines of code. . %load_ext google.colab.data_table from google.colab import data_table data_table.DataTable(df, include_index=False, num_rows_per_page=10) . Alternatively, if you&#39;d like to see some sample reviews from all the categories, the groupby function is quite handy. The trick here is to feed pd.DataFrame.sample to the apply function so that you can specify the number of reviews to inspect from each product category. . df.groupby(&#39;cat&#39;).apply(pd.DataFrame.sample, n=3)[[&#39;label&#39;, &#39;review&#39;]] . label review . cat . Âπ≥Êùø 6247 0 | ÈÄôÂÄãÂπ≥ÊùøÁúüÁöÑÊòØ3GÁöÑÂóéÔºü‰Ω†ÂÄëÊúâÊ≤íÊúâÂøΩÊÇ†ÂîâÔºåÁÇ∫‰ªÄÈ∫ºÊàë‰∏ã‰∫Ü‰∏ÄÂÄãÁôæÂ∫¶ÂΩ±ÁâáÔºåÂ∞±Âç°ÁöÑË¶ÅÊ≠ªË¶ÅÊ¥ªÁöÑÔºåË∑üÊàë‰ª•... | . 1081 1 | ÁúãÁ∂≤È†ÅÁé©ÁéãËÄÖÊ¶ÆËÄÄÈÉΩÂæàÊµÅÊö¢ÔºåÈü≥Ë≥™Áï´Èù¢ÈÉΩ‰∏çÈåØÔºåÂ∞±ÊòØÁ®çÂæÆÈáç‰∫ÜÈªûÔºåÁ∂úÂêàÊÄßÂÉπÊØîÈÇÑÊòØÂæàÂ•ΩÁöÑ | . 1042 1 | ÊàëË¶∫ÂæóÈÇÑÂèØ‰ª•ÔºåÂ∞±ÊòØÊääËÜúË≤º‰∏ä‰∫Ü‰πãÂæåÔºåÊúâÈªûÊªë‰∏çÂãïÔºåÊâìÈÅäÊà≤ÁöÑÊôÇÂÄôÂ∞±ÂæàÁÖ©‰∫Ü | . Ê∞¥Êûú 10468 1 | ÈÇÑ‰∏çÈåØÔºåÈÄôÂÄãÂÉπÊ†ºÊØîÊàëÂú®Â§ñÈù¢Ë≤∑ÁöÑÂàíÁÆóÔºå‰ª•ÂæåÈÇÑÊúÉÁ∂ìÂ∏∏‰æÜÔºåÂÄãÈ†≠‰∏çÊòØÂæàÂ§ßÔºåÈÇÑÂèØ‰ª•Âêß | . 10986 1 | ËòãÊûúÂë≥ÈÅìÂ•ΩÔºåÂ∞±ÊòØÂ∞è‰∫Ü‰∏ÄÈªûÔºåÊØîÊÉ≥Ë±°ÁöÑË¶ÅÂ∞èÈáè‰∫Ü‰∏Ä‰∏ãÔºåÂ•ΩÂÉèÂü∫Êú¨‰∏äÈÉΩÊ≤íÂà∞70ÊØ´Á±≥„ÄÇÂø´ÈÅûÈÇÑÊòØÊå∫Âø´ÁöÑÂåÖË£ù... | . 18062 0 | ÈÄôÊòØÊàëÂú®‰∫¨Êù±Ê∂àË≤ªÈÄôÈ∫ºÂ§öÂπ¥‰æÜË≤∑Âà∞ÂîØ‰∏ÄÊ¨°ÊúÄÁàõÁöÑÊù±Ë•øÔºåÈÇÑÊòØËá™Ááü‰∏ÄÊñ§6Â°äÈå¢ÁöÑÂ∞±ÈÄôË≤®Ëâ≤ÈÇÑÊúâ‰∏ÄÂÄãÊòØÂ£ûÁöÑÔºå... | . Ê¥óÈ´ÆÊ∞¥ 23271 1 | ÂæàÂ•ΩÔºåÂæàËàíÊúçÔºåÊ∏ÖÊèöÂ∞±ÊòØÂ•ΩÁî®ÔºåË¨ùË¨ùËÄÅÈóÜÔºåÂ∏åÊúõ‰∏ÄÁõ¥Â•ΩÁî®ÔºåÂ•ΩÂ•ΩÂ•ΩÂ•ΩÂ•ΩÂ•ΩÂ•ΩÂ•ΩÂ•ΩÔºåÂø´Ê®Ç | . 21992 1 | ‰∏ÄÂ¶ÇÊó¢ÂæÄÁöÑÂ•Ω ‰∫¨Êù±ÈÄüÂ∫¶Âø´ ÂÄºÂæó‰ø°Ë≥¥ ÂÑ™ÊÉ†Â§öÂ§ö | . 21867 1 | ‰∫¨Êù±Ë≥ºÁâ© Â§öÂø´Â•ΩÁúÅ ÂØ´Ë©ïË´ñÁúüÁöÑÂæàÁ¥Ø ÊúâÊú®Êúâ ÊØèÊ¨°ÂïÜÂìÅÂæàÊªøÊÑèÂ∞±Áî®ÈÄôÂÄã ÂêÑ‰ΩçÂ§ß‰Ω¨Ë´ãÊîæÂøÉË≥ºË≤∑ | . Ë°£Êúç 30157 1 | Ë§≤Â≠êË≥™Èáè‰∏çÈåØÔºåË≤®ÁúüÂÉπÂØ¶ÔºåÂíåÂ∫óÂÆ∂‰ªãÁ¥πÁöÑÂü∫Êú¨Áõ∏Á¨¶ÔºåÂ§ßÂ∞èÂêàÈÅ©ÔºåÊ®£Âºè‰πüÂæàÊªøÊÑèÔºåÁ©ø‰∏äË§≤Â≠êËµ∞Ë∑ØÊÑüÂà∞ÂæàËºï‰æøÔºåËàíÊúç | . 33190 1 | ÂÅöÂ∑•Á≤æÁ¥∞Á©øËµ∑‰æÜÂæàËàíÊúçÔºåË≥™ÈáèÂæàÂ•Ω | . 35326 0 | Ë≥™ÈáèÂæàÂ∑ÆÔºåÊ≤íÊúâÊÉ≥Ë±°ÁöÑÈÇ£È∫ºÂ•Ω | . ÈÖíÂ∫ó 42243 1 | ÊàøÈñìÊòØÊñ∞Ë£ù‰øÆÁöÑ,Áî®ÁöÑÊòØÊ∑∫Ëâ≤Ë™ø,ÊÑüË¶∫ÂæàÊ∫´È¶®,‰ΩàÂ±ÄËºÉÂêàÁêÜ,È°ØÁöÑÊØîËºÉÂØ¨Êïû.ÈÖíÂ∫óÈÅ∏Áî®ÁöÑÂ∏ÉËçâÂæàË¨õÁ©∂,Êîæ... | . 48082 0 | ÂØ¶Âú®ÂøçÁÑ°ÂèØÂøç„ÄÇ1„ÄÅÊ∞¥„ÄÇÁº∫Ê∞¥ÁèæË±°‚Äî‚ÄîÊ¥óÊæ°Ëá≥‰∏≠ÈÄîÔºåÁ™ÅÁÑ∂Êñ∑Ê∞¥ÔºåÊúõËëóÊªøË∫´ÁöÑËÇ•ÁöÇÊ≥°Ê¨≤Âì≠ÁÑ°Ê∑öÔºõÂ§öÊ∞¥ÁèæË±°‚Äî... | . 49552 0 | ‰∏ãÈõ®Â§©ÂÜ∑ÔºåÊÉ≥Ê¥óÁÜ±Ê∞¥Êæ°ÔºåÂèØÊÉúÈñã‰∫ÜÂçäÂ∞èÊôÇÊ∞¥ÈÇÑÊòØÂÜ∑ÁöÑ | . Finally, one of the most powerful ways of exploring a dataset is to use the facets-overview library. Let&#39;s first create a column for the length of review texts. . df[&#39;len&#39;] = df[&#39;review&#39;].apply(len) df.tail() . cat label review len . 49995 ÈÖíÂ∫ó | 0 | ÊàëÂÄëÂéªÈπΩÂüéÁöÑÊôÇÂÄôÈÇ£Ë£°ÁöÑÊúÄ‰ΩéÊ∞£Ê∫´Âè™Êúâ4Â∫¶ÔºåÊôö‰∏äÂÜ∑ÂæóË¶ÅÊ≠ªÔºåÂ±ÖÁÑ∂ÈÇÑ‰∏çÈñãÁ©∫Ë™øÔºåÊäïË®¥Âà∞ÈÖíÂ∫óÂÆ¢ÊàøÈÉ®ÔºåÂæóÂà∞... | 86 | . 49996 ÈÖíÂ∫ó | 0 | ÊàøÈñìÂæàÂ∞èÔºåÊï¥È´îË®≠ÊñΩËÄÅÂåñÔºåÂíåÂõõÊòüÁöÑÂ∑ÆË∑ùÂæàÂ§ß„ÄÇÊØõÂ∑æÂ§™Á†¥Ëàä‰∫Ü„ÄÇÊó©È§êÂæàÁ∞°Èôã„ÄÇÊàøÈñìÈöîÈü≥ÂæàÂ∑ÆÔºåÈöîÂÖ©ÈñìÊàøÈñì... | 102 | . 49997 ÈÖíÂ∫ó | 0 | ÊàëÊÑüË¶∫‰∏çË°å„ÄÇ„ÄÇ„ÄÇÂÉπÊïàÊØîÂæàÂ∑Æ„ÄÇ‰∏çÁü•ÈÅìÊòØÈäÄÂ∑ùÈÉΩÈÄôÊ®£ÈÇÑÊòØÊÄéÈ∫ºÁöÑÔºÅ | 29 | . 49998 ÈÖíÂ∫ó | 0 | ÊàøÈñìÊôÇÈñìÈï∑ÔºåÈÄ≤ÂéªÊúâÈªûÁï∞Âë≥ÔºÅÊúçÂãôÂì°ÊòØ‰∏çÊòØ‰∏çÂ§†Áî®ÂïäÔºÅÊàëÂú®‰∏ÄÊ®ìÊâæ‰∫ÜÂçäÂÄãÂ∞èÊôÇ‰ª•‰∏äÊâçÊâæÂà∞Ëá™Â∑±ÊàøÈñìÔºåÊÉ≥Êâæ... | 64 | . 49999 ÈÖíÂ∫ó | 0 | ËÄÅ‰∫∫Â∞èÂ≠©‰∏ÄÂ§ßÂÆ∂ÊóèËÅöÊúÉÔºåÈÅ∏Âú®Âê≥ÂÆÆÊ≥õÂ§™Âπ≥Ê¥ãÔºå‰ª•ÁÇ∫Êñ∞Âä†Âù°ÂìÅÁâå‰∏ÄÂÆöÂæà‰∏çÈåØÔºåÊ≤íÊÉ≥Âà∞11Èªû30ÂàÜÂà∞ÂâçËá∫Ôºå... | 455 | . Then we install the library. . !pip install facets-overview . Collecting facets-overview Downloading https://files.pythonhosted.org/packages/df/8a/0042de5450dbd9e7e0773de93fe84c999b5b078b1f60b4c19ac76b5dd889/facets_overview-1.0.0-py2.py3-none-any.whl Requirement already satisfied: protobuf&gt;=3.7.0 in /usr/local/lib/python3.7/dist-packages (from facets-overview) (3.12.4) Requirement already satisfied: pandas&gt;=0.22.0 in /usr/local/lib/python3.7/dist-packages (from facets-overview) (1.1.5) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from facets-overview) (1.19.5) Requirement already satisfied: six&gt;=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf&gt;=3.7.0-&gt;facets-overview) (1.15.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf&gt;=3.7.0-&gt;facets-overview) (53.0.0) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.22.0-&gt;facets-overview) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.22.0-&gt;facets-overview) (2018.9) Installing collected packages: facets-overview Successfully installed facets-overview-1.0.0 . . In order to render an interative visualization of the dataset, we first convert the DataFrame object df to the json format and then add it to an HTML template, as shown below. If you choose len for Binning | X-Axis, cat for Binning | Y-Axis, and finally review for Label By, you&#39;ll see all the reviews are beautifully arranged in term of text length along the X axis and product categories along the Y axis. They&#39;re also color-coded with respect to sentiment, blue for positive and red for negative. Clicking on a point of either color shows the values of that particular datapoint. Feel free to play around. . from IPython.core.display import display, HTML jsonstr = df.to_json(orient=&#39;records&#39;) HTML_TEMPLATE = &quot;&quot;&quot; &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;import&quot; href=&quot;https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html&quot;&gt; &lt;facets-dive id=&quot;elem&quot; height=&quot;600&quot;&gt;&lt;/facets-dive&gt; &lt;script&gt; var data = {jsonstr}; document.querySelector(&quot;#elem&quot;).data = data; &lt;/script&gt;&quot;&quot;&quot; html = HTML_TEMPLATE.format(jsonstr=jsonstr) display(HTML(html)) . . Training spaCy models . Instantiating a pretrained spaCy model . spaCy supports many pretrained models in multiple languages, and offers a convenient widget for working out the code for downloading a particular model for a particular language. I specifically picked zh_core_web_md for Chinese. . !pip install -U pip setuptools wheel !pip install -U spacy !python -m spacy download zh_core_web_md . Everthing in spaCy starts with loading a model. The model we downloaded has five built-in components, accessible via the pipe_names attribute. . import spacy nlp=spacy.load(&quot;zh_core_web_md&quot;) nlp.pipe_names . [&#39;tok2vec&#39;, &#39;tagger&#39;, &#39;parser&#39;, &#39;ner&#39;, &#39;attribute_ruler&#39;] . Let&#39;s call the nlp object with a sample review text and print out its tokens to make sure the model is working. . test = df.loc[22, &#39;review&#39;] doc = nlp(test) for tok in doc: print(tok.text, tok.pos_) . Ë≤∑ VERB ‰∫Ü PART ÈÄÅ‰∫∫ NOUN ÁöÑ PART Ôºå PUNCT Ê≤íÊúâ VERB ÊãÜÈñã NOUN Ôºå PUNCT ÂåÖË£ù VERB È´òÂ§ß‰∏ä NOUN Ôºå PUNCT ‰ª•Ââç NOUN Ëá™Áî® VERB Ë≤∑ VERB ‰∫Ü PART ‰∏Ä NUM ÂÄã NUM Ôºå PUNCT ÈÄôÊ¨° ADJ Ê¥ªÂãï NOUN ‰æøÂÆú VERB Â•Ω VERB ÂπæÁôæ VERB Ôºå PUNCT ÂÉπÊ†º NOUN Â•Ω ADV ÂàíÁÆó VERB Âïä PART Ôºå PUNCT ‰ª•Âæå NOUN ÈÇÑÊúÉ VERB ÈôçÂÉπ NOUN Âóé VERB ÔºüÔºüÔºü PUNCT . . Converting the dataset to spaCy format for training . To train a classification model wtih spaCy, we have to convert our dataset to spaCy format. Before that, we&#39;ll create a directory called data under the current working directory cwd. This is where we&#39;ll save the data in spaCy format. . . Tip: I find the os.makedirs function much more useful than the more commonly seen os.mkdir() function because the former creates all the intermediate directories for you if they don&#8217;t exist yet. . import os def create_dir(dir_name): cwd = os.getcwd() project_dir = os.path.join(cwd, dir_name) os.makedirs(project_dir) return project_dir project_dir = create_dir(&quot;data&quot;) project_dir . . &#39;/content/data&#39; . The first step for the conversion is to create a list of tuples with two elements, one for the text and the other for the text class label. Let&#39;s start with the binary classification for sentiment first and generalize to multiclass classification for product categories later. . The easiest way to generate such a list is to create a new column called tuples (or whatever), whose values are derived by applying a lambda function to review for text and label for text class. I learned this trick from this article. Here&#39;re the first 10 tuples in the newly created dataset list. . df[&#39;tuples&#39;] = df.apply(lambda row: (row[&#39;review&#39;], row[&#39;label&#39;]), axis=1) dataset = df[&#39;tuples&#39;].tolist() dataset[:10] . [(&#39; ufeffÂæà‰∏çÈåØ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇÂæàÂ•ΩÁöÑÂπ≥Êùø&#39;, 1), (&#39;Âπ´ÂêåÂ≠∏Ë≤∑ÁöÑÔºåÂêåÂ≠∏Ë™™ÊÑüË¶∫Êå∫Â•ΩÔºåË≥™Èáè‰πü‰∏çÈåØ&#39;, 1), (&#39;Êù±Ë•ø‰∏çÈåØÔºå‰∏ÄÁúãÂ∞±ÊòØÊ≠£ÂìÅÂåÖË£ùÔºåÈÇÑÊ≤íÊúâÈñãÊ©üÔºåÁõ∏‰ø°‰∫¨Êù±ÔºåÈÉΩÊòØËÄÅÈ°ßÂÆ¢ÔºåÈÇÑÊòØ‰∫¨Êù±ÂÄºÂæó‰ø°Ë≥¥ÔºåÁµ¶‰∫îÊòüÂ•ΩË©ï&#39;, 1), (&#39;Á∏ΩÈ´îËÄåË®ÄÔºåÁî¢ÂìÅÈÇÑÊòØ‰∏çÈåØÁöÑ„ÄÇ&#39;, 1), (&#39;Â•ΩÔºå‰∏çÈåØÔºåÁúüÁöÑÂæàÂ•Ω‰∏çÈåØ&#39;, 1), (&#39;ÂæàÂ•ΩÔºåÈü≥ÈüøÊïàÊûú‰∏çÈåØ„ÄÇÊå∫ÂñúÊ≠°ÁöÑÔºåÁî®‰∫Ü‰∏ÄÊÆµÊôÇÈñìÊâç‰æÜË©ïÂÉπÁöÑ„ÄÇ&#39;, 1), (&#39;ÂåÖË£ù‰∏çÊòØÂæàÂ•ΩÔºåË£°Èù¢Â§™Á©∫‰∫ÜÊàë‰∏çÂ°ûÂ§öÈªûÊ±ΩÊ≥°Ë¢ãÔºåÂÖ∂ÂÆÉÁöÑÈÇÑÂèØ‰ª•&#39;, 1), (&#39;‰πãÂâç‰∏ÄÁõ¥Áî®ËèØÁÇ∫ÊâãÊ©üÔºåË¶∫Âæó‰∏çÈåØÔºåÊÉ≥Ë©¶‰∏Ä‰∏ãÂπ≥ÊùøÔºåÂâçÂ§©‰∏ãÂñÆÔºåÊò®Â§©Âà∞Ë≤®ÔºåË©¶‰∫Ü‰∏Ä‰∏ãÔºåÊÑüË¶∫ÈÇÑ‰∏çÈåØÔºåÂÆ∂Ë£°‰πüÊúâipadÔºåÂÖ©ËÄÖ‰∏çËÉΩÊØîËºÉÔºåÂêÑÊúâÂêÑÁöÑÂ•ΩÔºåÂè™ÊòØËøëÂπ¥‰∏ÄÁõ¥ÈÉΩÁî®ËèØÁÇ∫ÔºåÊâãÊ©üÔºåÁõíÂ≠êÁî®Ëµ∑‰æÜÈÉΩ‰∏çÈåØÔºåÁèæÂú®ÊòØËÄÅÂ™ΩÁî®ËòãÊûúÊâãÊ©üÁî®ipadÔºåÊàëÈÉΩÁî®ËèØÁÇ∫ÔºåÁî®ÁøíÊÖ£‰∫Ü„ÄÇÂ§ßÂ∞è‰πüÊ≠£ÂêàÈÅ©ÔºåÊå∫ÂñúÊ≠°ÁöÑÔºåÂîØ‰∏ÄÊ¨†Áº∫ÁöÑ‰∏ÄÈªûÂ∞±ÊòØÊ≤íÊúâËÄ≥Ê©ü„ÄÇÂæåÈù¢Â§öÁî®ÂπæÊ¨°ÂÜçËøΩË©ïÂêß„ÄÇ&#39;, 1), (&#39;Ë™™ÂØ¶Ë©±ÔºåÈùûÂ∏∏ÂñúÊ≠°ÔºåÈÄôÂÄãÊòØÈÄÅÁµ¶ÂÆ¢Êà∂ÁöÑÔºå‰πãÂâçÈÄÅÁöÑËòãÊûúÊ¥æÁöÑÔºå‰ΩÜÊòØ‰∏çËÉΩÊã∑Ê™îÊ°àÔºåËá™ÂæûÊâæÂà∞ÈÄôÊ¨æÔºåÁâ©ÁæéÂÉπÂªâÔºåÁ∂ìÊøüÂØ¶ÊÉ†ÔºåÂæàÂñúÊ≠°ÔºÅÔºÅÔºÅË≤∑‰∫ÜÂ•ΩÂπæÂÄã‰∫Ü&#39;, 1), (&#39;Á∫åËà™ËÉΩÂäõ‰πüÂ§™Â∑Æ‰∫ÜÂêßÔºåÁúãÂΩ±Áâá1ÂÄãÂ∞èÊôÇÂ∞±25%‰∫Ü&#39;, 1)] . Then I split the dataset using train_test_split from scikit-learn. The train_data and valid_data hold 80% and 20% of the dataset, respectively. . from sklearn.model_selection import train_test_split train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1) . Then we need to turn the dataset list into spaCy Doc objects and assign text classes to each of them so that the model can start learning. Assigning text classes is the trickiest part and took me lots of trials and errors to figure out. Unfortunately, official documentation of spaCy is not very clear about this part. At first, I used this make_docs function from p-sodmann/Spacy3Textcat to create Doc objects. That was successful, but the trained model gave weird results. Then I realized that the values of text classes need to be either True or False. So here&#39;s my revised version of the make_docs function. The trick here is to use bool(label), which will be True if the value of the label is 1 and False if its value is 0. . from tqdm.auto import tqdm from spacy.tokens import DocBin def make_docs(data): &quot;&quot;&quot; this will take a list of texts and labels and transform them in spacy documents texts: List(str) labels: List(labels) returns: List(spacy.Doc.doc) &quot;&quot;&quot; docs = [] # nlp.pipe([texts]) is way faster than running nlp(text) for each text # as_tuples allows us to pass in a tuple, the first one is treated as text # the second one will get returned as it is. for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)): # we need to set the (text)cat(egory) for each document doc.cats[&quot;POSITIVE&quot;] = bool(label) doc.cats[&quot;NEGATIVE&quot;] = not bool(label) # put them into a nice list docs.append(doc) return docs . spaCy v3.0 introduces the DocBin class, which is the recommended container for serializing a list of Doc objects, much like the pickle format, but better. After we create a list of Doc objects with the make_docs function, we can then generate an instance of the DocBin class for holding that list and save the serialized object to disk by calling the to_disk function. We first do this to valid_data and save the serialized file as valid.spacy in the data directory. . valid_docs = make_docs(valid_data) doc_bin = DocBin(docs=valid_docs) doc_bin.to_disk(&quot;./data/valid.spacy&quot;) . Then we do the same thing to train_data and save it as train.spacy. This will take much more time to run since train_data is much larger than valid_data. . train_docs = make_docs(train_data) doc_bin = DocBin(docs=train_docs) doc_bin.to_disk(&quot;./data/train.spacy&quot;) . And that was the end of the converting process! Now I&#39;d like to save the serialized data for later use, so I copied it to dest, which is a directory in my Google Drive. . . Note: Remember to use the -R flag when copying whatever in a directory to another. . source = &quot;/content/data&quot; dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-sentiment-model/&quot; !cp -R {source} {dest} . Training the textcat component with CLI . spaCy v3.0 offers a nice and easy way to train spaCy&#39;s textcat component with a command line interfact (CLI). The first step is to get the base_config.cfg file, which is generated for you if you use spaCy&#39;s quickstart widget. The only thing that needs to changed is train and dev under [paths], which indicate where the serialized files are stored. . %%writefile base_config.cfg # This is an auto-generated partial config. To use it with &#39;spacy train&#39; # you can run spacy init fill-config to auto-fill all default settings: # python -m spacy init fill-config ./base_config.cfg ./config.cfg [paths] train = &quot;data/train.spacy&quot; dev = &quot;data/valid.spacy&quot; [system] gpu_allocator = null [nlp] lang = &quot;zh&quot; pipeline = [&quot;tok2vec&quot;,&quot;textcat&quot;] batch_size = 1000 [components] [components.tok2vec] factory = &quot;tok2vec&quot; [components.tok2vec.model] @architectures = &quot;spacy.Tok2Vec.v2&quot; [components.tok2vec.model.embed] @architectures = &quot;spacy.MultiHashEmbed.v1&quot; width = ${components.tok2vec.model.encode.width} attrs = [&quot;ORTH&quot;, &quot;SHAPE&quot;] rows = [5000, 2500] include_static_vectors = false [components.tok2vec.model.encode] @architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot; width = 96 depth = 4 window_size = 1 maxout_pieces = 3 [components.textcat] factory = &quot;textcat&quot; [components.textcat.model] @architectures = &quot;spacy.TextCatBOW.v1&quot; exclusive_classes = true ngram_size = 1 no_output_layer = false [corpora] [corpora.train] @readers = &quot;spacy.Corpus.v1&quot; path = ${paths.train} max_length = 2000 [corpora.dev] @readers = &quot;spacy.Corpus.v1&quot; path = ${paths.dev} max_length = 0 [training] dev_corpus = &quot;corpora.dev&quot; train_corpus = &quot;corpora.train&quot; [training.optimizer] @optimizers = &quot;Adam.v1&quot; [training.batcher] @batchers = &quot;spacy.batch_by_words.v1&quot; discard_oversize = false tolerance = 0.2 [training.batcher.size] @schedules = &quot;compounding.v1&quot; start = 100 stop = 1000 compound = 1.001 [initialize] vectors = null . . Writing base_config.cfg . Then we create the config.cfg configuration file with base_config.cfg by using this command. . !python -m spacy init fill-config ./base_config.cfg ./config.cfg . 2021-03-05 01:43:52.105583: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 ‚úî Auto-filled config with all values ‚úî Saved config config.cfg You can now add your data and train your pipeline: python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy . Here comes the fun part, where we can see how a computer model learns over time! The initial score was 43, which is worse than chance. But only after 200 iterations, the score already skyrocketed to 83! By the end of the training, we got a score of 92, which is pretty awesome, considering that we didn&#39;t do any text preprocessing. . !python -m spacy train ./config.cfg --output ./output . 2021-03-05 01:44:00.345586: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 ‚úî Created output directory: output ‚Ñπ Using CPU =========================== Initializing pipeline =========================== Set up nlp object from config Pipeline: [&#39;tok2vec&#39;, &#39;textcat&#39;] Created vocabulary Finished initializing nlp object Initialized pipeline components: [&#39;tok2vec&#39;, &#39;textcat&#39;] ‚úî Initialized pipeline ============================= Training pipeline ============================= ‚Ñπ Pipeline: [&#39;tok2vec&#39;, &#39;textcat&#39;] ‚Ñπ Initial learn rate: 0.001 E # LOSS TOK2VEC LOSS TEXTCAT CATS_SCORE SCORE - 0 0 0.00 0.06 43.32 0.43 0 200 0.00 34.18 83.25 0.83 0 400 0.00 21.43 86.60 0.87 0 600 0.00 12.77 87.25 0.87 0 800 0.00 15.29 89.00 0.89 0 1000 0.00 11.78 89.71 0.90 0 1200 0.00 5.92 89.62 0.90 0 1400 0.00 4.52 89.98 0.90 0 1600 0.00 1.51 90.40 0.90 0 1800 0.00 4.41 90.82 0.91 0 2000 0.00 0.80 90.76 0.91 0 2200 0.00 1.53 90.97 0.91 0 2400 0.00 0.16 91.06 0.91 0 2600 0.00 0.37 91.34 0.91 0 2800 0.00 0.13 91.33 0.91 0 3000 0.00 0.16 91.40 0.91 0 3200 0.00 0.20 91.56 0.92 0 3400 0.00 0.38 91.57 0.92 0 3600 0.00 0.10 91.70 0.92 1 3800 0.00 0.12 91.41 0.91 1 4000 0.00 0.17 91.65 0.92 1 4200 0.00 0.10 91.54 0.92 1 4400 0.00 0.14 91.58 0.92 1 4600 0.00 0.15 91.58 0.92 1 4800 0.00 0.19 91.59 0.92 1 5000 0.00 0.09 91.66 0.92 1 5200 0.00 2.16 91.91 0.92 1 5400 0.00 0.12 91.86 0.92 1 5600 0.00 0.21 91.68 0.92 1 5800 0.00 0.10 91.69 0.92 2 6000 0.00 2.18 91.22 0.91 2 6200 0.00 0.15 91.50 0.92 2 6400 0.00 0.16 91.61 0.92 2 6600 0.00 0.10 91.67 0.92 2 6800 0.00 0.11 91.57 0.92 ‚úî Saved pipeline to output directory output/model-last . . spaCy automatically saves two models to the path specified by the --output argument. We&#39;ll use the best model for testing. Here&#39;s the py file adapted from p-sodmann/Spacy3Textcat. . %%writefile test_input.py import spacy # load the best model from training nlp = spacy.load(&quot;output/model-best&quot;) text = &quot;&quot; print(&quot;type : &#39;quit&#39; to exit&quot;) # predict the sentiment until someone writes quit while text != &quot;quit&quot;: text = input(&quot;Please enter a review here: &quot;) doc = nlp(text) print(doc.cats) . . Writing test_input.py . %%writefile test_input.py import spacy # load the best model from training nlp = spacy.load(&quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-text-classification/spaCy-sentiment-model/model-best&quot;) text = &quot;&quot; print(&quot;type : &#39;quit&#39; to exit&quot;) # predict the sentiment until someone writes quit while text != &quot;quit&quot;: text = input(&quot;Please enter a review here: &quot;) doc = nlp(text) print(doc.cats) . . Writing test_input.py . Let&#39;s print out some reviews in valid_data for the sake of testing the model. . valid_data[:10] . [(&#39;Âíå‰ª•ÂâçË≤∑ÁöÑ‰∏ç‰∏ÄÊ®£ÔºÅ‰ª•ÂâçÁî®ÁöÑÁâπÂà•ÊªëÔºÅÁî®ÁöÑ‰πüÂ∞ëÔºÅÈÄôÊ¨°Ë≤∑ÁöÑ‰∏çÂÉÖÁî®ÁöÑÊù±Ë•øÂ§öÔºÅËÄå‰∏îÈÇÑÈ†≠ÁöÆÁô¢ÔºÅÊúâÈ†≠Â±ëÔºÅ&#39;, 0), (&#39;‰∏çÂ•ΩÊÑèÊÄùÊàëË≤®Êú™Êî∂Âà∞ÔºåÊàëÊÉ≥Ëµ∑Ë®¥‰Ω†ÂÄëÔºÅ&#39;, 0), (&#39;ÂØ∂Ë≤ùÊî∂Âà∞‰∫ÜÔºåÊòØÊàëÊÉ≥Ë¶ÅÁöÑÁâàÂûãÔºåË≥™ÈáèÂçÅÂàÜÂ•ΩÔºåÈÄôÊ¢ùË§≤Â≠êÁöÑÈ°èËâ≤Ë∑üÂúñÁâá‰∏ä‰∏ÄÊ®£ÔºåÊòØÊàëÊÉ≥Ë¶ÅÁöÑÈ°èËâ≤ÔºåÁ∏Ω‰πãÔºåÊòØ‰∏ÄÊ¨°ÂæàÊÑâÂø´ÁöÑË≥ºÁâ©&#39;, 1), (&#39;‰∏ÄÈªû‰πü‰∏çÁîú‰∏çÊ∏ÖËÑÜÔºåÂè™ËÉΩÊ¶®Ê±Å‰∫Ü„ÄÇ&#39;, 0), (&#39;ËòãÊûúÂ∞èÔºå‰ΩÜÂë≥ÈÅì‰∏çÈåØÔºåÁµ¶ÂÄãÂ•ΩË©ïÂêß&#39;, 1), (&#39;Âπ≥ÊùøÊî∂Âà∞ÔºåÂæàÊªøÊÑèÂíåÊÉ≥ÂÉèÁöÑ‰∏ÄÊ®£ÔºåÊ≤íÂ§±ÊúõÁ¨¨‰∏ÄÊôÇÈñìÈ´îÈ©ó‰∏≠ÔºåÁ®çÂæåÂÜçË©ï&#39;, 1), (&#39;Ë™™Â•ΩÁöÑÊ®ÇË¶ñÊúÉÂì°‰∏ÄÂπ¥ ÈÄÅÂì™Âéª‰∫Ü Â∑ÆË©ï&#39;, 0), (&#39;Áõ∏Áï∂ÂÆåÁæéÂïä Â•ΩÊù±Ë•ø&#39;, 1), (&#39;ÈÄ£ÁîüÁî¢ÁîüÊó•ÈÉΩÊ≤íÊúâÔºåÁúü‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÁúüÁöÑ„ÄÇ&#39;, 0), (&#39;ËòãÊûú‰∏ÄÂ¶ÇÊó¢ÂæÄÂæóÂ•ΩÔºåËÑÜ„ÄÅÁîú„ÄÅÊ∞¥ÂàÜË∂≥ÔºåÊé®Ëñ¶Ë≥ºË≤∑Âì¶Â•ßÔΩû&#39;, 1)] . Now let&#39;s run test_input.py to start grilling the model about the sentiment of reviews. The results are quite satisfactory. I even intentionally asked about a mixed review, ‰ªñÂÄëÂÆ∂ÁöÑÈ¶ôËïâÂ•ΩÂêÉÔºå‰ΩÜÊòØËòãÊûúÂçª‰∏ÄÈªû‰πü‰∏çÁîú! (Their bananas are delicious, but their apples are not sweet at all!). And the model gave almost equal scores to the two sentiments! . !python test_input.py . . 2021-03-05 05:16:18.092297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 type : &#39;quit&#39; to exit Please enter a review here: Âíå‰ª•ÂâçË≤∑ÁöÑ‰∏ç‰∏ÄÊ®£ÔºÅ‰ª•ÂâçÁî®ÁöÑÁâπÂà•ÊªëÔºÅÁî®ÁöÑ‰πüÂ∞ëÔºÅÈÄôÊ¨°Ë≤∑ÁöÑ‰∏çÂÉÖÁî®ÁöÑÊù±Ë•øÂ§öÔºÅËÄå‰∏îÈÇÑÈ†≠ÁöÆÁô¢ÔºÅÊúâÈ†≠Â±ëÔºÅ {&#39;POSITIVE&#39;: 0.022647695615887642, &#39;NEGATIVE&#39;: 0.9773523211479187} Please enter a review here: Âπ≥ÊùøÊî∂Âà∞ÔºåÂæàÊªøÊÑèÂíåÊÉ≥ÂÉèÁöÑ‰∏ÄÊ®£ÔºåÊ≤íÂ§±ÊúõÁ¨¨‰∏ÄÊôÇÈñìÈ´îÈ©ó‰∏≠ÔºåÁ®çÂæåÂÜçË©ï {&#39;POSITIVE&#39;: 0.43043994903564453, &#39;NEGATIVE&#39;: 0.5695600509643555} Please enter a review here: Ë™™Â•ΩÁöÑÊ®ÇË¶ñÊúÉÂì°‰∏ÄÂπ¥ ÈÄÅÂì™Âéª‰∫Ü Â∑ÆË©ï {&#39;POSITIVE&#39;: 0.20790322124958038, &#39;NEGATIVE&#39;: 0.792096734046936} Please enter a review here: ÈÄ£ÁîüÁî¢ÁîüÊó•ÈÉΩÊ≤íÊúâÔºåÁúü‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÁúüÁöÑ„ÄÇ {&#39;POSITIVE&#39;: 0.04043734818696976, &#39;NEGATIVE&#39;: 0.9595626592636108} Please enter a review here: ËòãÊûú‰∏ÄÂ¶ÇÊó¢ÂæÄÂæóÂ•ΩÔºåËÑÜ„ÄÅÁîú„ÄÅÊ∞¥ÂàÜË∂≥ÔºåÊé®Ëñ¶Ë≥ºË≤∑Âì¶Â•ßÔΩû {&#39;POSITIVE&#39;: 0.9967644214630127, &#39;NEGATIVE&#39;: 0.0032355356961488724} Please enter a review here: ‰ªñÂÄëÂÆ∂ÁöÑÈ¶ôËïâÂ•ΩÂêÉÔºå‰ΩÜÊòØ {&#39;POSITIVE&#39;: 0.7071358561515808, &#39;NEGATIVE&#39;: 0.2928641438484192} Please enter a review here: ‰ªñÂÄëÂÆ∂ÁöÑÈ¶ôËïâÂ•ΩÂêÉÔºå‰ΩÜÊòØËòãÊûúÂçª‰∏ÄÈªû‰πü‰∏çÁîú! {&#39;POSITIVE&#39;: 0.5702691674232483, &#39;NEGATIVE&#39;: 0.4297308325767517} Please enter a review here: quit {&#39;POSITIVE&#39;: 0.46642377972602844, &#39;NEGATIVE&#39;: 0.533576250076294} . Finally, I saved the best trained model to Google Drive. . source = &quot;/content/output/model-best&quot; dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-sentiment-model/&quot; !cp -R {source} {dest} . Going from binary to multiclass classification . Next, we&#39;ll go over pretty much the same steps to train a multiclass classification model. This time, the dataset is a list of tuples with review texts and product categories. . df[&#39;tuples&#39;] = df.apply(lambda row: (row[&#39;review&#39;], row[&#39;cat&#39;]), axis=1) dataset = df[&#39;tuples&#39;].tolist() dataset[-10:] . . [(&#39;ÈùûÂ∏∏‰∏ÄËà¨ÁöÑÈÖíÂ∫óÔºåÊàøË£°Ë®≠ÊñΩÂæàËàäÔºåÊàøÈñìÈÄÅÈ§êÁ´üÁÑ∂Ë¶ÅÂä†Â§ö50%ÁöÑÈÄÅÈ§êË≤ª„ÄÇÁ∏Ω‰πãÊâæ‰∏çÂà∞Â•ΩÁöÑÊñπÈù¢‰æÜË™™ÔºåÊúâÂÖ∂‰ªñÈÅ∏ÊìáÂ∞±‰∏çË¶ÅÂéª‰∫Ü&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;ÊàøÈñìÊ≤íÁ™óÊà∂ÔºåÊîúÁ®ãÁ∂≤Á´üÁÑ∂Ê≤íÊúâË™™ÊòéÔºÅ&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;1„ÄÅ‰∏≠ÂçàÂø´‰∏ÄÈªûÂà∞Â∫óÔºåË™™ÊàøÈñìÊ≤íÊúâÊî∂ÊãæÂá∫‰æÜÔºåÂè™Â•ΩÂØÑÂ≠òË°åÊùéÂá∫ÂéªÁé©ÔºåÊôö‰∏äÂõûÂà∞ÊàøÈñìÊªøÂ±ãÁÖôÂë≥ÔºåÊâìÈõªË©±ÂæåÁµ¶Ë™ø‰∫ÜÂêåÂûãÊàøÈñì„ÄÇÊòéÁü•ÈÅìÂÖ•‰ΩèÁöÑÊòØ‰∏ÄÂÆ∂‰∏âÂè£Ë¶™Â≠êÊ∏∏ÔºåÈÇÑÈÄôÊ®£ÂÆâÊéíÔºåÂàùÂßãÂç∞Ë±°ÂàÜ-1 2„ÄÅÈõªË¶ñÂæàÂ§öÈ†ªÈÅìÈÉΩÊòØÈõ™Ëä±ÔºåË™ø‰∫ÜÂπæÂÄãËá∫ÈÉΩÊòØÂ¶ÇÊ≠§Ôºå‰∏ÄÈªûÁúãÈõªË¶ñÁöÑËààËá¥ÈÉΩÊ≤í‰∫ÜÔºå-0.5ÊØ´‰∏çÈÅéÂàÜ 3„ÄÅÊàøÈñìË®≠ÊñΩ‰∏ÄÈªû‰∏ç‰∫∫ÊÄßÂåñÔºåÂÖàË™™ÁáàÔºåÈô§‰∫ÜÁ∏ΩÊéßÔºåÈÇÑË¶ÅÈñãÁáàÂÖ∑ÈñãÈóúÔºåÊôö‰∏äËµ∑Â§úÔºåÂâçÂâçÂæåÂæåÊëÅ‰∫Ü‰∫îÂÖ≠ÂÄãÈñãÈóúÊâçÊääÁáàÈñãÂïüÔºå‰∏ÄÈªûÁù°ÊÑèÈÉΩÊ≤í‰∫ÜÔºå-0.5‰∏çÂÜ§ÂêßÔºõÂÜçË™™Ê¥óÊæ°ÔºåÊµ¥Áº∏ÂæàÊ∑∫ÔºåÁ´ôÂú®Ë£°Èù¢Ê∑ãÊµ¥ÂÆåÂæåÂ§ñÈù¢Âú∞‰∏äÂÖ®ÊòØÊ∞¥ÔºåÂè™ËÉΩÂ¢äÂÄãÊµ¥Â∑æ‰πãÈ°ûÁöÑË∑≥Âà∞ÊàøÈñìÁ©øË°£ÔºåÈÇÑÂ•ΩÊàëÂíåÂ®ÉÂ•πÂ™ΩÈÇÑÊ≤íËÄÅÂà∞Ë∑≥‰∏çÂãï„ÄÇÁ¨¨‰∫åÂ§©Êôö‰∏ä‰∏ÄÂÆ∂‰∏âÂè£Ê≤í‰∏ÄÂÄã‰∫∫ÊúâÊ¥óÊæ°ÁöÑËààËá¥ÔºåÈÄôÂÄã-1ÁÆóÂÆ¢Ê∞£‰∫ÜÂêß 4„ÄÅÊúÄÂæå‰æÜË™™Êó©È§êÔºåÂâõÈÄ≤ÂéªÊúçÂãôÂì°‰∏ÄÊääÊîî‰ΩèÔºåÂ®ÉË∫´È´òË∂Ö‰∏ÄÁ±≥‰∫åÔºåË¶ÅÂÖ®ÂÉπ‰∏ÄÁôæ‰πùÔºåÈõñÁÑ∂ËÇâÁóõÔºå‰∏ÄÊÉ≥‰∫îÊòüÁ¥öÈ£ØÂ∫óÊó©È§êÊáâË©≤ÂÄºÈÄôÂÄãÂÉπÔºåÂ∞±Âéª‰ªò‰∫ÜÔºå‰∏ÄÂÄãÁî∑È†òÁè≠Ê®°Ê®£ÁöÑÂ∏∂Âà∞‰∏ÄÈÇäÔºåËÆì‰ªò‰πùÂçÅÁèæÈáëÂ∞±ÂèØ‰ª•‰∫Ü„ÄÇ‰ªòÂÆå‰∫ÜÈÇÑÁ´äÂñú‰Ωî‰∫Ü‰æøÂÆúÔºåÁ≠âÁ´Ø‰∫ÜÁõ§Â≠êÊâæÂêÉÁöÑÊôÇÊâçÁôºÁèæÂìÅÁ®ÆÂ∞ëÁöÑÂèØÊÜê„ÄÇÂ®É‰πãÂâçÂú®ÂõõÊòüÈÉΩËÉΩÂêÉÂà∞ÁöÑÂÜ∞ÊøÄÊ∑ãÈÄôÊ†πÊú¨Êâæ‰∏çÂà∞„ÄÇÈÇÑÊ≤íÂêÉÂÆåÂ®ÉÂ∞±ÂíåÊàëÂÄëÂòüÂô•ÔºåÊòéÂ§©ÊâìÊ≠ª‰πü‰∏ç‰æÜÂêÉ„ÄÇ-1ÁÆóÁêÜÊô∫‰∫ÜÂêß 5„ÄÅÊàëË¶ÅÈÄôÈ∫ºÁµ¶ÊÇ®Ê∏õÂÆå‰∫ÜÔºåÊÇ®‰º∞Ë®àÊúÉ‰ª•ÁÇ∫ÊàëÊåëÂâîÔºåÊïÖÊÑèÊâæÁ¢¥„ÄÇÈÄôÈ∫º‰æÜË™™ÂêßÔºåÁ®çÂæÆËÆìÊàëÂÄëÊªøÊÑèÁöÑÂ∞±ÊòØÁ©∫Ë™øÊ∫´Â∫¶‰∫ÜÔºåÈÇÑÊúâÁ¶ÆË≥ìÈÉ®ÂØÑÂ≠òË°åÊùéÁöÑÊúçÂãôÂì°‰∫Ü„ÄÇËÆìÊàëÊö´ÊôÇÂøòÂçª‰Ω†ÂÄëÊòØ‰∫îÊòüÁ¥öÈ£ØÂ∫óÂêßÔºåÈÄôË£°Áµ¶ÊÇ®+0.5 6„ÄÅÊ≤íÊúâÊØîËºÉÂ∞±Ê≤íÊúâÂ∑ÆË∑ùÔºåÈÅ†ÁöÑ‰∏çË™™ÔºåÈÄôÊ¨°‰æÜÂåó‰∫¨‰Ωè‰∫îÊôöÔºåÂâçÂÖ©Â§©ÊòØÁ¥ÖÊùâÂÅáÊó•ÈÖíÂ∫óÔºåÊàëÁµ¶Ë©ï‰∫Ü4.8ÔºåÂíåÊÇ®ÊØîËµ∑‰æÜÔºåÈô§‰∫ÜÂú∞ÊÆµÂ§ñÔºåÊÇ®Â∞±Ê≤íÂì™È†ÖËÉΩËêΩ‰∫ÜÂ•ΩÁöÑ„ÄÇ‰∫∫ÂÆ∂ÈÖíÂ∫óÂâõÂÖ•‰ΩèÊôÇ‰πüË™™‰∫ÜÂ∞èÂ≠©ÂçäÂÉπÊó©È§êÔºåÂèØË¶∫ÂæóÈÅéÂπ¥Êó©È§êÂìÅÁ®Æ‰∏çÂ§†Ë±êÂØåÔºåÂ∞±ÂÖç‰∫ÜÂ®ÉÁöÑË≤ªÁî®„ÄÇË´∑Âà∫ÁöÑÊòØÔºåÊàëÂÄë‰∏ÄÂÆ∂Â∞§ÂÖ∂ÊòØÂ®ÉË¶∫ÂæóÁ¥ÖÊùâÁöÑÊó©È§êÊØîÊÇ®ÈÄôË¶ÅË±êÁõõÂèØÂè£ÔºåÂîØ‰∏ÄÈÅ∫ÊÜæÁöÑÊòØÈÇ£‰πüÊ≤íÂÜ∞Ê∑áÊ∑ã„ÄÇ 7„ÄÅÊÇ®Ë¶ÅÊòØÂ∞±ÊÜëÂú∞ÊÆµÂ∞±ËÉΩÊãâÁ¥ÖÊùâ‰∏ÄÊôö‰∏âÁôæÁöÑÂ∑ÆË∑ùÔºåÊÇ®Ëá™ÂÄã‰πüË¶∫ÂæóÂøÉÂÆâÁêÜÂæó„ÄÇÈÇ£ÊàëÊèêÈÜíÊÇ®‰∏ÄÂè•Ôºö‰∏îË°å‰∏îÁèçÊÉú&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;ÈÖíÂ∫óÂæàËàä ÊàøÈñìÂæàÂ∞è„ÄÇÂÖ•‰ΩèÈ´îÈ©óÈùûÂ∏∏‰∏çÂ•Ω„ÄÇÊúçÂãôÈÇÑÂèØ‰ª•„ÄÇ‰∏ãÈù¢ÊòØÁõ§ÈñÄÊôØÂçÄÔºåÈÄôÈªû‰∏çÈåØ„ÄÇÂà∞ÂêÑÊôØÂçÄ‰∫§ÈÄöÊØîËºÉÊñπ‰æø„ÄÇ&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;ÈñãÂßã‰ª•ÁÇ∫ÊáâË©≤‰∏çÈåØÔºåÁµêÊûúÂ§ßÂ§±ÊâÄÊúõÔºÅÔºÅÈ†ÇÂ§öË∑ü100Â°äÁöÑÂ∞èÊóÖÈ§®Â∑Æ‰∏çÂ§öÔºÅÂêÑÁ®ÆË®≠ÊñΩÈÉΩËÄÅËàäÁöÑË¶ÅÂëΩÔºÅÈÄ£Ê∑ãÈõ®Ëä±ÁÅëÂ£ûÁöÑÔºÅÊúÄÂùëÁöÑÊòØÁ∂≤ÈÄüÈÇÑÈôêÂà∂ÊØèÂÄãÊàøÈñìÂπæÁôæK!!ÈñãÂÄãÁ∂≤È†ÅÈÉΩË¶Å‰∫îÂàÜÈêòÂ•ΩÂóéÔºüÁúüÁ†¥ÈÖíÂ∫óÔºÅ&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;ÊàëÂÄëÂéªÈπΩÂüéÁöÑÊôÇÂÄôÈÇ£Ë£°ÁöÑÊúÄ‰ΩéÊ∞£Ê∫´Âè™Êúâ4Â∫¶ÔºåÊôö‰∏äÂÜ∑ÂæóË¶ÅÊ≠ªÔºåÂ±ÖÁÑ∂ÈÇÑ‰∏çÈñãÁ©∫Ë™øÔºåÊäïË®¥Âà∞ÈÖíÂ∫óÂÆ¢ÊàøÈÉ®ÔºåÂæóÂà∞ÁöÑÁ≠îË¶ÜÊòØÁèæÂú®ÈÇÑÊ≤íÊúâÈ†òÂ∞éÊåáÁ§∫ÈúÄË¶ÅÈñãÊöñÊ∞£ÔºåÂ¶ÇÊûúÂÜ∑Âà∞Ë©±ÂèØ‰ª•Â§öÁµ¶‰∏ÄÂ∫äË¢´Â≠êÔºåÂ§™ÂèØÊÜê‰∫Ü„ÄÇ„ÄÇ„ÄÇ&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;ÊàøÈñìÂæàÂ∞èÔºåÊï¥È´îË®≠ÊñΩËÄÅÂåñÔºåÂíåÂõõÊòüÁöÑÂ∑ÆË∑ùÂæàÂ§ß„ÄÇÊØõÂ∑æÂ§™Á†¥Ëàä‰∫Ü„ÄÇÊó©È§êÂæàÁ∞°Èôã„ÄÇÊàøÈñìÈöîÈü≥ÂæàÂ∑ÆÔºåÈöîÂÖ©ÈñìÊàøÈñìÁöÑËÅ≤Èü≥ÈÉΩËÅΩÂæóË¶ã„ÄÇÊôö‰∏äÊúâ‰∫∫Âú®Ëµ∞ÂªäË£°Â§ßËÅ≤ÂñßË≠ÅÂæà‰πÖÔºå‰πüÊ≤íÊúâ‰∫∫‰æÜÂã∏Ê≠¢„ÄÇÊØî‰∏ç‰∏ä‰ª•ÂâçÂÖ•‰ΩèÁöÑÈôÑËøëÁöÑÁ∂ìÊøüÂûãÈÖíÂ∫ó„ÄÇ‰∏ãÊ¨°‰∏çÊúÉÂÖ•‰Ωè‰∫Ü„ÄÇ&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;ÊàëÊÑüË¶∫‰∏çË°å„ÄÇ„ÄÇ„ÄÇÂÉπÊïàÊØîÂæàÂ∑Æ„ÄÇ‰∏çÁü•ÈÅìÊòØÈäÄÂ∑ùÈÉΩÈÄôÊ®£ÈÇÑÊòØÊÄéÈ∫ºÁöÑÔºÅ&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;ÊàøÈñìÊôÇÈñìÈï∑ÔºåÈÄ≤ÂéªÊúâÈªûÁï∞Âë≥ÔºÅÊúçÂãôÂì°ÊòØ‰∏çÊòØ‰∏çÂ§†Áî®ÂïäÔºÅÊàëÂú®‰∏ÄÊ®ìÊâæ‰∫ÜÂçäÂÄãÂ∞èÊôÇ‰ª•‰∏äÊâçÊâæÂà∞Ëá™Â∑±ÊàøÈñìÔºåÊÉ≥ÊâæÂÄãÊúçÂãôÂì°Âïè‰∏Ä‰∏ãÈÉΩÊâæ‰∏çÂà∞ÔºåÁ∏Ω‰πã‰∏çÊé®Ëñ¶ÔºÅ&#39;, &#39;ÈÖíÂ∫ó&#39;), (&#39;ËÄÅ‰∫∫Â∞èÂ≠©‰∏ÄÂ§ßÂÆ∂ÊóèËÅöÊúÉÔºåÈÅ∏Âú®Âê≥ÂÆÆÊ≥õÂ§™Âπ≥Ê¥ãÔºå‰ª•ÁÇ∫Êñ∞Âä†Âù°ÂìÅÁâå‰∏ÄÂÆöÂæà‰∏çÈåØÔºåÊ≤íÊÉ≥Âà∞11Èªû30ÂàÜÂà∞ÂâçËá∫ÔºåÂ§ßÂ†ÇË£°Êì†Êªø‰∫Ü‰∫∫ÔºåÊàëÂÄëÊâÄÊúâ‰∫∫ÈÉΩË®óËëóÂ§ßÂ§ßÁöÑË°åÊùéÁÆ±Á´ôËëóÔºåÈÇÑÊúâÁöÑÊä±ËëóÂ∞èÂ¨∞ÂÖíÔºåÊéíÈöäËæ¶ÁêÜÂÖ•‰ΩèÔºåË∂≥Ë∂≥Á≠â‰∫Ü1ÂÄãÂ§öÂ∞èÊôÇÔºåÊúüÈñìÈÇÑË¢´ÂâçËá∫ÂºµÊÇÖÔºàËá™Á®±ÊòØÁï∂Â§©ÁöÑÂÄºÁè≠Á∂ìÁêÜÔºåÂíåÂè¶‰∏Ä‰ΩçËá™Á®±ÂÄºÁè≠Á∂ìÁêÜÊúâÈáçË§áÔºå‰∏çÁü•Ë™∞ÊòØË™∞ÈùûÔºâÂÜ∑Âò≤ÁÜ±Ë´∑ÔºåÁ∏Ω‰πãÁ≠âÂæÖÁöÑÂÆ¢‰∫∫ÈÉΩÂæà‰∏çÈ´òËààÔºåÂâçËá∫ÈÄüÂ∫¶Â§™ÊÖ¢Ôºå‰∏ÄÊúÉÂÖíÈÄ≤Ë£°Èù¢ÂïèÂïèÔºå‰∏ÄÊúÉÂÖíÂéªÈÇ£Ë£°ÂïèÂïèÔºåÂæàÊ∑∑‰∫ÇÁöÑÊÑüË¶∫ÔºÅÊàëÂÄëÁöÑÊàøÈñìÁ≠âÂà∞4ÈªûÈÇÑÊ≤íÊãøÂà∞ÊàøÔºÅ ÊàøÈñìË£°ÁöÑÂ∫äÂìÅÊúâÁôºÈª¥ÊΩÆÊ∫ºÁöÑÂë≥ÈÅìÔºåÁµïÂ∞ç‰∏çÂÉè5ÊòüÁ¥öÔºåÂÆ∂‰∫∫ÂÄëÈÉΩË™™‰∏çÂ¶Ç3ÊòüÁöÑÔºÅ ÊéíÈöäÂêÉÊó©È£ØÊéí‰∫ÜÂçäÂ∞èÊôÇÔºå‰πüÁÆóÊòØÈÜâ‰∫ÜÔºåËÄå‰∏îÊ≤í‰ªÄÈ∫ºÊù±Ë•øÂêÉÔºåÂïèÊúâÊ≤íÊúâÂ∞èÈ§õÈ£©ÔºåÁÖéËç∑ÂåÖËõãÁöÑÂªöÂ∏´Ë™™ÊúâÔºå‰ΩÜË¶ÅÈªûÂñÆ45ÂÖÉ‰∏ÄÁ¢óÔºåÊ≤íËÅΩË™™ÈÅéËá™Âä©È§êÂª≥Ë£°ÊúâÊî∂Ë≤ªÁöÑÂ∞àÊ°àÔºåË¶ÅÈ∫ºË™™Ê≤íÊúâÔºåÂÜçË™™‰∏ÄÁ¢óÂ∞èÈ§õÈ£©Ë¶Å45ÂÖÉÔºüÈÇä‰∏äËÅΩËëóÁöÑÂÆ¢‰∫∫Áõ¥ÊêñÈ†≠ÔºÅ Ê≤íËæ¶Ê≥ïÊâøÊé•ÈÄôÈ∫ºÂ§öÂÆ¢ÊµÅÔºåÂ∞±‰∏çË¶ÅÊé•ÔºåÂÖ•‰ΩèÈ´îÈ©óÂæà‰∏çÂ•ΩÔºÅÂã∏‰∏äÊµ∑ÁöÑÊúãÂèã‰∏çË¶ÅË¢´ÂÆ£ÂÇ≥ÂúñÁâáÂíåÊñáÂ≠óÂøΩÊÇ†‰∫ÜÔºåËµ∑Á¢ºÊó∫Â≠£ÁúüÂøÉ‰∏çÂíãÁöÑÔºåÂá∫ÂÖ•‰∏çÊñπ‰æøÔºåÈõªÊ¢Ø‰∏çÊòØÁõ¥ÈÅîÁöÑÔºåÂøÖÈ†àÁ∂ìÈÅéÂâçËá∫ÊâçËÉΩÂà∞Â∫ïÔºåÁ∏Ω‰πãÈ´îÈ©óÈùûÂ∏∏Â∑ÆÔºåÊØèÊ¨°ÂéªËòáÂ∑ûÈÉΩ‰ΩèÂúíÂçÄÈÇ£Ë£°ÔºåËÄÅÂüéÂçÄÁ¨¨‰∏ÄÊ¨°È´îÈ©óÔºå‰∏çÂ•ΩÔºå‰∏çÊúÉÂÜçÂéªÔºÅ&#39;, &#39;ÈÖíÂ∫ó&#39;)] . In the make_doc function above, we hardcoded the string names of text classes, that is, POSITIVE and NEGATIVE. That was not a good option since the function cannot be reused in other cases. So I&#39;d like a general function that works just as well even if we don&#39;t know in advance how many text classes there are in the dataset and what their string names are. After doing some experiments, I finally came up with the make_docs_multiclass function, which does the job. The trick here is to create the label_dict dictionary with every unique class name as keys and False as their default values for every Doc object in the for-loop. Then we assign label_dict to the cats attribute of every Doc object, that is, doc.cats = label_dict. At last, we update the value of a class in label_dict to True only when that is the class of the Doc object in question. . unique_labels = df.cat.unique().tolist() def make_docs_multiclass(data): &quot;&quot;&quot; this will take a list of texts and labels and transform them in spacy documents texts: List(str) labels: List(labels) returns: List(spacy.Doc.doc) &quot;&quot;&quot; docs = [] # nlp.pipe([texts]) is way faster than running nlp(text) for each text # as_tuples allows us to pass in a tuple, the first one is treated as text # the second one will get returned as it is. for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)): label_dict = {label: False for label in unique_labels} # we need to set the (text)cat(egory) for each document doc.cats = label_dict doc.cats[label] = True # put them into a nice list docs.append(doc) return docs . . Then we split the dataset, convert it to sapCy format, save the converted data, and train a model with CLI, just like we did earlier. . from sklearn.model_selection import train_test_split train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1) . . valid_docs = make_docs_multiclass(valid_data) doc_bin = DocBin(docs=valid_docs) doc_bin.to_disk(&quot;./data/valid.spacy&quot;) . train_docs = make_docs_multiclass(train_data) doc_bin = DocBin(docs=train_docs) doc_bin.to_disk(&quot;./data/train.spacy&quot;) . source = &quot;/content/data&quot; dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-product-cat-model/&quot; !cp -R {source} {dest} . After training, we got an overall score of 89, which is pretty awesome. . !python -m spacy train ./config.cfg --output ./output-multiclass . 2021-03-05 03:12:00.200786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 ‚úî Created output directory: output-multiclass ‚Ñπ Using CPU =========================== Initializing pipeline =========================== Set up nlp object from config Pipeline: [&#39;tok2vec&#39;, &#39;textcat&#39;] Created vocabulary Finished initializing nlp object Initialized pipeline components: [&#39;tok2vec&#39;, &#39;textcat&#39;] ‚úî Initialized pipeline ============================= Training pipeline ============================= ‚Ñπ Pipeline: [&#39;tok2vec&#39;, &#39;textcat&#39;] ‚Ñπ Initial learn rate: 0.001 E # LOSS TOK2VEC LOSS TEXTCAT CATS_SCORE SCORE - 0 0 0.00 0.09 0.00 0.00 0 200 0.00 45.82 37.83 0.38 0 400 0.00 14.59 63.48 0.63 0 600 0.00 12.88 71.83 0.72 0 800 0.00 12.18 76.29 0.76 0 1000 0.00 5.34 78.87 0.79 0 1200 0.00 3.11 80.73 0.81 0 1400 0.00 4.12 82.13 0.82 0 1600 0.00 1.41 82.99 0.83 0 1800 0.00 0.76 83.86 0.84 0 2000 0.00 0.58 84.57 0.85 0 2200 0.00 0.40 85.13 0.85 0 2400 0.00 0.22 85.54 0.86 0 2600 0.00 0.24 86.10 0.86 0 2800 0.00 0.17 86.51 0.87 0 3000 0.00 0.30 86.82 0.87 0 3200 0.00 0.20 87.06 0.87 0 3400 0.00 0.26 87.13 0.87 0 3600 0.00 0.14 87.48 0.87 1 3800 0.00 0.13 87.73 0.88 1 4000 0.00 0.13 87.80 0.88 1 4200 0.00 0.31 87.80 0.88 1 4400 0.00 0.12 88.07 0.88 1 4600 0.00 0.25 88.07 0.88 1 4800 0.00 0.26 87.97 0.88 1 5000 0.00 0.10 88.25 0.88 1 5200 0.00 0.20 88.27 0.88 1 5400 0.00 0.15 88.39 0.88 1 5600 0.00 0.13 88.51 0.89 1 5800 0.00 0.11 88.53 0.89 2 6000 0.00 0.16 88.77 0.89 2 6200 0.00 0.15 88.39 0.88 2 6400 0.00 0.15 88.67 0.89 2 6600 0.00 0.11 88.77 0.89 2 6800 0.00 0.11 88.65 0.89 2 7000 0.00 0.14 88.73 0.89 2 7200 0.00 0.11 88.69 0.89 2 7400 0.00 0.17 88.67 0.89 2 7600 0.00 0.17 88.58 0.89 2 7800 0.00 0.11 88.68 0.89 2 8000 0.00 0.28 88.96 0.89 3 8200 0.00 0.21 88.93 0.89 3 8400 0.00 0.11 88.91 0.89 3 8600 0.00 0.23 88.86 0.89 3 8800 0.00 0.15 88.80 0.89 3 9000 0.00 0.11 88.95 0.89 3 9200 0.00 0.30 88.90 0.89 3 9400 0.00 0.11 89.11 0.89 3 9600 0.00 0.12 89.14 0.89 3 9800 0.00 0.12 89.09 0.89 3 10000 0.00 0.16 88.95 0.89 3 10200 0.00 0.17 89.01 0.89 4 10400 0.00 0.10 89.07 0.89 4 10600 0.00 0.12 89.14 0.89 4 10800 0.00 0.12 89.15 0.89 4 11000 0.00 0.11 89.09 0.89 4 11200 0.00 0.13 89.03 0.89 4 11400 0.00 0.09 89.11 0.89 4 11600 0.00 0.11 89.28 0.89 4 11800 0.00 0.18 89.15 0.89 4 12000 0.00 0.14 89.33 0.89 4 12200 0.00 0.09 89.22 0.89 4 12400 0.00 0.15 89.22 0.89 5 12600 0.00 0.10 89.08 0.89 5 12800 0.00 0.11 89.17 0.89 5 13000 0.00 0.15 89.24 0.89 5 13200 0.00 0.12 89.20 0.89 5 13400 0.00 0.12 89.08 0.89 5 13600 0.00 0.15 89.18 0.89 ‚úî Saved pipeline to output directory output-multiclass/model-last . . Here&#39;s the py file for testing the multiclass classification model. . %%writefile test_input_multiclass.py import spacy # load the best model from training nlp = spacy.load(&quot;output-multiclass/model-best&quot;) text = &quot;&quot; print(&quot;type : &#39;quit&#39; to exit&quot;) # predict the product category until someone writes quit while text != &quot;quit&quot;: text = input(&quot;Please enter a review here: &quot;) doc = nlp(text) print(doc.cats) . . Writing test_input_multiclass.py . Again, let&#39;s print out 10 reviews in valid_data for the purpose of testing. . %%writefile test_input_multiclass.py import spacy # load the best model from training nlp = spacy.load(&quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-text-classification/spaCy-product-cat-model/model-best&quot;) text = &quot;&quot; print(&quot;type : &#39;quit&#39; to exit&quot;) # predict the product category until someone writes quit while text != &quot;quit&quot;: text = input(&quot;Please enter a review here: &quot;) doc = nlp(text) print(doc.cats) . . Writing test_input_multiclass.py . valid_data[:10] . [(&#39;Âíå‰ª•ÂâçË≤∑ÁöÑ‰∏ç‰∏ÄÊ®£ÔºÅ‰ª•ÂâçÁî®ÁöÑÁâπÂà•ÊªëÔºÅÁî®ÁöÑ‰πüÂ∞ëÔºÅÈÄôÊ¨°Ë≤∑ÁöÑ‰∏çÂÉÖÁî®ÁöÑÊù±Ë•øÂ§öÔºÅËÄå‰∏îÈÇÑÈ†≠ÁöÆÁô¢ÔºÅÊúâÈ†≠Â±ëÔºÅ&#39;, &#39;Ê¥óÈ´ÆÊ∞¥&#39;), (&#39;‰∏çÂ•ΩÊÑèÊÄùÊàëË≤®Êú™Êî∂Âà∞ÔºåÊàëÊÉ≥Ëµ∑Ë®¥‰Ω†ÂÄëÔºÅ&#39;, &#39;Ë°£Êúç&#39;), (&#39;ÂØ∂Ë≤ùÊî∂Âà∞‰∫ÜÔºåÊòØÊàëÊÉ≥Ë¶ÅÁöÑÁâàÂûãÔºåË≥™ÈáèÂçÅÂàÜÂ•ΩÔºåÈÄôÊ¢ùË§≤Â≠êÁöÑÈ°èËâ≤Ë∑üÂúñÁâá‰∏ä‰∏ÄÊ®£ÔºåÊòØÊàëÊÉ≥Ë¶ÅÁöÑÈ°èËâ≤ÔºåÁ∏Ω‰πãÔºåÊòØ‰∏ÄÊ¨°ÂæàÊÑâÂø´ÁöÑË≥ºÁâ©&#39;, &#39;Ë°£Êúç&#39;), (&#39;‰∏ÄÈªû‰πü‰∏çÁîú‰∏çÊ∏ÖËÑÜÔºåÂè™ËÉΩÊ¶®Ê±Å‰∫Ü„ÄÇ&#39;, &#39;Ê∞¥Êûú&#39;), (&#39;ËòãÊûúÂ∞èÔºå‰ΩÜÂë≥ÈÅì‰∏çÈåØÔºåÁµ¶ÂÄãÂ•ΩË©ïÂêß&#39;, &#39;Ê∞¥Êûú&#39;), (&#39;Âπ≥ÊùøÊî∂Âà∞ÔºåÂæàÊªøÊÑèÂíåÊÉ≥ÂÉèÁöÑ‰∏ÄÊ®£ÔºåÊ≤íÂ§±ÊúõÁ¨¨‰∏ÄÊôÇÈñìÈ´îÈ©ó‰∏≠ÔºåÁ®çÂæåÂÜçË©ï&#39;, &#39;Âπ≥Êùø&#39;), (&#39;Ë™™Â•ΩÁöÑÊ®ÇË¶ñÊúÉÂì°‰∏ÄÂπ¥ ÈÄÅÂì™Âéª‰∫Ü Â∑ÆË©ï&#39;, &#39;Âπ≥Êùø&#39;), (&#39;Áõ∏Áï∂ÂÆåÁæéÂïä Â•ΩÊù±Ë•ø&#39;, &#39;Âπ≥Êùø&#39;), (&#39;ÈÄ£ÁîüÁî¢ÁîüÊó•ÈÉΩÊ≤íÊúâÔºåÁúü‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÁúüÁöÑ„ÄÇ&#39;, &#39;Ê¥óÈ´ÆÊ∞¥&#39;), (&#39;ËòãÊûú‰∏ÄÂ¶ÇÊó¢ÂæÄÂæóÂ•ΩÔºåËÑÜ„ÄÅÁîú„ÄÅÊ∞¥ÂàÜË∂≥ÔºåÊé®Ëñ¶Ë≥ºË≤∑Âì¶Â•ßÔΩû&#39;, &#39;Ê∞¥Êûú&#39;)] . The model works as expected. I intentionally tested the ambiguous review, ÈÄôÂÄãÁâåÂ≠êÂÄºÂæó‰ø°Ë≥¥ (This brand is trustworthy.), which could have been a review for tablets, clothing, or shampoo. And our model gave top three scores to precisely these three classes! . !python test_input_multiclass.py . . 2021-03-05 05:07:28.215017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 type : &#39;quit&#39; to exit Please enter a review here: Âíå‰ª•ÂâçË≤∑ÁöÑ‰∏ç‰∏ÄÊ®£ÔºÅ‰ª•ÂâçÁî®ÁöÑÁâπÂà•ÊªëÔºÅÁî®ÁöÑ‰πüÂ∞ëÔºÅÈÄôÊ¨°Ë≤∑ÁöÑ‰∏çÂÉÖÁî®ÁöÑÊù±Ë•øÂ§öÔºÅËÄå‰∏îÈÇÑÈ†≠ÁöÆÁô¢ÔºÅÊúâÈ†≠Â±ëÔºÅ {&#39;Âπ≥Êùø&#39;: 2.058545214822516e-05, &#39;Ê∞¥Êûú&#39;: 1.1623805221461225e-05, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.9999675750732422, &#39;Ë°£Êúç&#39;: 2.6233973926537146e-07, &#39;ÈÖíÂ∫ó&#39;: 7.811570945648327e-09} Please enter a review here: ËòãÊûúÂ∞èÔºå‰ΩÜÂë≥ÈÅì‰∏çÈåØÔºåÁµ¶ÂÄãÂ•ΩË©ïÂêß {&#39;Âπ≥Êùø&#39;: 0.0010689852060750127, &#39;Ê∞¥Êûú&#39;: 0.9964427351951599, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.00197225296869874, &#39;Ë°£Êúç&#39;: 0.00019674153008963913, &#39;ÈÖíÂ∫ó&#39;: 0.0003193040902260691} Please enter a review here: ËÑÜ„ÄÅÁîú„ÄÅÊ∞¥ÂàÜË∂≥ÔºåÊé®Ëñ¶Ë≥ºË≤∑Âì¶Â•ßÔΩû {&#39;Âπ≥Êùø&#39;: 0.00028378094430081546, &#39;Ê∞¥Êûú&#39;: 0.9972990155220032, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.0011949185281991959, &#39;Ë°£Êúç&#39;: 0.0006593622965738177, &#39;ÈÖíÂ∫ó&#39;: 0.0005628817598335445} Please enter a review here: Êî∂Âà∞ÔºåÂæàÊªøÊÑèÂíåÊÉ≥ÂÉèÁöÑ‰∏ÄÊ®£ÔºåÊ≤íÂ§±ÊúõÁ¨¨‰∏ÄÊôÇÈñìÈ´îÈ©ó‰∏≠ÔºåÁ®çÂæåÂÜçË©ï {&#39;Âπ≥Êùø&#39;: 0.5766726732254028, &#39;Ê∞¥Êûú&#39;: 0.07788817584514618, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.16239948570728302, &#39;Ë°£Êúç&#39;: 0.16160377860069275, &#39;ÈÖíÂ∫ó&#39;: 0.0214359350502491} Please enter a review here: ÈÄ£ÁîüÁî¢ÁîüÊó•ÈÉΩÊ≤íÊúâÔºåÁúü‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÁúüÁöÑ„ÄÇ {&#39;Âπ≥Êùø&#39;: 0.23529699444770813, &#39;Ê∞¥Êûú&#39;: 0.032520271837711334, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.6238041520118713, &#39;Ë°£Êúç&#39;: 0.017157811671495438, &#39;ÈÖíÂ∫ó&#39;: 0.09122073650360107} Please enter a review here: ÈÄôÂÄãÁâåÂ≠êÂÄºÂæó‰ø°Ë≥¥Ôºå {&#39;Âπ≥Êùø&#39;: 0.19572772085666656, &#39;Ê∞¥Êûú&#39;: 0.10571669787168503, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.362021267414093, &#39;Ë°£Êúç&#39;: 0.29673027992248535, &#39;ÈÖíÂ∫ó&#39;: 0.03980398178100586} Please enter a review here: quit {&#39;Âπ≥Êùø&#39;: 0.25125113129615784, &#39;Ê∞¥Êûú&#39;: 0.061232421547174454, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.27915307879447937, &#39;Ë°£Êúç&#39;: 0.12709283828735352, &#39;ÈÖíÂ∫ó&#39;: 0.2812705338001251} . !python test_input_multiclass.py . 2021-03-16 10:14:45.823434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 type : &#39;quit&#39; to exit Please enter a review here: ÊòØËòãÊûúÔºåÂ§ßÂÆ∂ÈÉΩÊÉ≥Ë¶Å {&#39;Âπ≥Êùø&#39;: 0.07115291804075241, &#39;Ê∞¥Êûú&#39;: 0.872447669506073, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.011495854705572128, &#39;Ë°£Êúç&#39;: 0.027250811457633972, &#39;ÈÖíÂ∫ó&#39;: 0.01765277236700058} Please enter a review here: ÊòØËòãÊûúÁöÑÔºåÂ§ßÂÆ∂ÈÉΩÊÉ≥Ë¶Å {&#39;Âπ≥Êùø&#39;: 0.05921125411987305, &#39;Ê∞¥Êûú&#39;: 0.8904093503952026, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.01062779687345028, &#39;Ë°£Êúç&#39;: 0.025291932746767998, &#39;ÈÖíÂ∫ó&#39;: 0.014459758996963501} Please enter a review here: ÊòØËòãÊûúÁöÑÔºåÂ§ßÂÆ∂ÈÉΩÊÉ≥Ë¶Å {&#39;Âπ≥Êùø&#39;: 0.05921125411987305, &#39;Ê∞¥Êûú&#39;: 0.8904093503952026, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.01062779687345028, &#39;Ë°£Êúç&#39;: 0.025291932746767998, &#39;ÈÖíÂ∫ó&#39;: 0.014459758996963501} Please enter a review here: ÊòØ {&#39;Âπ≥Êùø&#39;: 0.23912471532821655, &#39;Ê∞¥Êûú&#39;: 0.11478321999311447, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.206477552652359, &#39;Ë°£Êúç&#39;: 0.26159045100212097, &#39;ÈÖíÂ∫ó&#39;: 0.17802411317825317} Please enter a review here: ÊòØËòãÊûúÁöÑÁî¢ÂìÅÔºåÂ§ßÂÆ∂ÈÉΩÊÉ≥Ë¶Å„ÄÇ {&#39;Âπ≥Êùø&#39;: 0.09895286709070206, &#39;Ê∞¥Êûú&#39;: 0.8691270351409912, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.016442863270640373, &#39;Ë°£Êúç&#39;: 0.012551860883831978, &#39;ÈÖíÂ∫ó&#39;: 0.002925291657447815} Please enter a review here: ËòãÊûúÁöÑÁî¢ÂìÅÔºåÂ§ßÂÆ∂ÈÉΩÊÉ≥Ë¶Å„ÄÇ {&#39;Âπ≥Êùø&#39;: 0.1049538403749466, &#39;Ê∞¥Êûú&#39;: 0.865241289138794, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.014476561918854713, &#39;Ë°£Êúç&#39;: 0.01276717334985733, &#39;ÈÖíÂ∫ó&#39;: 0.0025611238088458776} Please enter a review here: ËòãÊûúÁöÑÊâãÊ©üÂ§ßÂÆ∂ÈÉΩÊÉ≥Ë¶Å„ÄÇ {&#39;Âπ≥Êùø&#39;: 0.7589152455329895, &#39;Ê∞¥Êûú&#39;: 0.208853617310524, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.0034141496289521456, &#39;Ë°£Êúç&#39;: 0.013454959727823734, &#39;ÈÖíÂ∫ó&#39;: 0.015362164005637169} Please enter a review here: quit {&#39;Âπ≥Êùø&#39;: 0.25125113129615784, &#39;Ê∞¥Êûú&#39;: 0.061232421547174454, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: 0.27915307879447937, &#39;Ë°£Êúç&#39;: 0.12709283828735352, &#39;ÈÖíÂ∫ó&#39;: 0.2812705338001251} . Now I can rest assured and save the trained model to Google Drive. . source = &quot;/content/output-multiclass/model-best&quot; dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-product-cat-model/&quot; !cp -R {source} {dest} . Checking model performance . I thought I&#39;d have to write custom functions to evaluate the performance of our classification models. But it turns out that spaCy, our unfailingly considerate friend, has done it for us under the hood. Performance metrics are hidden in the meta.json file under the model-best directory. Here&#39;s the content of the meta.json file for our multiclass classification model. . import json meta_path = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-text-classification/spaCy-product-cat-model/model-best/meta.json&quot; with open(meta_path) as json_file: metrics = json.load(json_file) metrics . . {&#39;author&#39;: &#39;&#39;, &#39;components&#39;: [&#39;tok2vec&#39;, &#39;textcat&#39;], &#39;description&#39;: &#39;&#39;, &#39;disabled&#39;: [], &#39;email&#39;: &#39;&#39;, &#39;labels&#39;: {&#39;textcat&#39;: [&#39;Âπ≥Êùø&#39;, &#39;Ê∞¥Êûú&#39;, &#39;Ê¥óÈ´ÆÊ∞¥&#39;, &#39;Ë°£Êúç&#39;, &#39;ÈÖíÂ∫ó&#39;], &#39;tok2vec&#39;: []}, &#39;lang&#39;: &#39;zh&#39;, &#39;license&#39;: &#39;&#39;, &#39;name&#39;: &#39;pipeline&#39;, &#39;performance&#39;: {&#39;cats_f_per_type&#39;: {&#39;Âπ≥Êùø&#39;: {&#39;f&#39;: 0.8329853862, &#39;p&#39;: 0.9140893471, &#39;r&#39;: 0.7651006711}, &#39;Ê∞¥Êûú&#39;: {&#39;f&#39;: 0.9107981221, &#39;p&#39;: 0.9525368249, &#39;r&#39;: 0.8725637181}, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: {&#39;f&#39;: 0.8430637386, &#39;p&#39;: 0.8827818284, &#39;r&#39;: 0.8067657611}, &#39;Ë°£Êúç&#39;: {&#39;f&#39;: 0.8985658409, &#39;p&#39;: 0.9303455724, &#39;r&#39;: 0.868885527}, &#39;ÈÖíÂ∫ó&#39;: {&#39;f&#39;: 0.9810741688, &#39;p&#39;: 0.9932677369, &#39;r&#39;: 0.9691763517}}, &#39;cats_macro_auc&#39;: 0.9846299582, &#39;cats_macro_auc_per_type&#39;: 0.0, &#39;cats_macro_f&#39;: 0.8932974513, &#39;cats_macro_p&#39;: 0.9346042619, &#39;cats_macro_r&#39;: 0.8564984058, &#39;cats_micro_f&#39;: 0.8939148603, &#39;cats_micro_p&#39;: 0.9357025697, &#39;cats_micro_r&#39;: 0.8557, &#39;cats_score&#39;: 0.8932974513, &#39;cats_score_desc&#39;: &#39;macro F&#39;, &#39;textcat_loss&#39;: 0.1365536493, &#39;tok2vec_loss&#39;: 0.0}, &#39;pipeline&#39;: [&#39;tok2vec&#39;, &#39;textcat&#39;], &#39;spacy_git_version&#39;: &#39;f4f46b617&#39;, &#39;spacy_version&#39;: &#39;&gt;=3.0.3,&lt;3.1.0&#39;, &#39;url&#39;: &#39;&#39;, &#39;vectors&#39;: {&#39;keys&#39;: 0, &#39;name&#39;: None, &#39;vectors&#39;: 0, &#39;width&#39;: 0}, &#39;version&#39;: &#39;0.0.0&#39;} . Specifically, values of the performance key are what we&#39;re looking for. . performance = metrics[&#39;performance&#39;] performance . . {&#39;cats_f_per_type&#39;: {&#39;Âπ≥Êùø&#39;: {&#39;f&#39;: 0.8329853862, &#39;p&#39;: 0.9140893471, &#39;r&#39;: 0.7651006711}, &#39;Ê∞¥Êûú&#39;: {&#39;f&#39;: 0.9107981221, &#39;p&#39;: 0.9525368249, &#39;r&#39;: 0.8725637181}, &#39;Ê¥óÈ´ÆÊ∞¥&#39;: {&#39;f&#39;: 0.8430637386, &#39;p&#39;: 0.8827818284, &#39;r&#39;: 0.8067657611}, &#39;Ë°£Êúç&#39;: {&#39;f&#39;: 0.8985658409, &#39;p&#39;: 0.9303455724, &#39;r&#39;: 0.868885527}, &#39;ÈÖíÂ∫ó&#39;: {&#39;f&#39;: 0.9810741688, &#39;p&#39;: 0.9932677369, &#39;r&#39;: 0.9691763517}}, &#39;cats_macro_auc&#39;: 0.9846299582, &#39;cats_macro_auc_per_type&#39;: 0.0, &#39;cats_macro_f&#39;: 0.8932974513, &#39;cats_macro_p&#39;: 0.9346042619, &#39;cats_macro_r&#39;: 0.8564984058, &#39;cats_micro_f&#39;: 0.8939148603, &#39;cats_micro_p&#39;: 0.9357025697, &#39;cats_micro_r&#39;: 0.8557, &#39;cats_score&#39;: 0.8932974513, &#39;cats_score_desc&#39;: &#39;macro F&#39;, &#39;textcat_loss&#39;: 0.1365536493, &#39;tok2vec_loss&#39;: 0.0} . Let&#39;s make a nice DataFrame object out of the metrics of the overall performance. . score = performance[&#39;cats_score&#39;] auc = performance[&#39;cats_macro_auc&#39;] f1 = performance[&#39;cats_macro_f&#39;] precision = performance[&#39;cats_macro_p&#39;] recall = performance[&#39;cats_macro_r&#39;] overall_dict = {&#39;score&#39;: score, &#39;precision&#39;: precision, &#39;recall&#39;: recall, &#39;F1&#39;: f1, &#39;AUC&#39;: auc} overall_df = pd.DataFrame(overall_dict, index=[0]) overall_df . . score precision recall F1 AUC . 0 0.893297 | 0.934604 | 0.856498 | 0.893297 | 0.98463 | . We can also break down the metrics into specific categories, which are saved as the values of the cats_f_per_type key of performance. . per_cat_dict = performance[&#39;cats_f_per_type&#39;] per_cat_df = pd.DataFrame(per_cat_dict) per_cat_df . . Âπ≥Êùø Ê∞¥Êûú Ê¥óÈ´ÆÊ∞¥ Ë°£Êúç ÈÖíÂ∫ó . p 0.914089 | 0.952537 | 0.882782 | 0.930346 | 0.993268 | . r 0.765101 | 0.872564 | 0.806766 | 0.868886 | 0.969176 | . f 0.832985 | 0.910798 | 0.843064 | 0.898566 | 0.981074 | . Previously, I also used the fasttext library to train a multiclass classification model on the same dataset. Here&#39;re the values of the parameters I set up. And the overall accuracy is 0.886, which is pretty close to our spaCy model. . {&#39;dim&#39;: 200, &#39;epoch&#39;: 5, &#39;loss&#39;: &#39;softmax&#39;, &#39;lr&#39;: 0.1, &#39;test_size&#39;: 0.2, &#39;window&#39;: 5, &#39;wordNgrams&#39;: 1} . And here&#39;s the breakdown for each category. One noticeable difference between the spaCy and fastText model is that the spaCy model has the highest scores in the HOTEL category (ÈÖíÂ∫ó) across all three metrics whereas the fastText model consistently performs the best in the TABLET category (Âπ≥Êùø) in terms of all three metrics. . . cat Âπ≥Êùø Ê∞¥Êûú Ê¥óÈ´ÆÊ∞¥ Ë°£Êúç ÈÖíÂ∫ó . p 0.976 | 0.892 | 0.851 | 0.899 | 0.816 | . r 0.972 | 0.908 | 0.832 | 0.885 | 0.836 | . f 0.974 | 0.9 | 0.841 | 0.892 | 0.825 | . Recap . Like fastText, spaCy is good for training text classification models, and this task has become a no-brainer with the release of spaCy v3.0. Even in cases where both methods work equally well, spaCy still has an edge over fastText. That is, with spaCy you don&#39;t need to deal with text preprocessing or tokenization. So, watch this space-y! .",
            "url": "https://howard-haowen.github.io/blog.ai/spacy/text-classification/sentiment-analysis/customer-reviews/fasttext/facets/2021/03/12/Classifying-customer-reviews-with-spaCy-v3.html",
            "relUrl": "/spacy/text-classification/sentiment-analysis/customer-reviews/fasttext/facets/2021/03/12/Classifying-customer-reviews-with-spaCy-v3.html",
            "date": " ‚Ä¢ Mar 12, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Adding a custom tokenizer to spaCy and extracting keywords",
            "content": ". Intro . spaCy is an industrial-strength natural language processing library in Python, and supports multiple human languages, including Chinese. For segmenting Chinese texts into words, spaCy uses Jieba or PKUSeg under the hood. However, neither of them beats CKIP Transformers in accuracy when it comes to traditional Chinese (see my previous post for a comparison). So I&#39;ll show how to plug in CKIP Transformers to spaCy to get the best out of both. . For the purpose of demonstration, I&#39;ll situate this integration in a pipeline for extracting keywords from texts. Compared with other NLP tasks, keyword extraction is a relatively easy job. TextRank and RAKE seem to be among the most widely adopted algorithms for keyword extraction. I tried most of the methods mentioned in this article, but there doesn&#39;t seem to be any easy-peasy implementation of TextRank or RAKE that produces decent results for traditional Chinese texts. So the first part of this post walks through a pipeline that actually works, and the second part records other methods that failed. I included the second part because I believe in this quote: . ‚ÄúWe learn wisdom from failure much more than from success. We often discover what will do, by finding out what will not do; and probably he who never made a mistake never made a discovery.‚Äù ‚Äï Samuel Smiles . . Note: TextRank is based on Google&#8217;s PageRank, which is used to compute the rank of webpages. This article on Natural Language Processing for Hackers demonstrates the connection between the two. From it I learned a tidbit: I always assumed that Page as in PageRank refers to webpages, but it turns out to be the family name of Larry Page, the creator of PageRank. . Working pipeline . Set variables . Let&#39;s start with defining two variables that users of our keyword extraction program might want to modify: CUSTOM_STOPWORDS for a list of words that users definitely hope to exclude from keyword candidates and KW_NUM for the number of keywords that they&#39;d like to extract from a document. . CUSTOM_STOPWORDS = [ &quot;Ê∞ëÁúæ&quot;,&quot;ÊúãÂèã&quot;,&quot;Â∏ÇÊ∞ë&quot;,&quot;‰∫∫Êï∏&quot;, &quot;ÂÖ®Ê∞ë&quot;,&quot;‰∫∫Âì°&quot;,&quot;‰∫∫Â£´&quot;,&quot;ÈáåÊ∞ë&quot;, &quot;ÂΩ±Êú¨&quot;,&quot;Á≥ªÁµ±&quot;, &quot;È†ÖÁõÆ&quot;, &quot;Ë≠â‰ª∂&quot;, &quot;Ë≥áÊ†º&quot;,&quot;ÂÖ¨Ê∞ë&quot;, &quot;Â∞çË±°&quot;,&quot;ÂÄã‰∫∫&quot;, ] KW_NUM = 10 . Preprocess texts . I took an announcement from Land Administration Bureau of Kaohsiung City Goverment as a sample text, but you can basically take any text in traditional Chinese to test the program. . . Tip: To run the program with your own text, follow the following steps: . Click on Open in Colab at the upper right corner of this page. | Click on File and then Save a copy in Drive. | Replace the following text with your own text. | Click on Runtime and then Run all. | Go to the section Put it together to see the outcome. | raw_text = &#39;&#39;&#39; Â∏ÇÂ∫úÂú∞ÊîøÂ±Ä109Âπ¥Â∫¶Á¨¨4Â≠£ÈñãÁôºÂçÄÂúüÂú∞Ê®ôÂîÆÔºåÂÖ±Ë®àÊé®Âá∫8Ê®ô9Á≠ÜÂÑ™Ë≥™Âª∫Âú∞ÔºåË®ÇÊñº109Âπ¥12Êúà16Êó•ÈñãÊ®ôÔºåÂêàË®àÁ∏ΩÂ∫ïÂÉπ12 ÂÑÑ4049Ëê¨6164 ÂÖÉ„ÄÇ Á¨¨93ÊúüÈáçÂäÉÂçÄÔºåÂéüÁÇ∫ÂúãËªçÁú∑ÊùëÔºåÁ∑äÈÑ∞ÂúãÂÆöÂè§Ëπü-„ÄåÂéüÊó•Êú¨Êµ∑ËªçÈ≥≥Â±±ÁÑ°Á∑öÈõª‰ø°ÊâÄ„ÄçÔºåÂ∏ÇÂ∫úÁÇ∫‰øùÂ≠òÂè§ËπüÂêåÊôÇÊ¥ªÂåñÁú∑ÊùëÈÅ∑ÁßªÂæåÂúüÂú∞Ôºå‰ª•ÈáçÂäÉÊñπÂºèÊï¥È´îÈñãÁôºÔºåÊñ∞Èó¢‰ΩèÂÆÖÂçÄ„ÄÅÈÅìË∑Ø„ÄÅÂÖ¨ÂúíÂèäÂÅúËªäÂ†¥Ôºå‰ΩøÊú¨ÂçÄÂÖ∑ÊúâÊ≠∑Âè≤ÊñáÂåñÂÖßÊ∂µËàáÁ∂†Ëâ≤‰ºëÈñíÁâπËâ≤ÔºåÁîüÊ¥ªÊ©üËÉΩÊõ¥Âä†ÂÅ•ÂÖ®„ÄÇÂú∞ÊîøÂ±ÄÈ¶ñÊ¨°Êé®Âá∫1Á≠ÜÂ§ßÈù¢Á©çÂúüÂú∞ÔºåÈù¢Á©çÁ¥Ñ2160Âù™ÔºåÂú∞ÂΩ¢ÊñπÊï¥ÔºåÈõôÈù¢Ëá®Ë∑ØÔºåÂà©ÊñºË¶èÂäÉËààÂª∫ÊôØËßÄÂ§ßÊ®ìÔºåÈôÑËøëÊúâÂ∏ÇÂ†¥„ÄÅÂ≠∏Ê†°„ÄÅÂÖ¨ÂúíÂèäÂ§ßÊù±ÊñáÂåñÂúíÂçÄÔºåË∑ùÊç∑ÈÅãÂ§ßÊù±Á´ô„ÄÅÈ≥≥Â±±Âúã‰∏≠Á´ôÂèäÈ≥≥Â±±ÁÅ´ËªäÁ´ôÂÉÖÊï∏ÂàÜÈêòËªäÁ®ãÔºå‰∫§ÈÄöÂõõÈÄöÂÖ´ÈÅîÔºåÂõ†ÂúüÂú∞Á®ÄÂ∞ëÊÄßÂèäÂçÄ‰ΩçÊ¢ù‰ª∂Áµï‰Ω≥ÔºåÂã¢ÂøÖÊàêÁÇ∫ÊäïË≥á‰∫∫ËøΩÈÄêÁÑ¶Èªû„ÄÇ Á¨¨87ÊúüÈáçÂäÉÂçÄÔºå‰ΩçÊñºÁúÅÈÅìÂè∞1Á∑öÊóÅÔºåÈÑ∞ËøëÊç∑ÈÅãÂçóÂ≤°Â±±Á´ôÔºåÈáçÂäÉÂæåÊìÅÊúâÂÆåÂñÑÁöÑÈÅìË∑ØÁ≥ªÁµ±„ÄÅÂÖ¨ÂúíÁ∂†Âú∞ÂèäÊØóÈÑ∞ÈÜíÊùëÊá∑ËàäÊñáÂåñÊôØËßÄÂª∫ÁØâÁæ§ÔºåÂÖ∑ÂÇôÂÑ™Ë≥™Â±Ö‰ΩèÁí∞Â¢ÉÂèä‰∫§ÈÄö‰æøÊç∑Ë¶Å‰ª∂ÔºåÂú∞ÊîøÂ±Ä‰∏ÄÊé®Âá∫ÂúüÂú∞Ê®ôÂîÆÔºåÂç≥ÊéÄËµ∑Êê∂Ê®ôÁÜ±ÊΩÆÔºåÊú¨Â≠£ÂÜçÈáãÂá∫1Á≠ÜÈù¢Á©çÁ¥Ñ93Âù™ÂúüÂú∞ÔºåËá®20Á±≥‰ªãÂ£ΩË∑ØÂèäÈµ¨Á®ãÊù±Ë∑ØÔºåÈôÑËøëÊúâÂ≤°Â±±ÊñáÂåñ‰∏≠ÂøÉ„ÄÅÂÖÜÊπòÂúãÂ∞è„ÄÅÂÖ¨13„ÄÅÂÖ¨14„ÄÅÈôΩÊòéÂÖ¨ÂúíÂèäÂäâÂéùÂÖ¨ÂúíÔºåÂçÄ‰ΩçÊ¢ù‰ª∂‰Ω≥ÔºåÊäïË≥á‰∫∫Ê∫ñÂÇôÊê∂ÈÄ≤ÔºÅ Á¨¨77ÊúüÂ∏ÇÂú∞ÈáçÂäÉÂçÄÔºå‰ΩçÊñºÈ≥≥Â±±ÂçÄÂø´ÈÄüÈÅìË∑ØÁúÅÈÅìÂè∞88Á∑öÊóÅÔºåËøë‰∏≠Â±±È´ò‰∫îÁî≤Á≥ªÁµ±‰∫§ÊµÅÈÅìÔºåËøëÂπ¥Êé®Âá∫ÂúüÂú∞Ê®ôÂîÆÁöÜÈ†ÜÂà©ÂÆåÈä∑„ÄÇÊú¨Â≠£ÂÜçÊé®Âá∫2Á≠ÜÂúüÂú∞ÔºåÂÖ∂‰∏≠1Á≠ÜÈù¢Á©çÁ¥Ñ526Âù™ÔºåËá®‰øùËèØ‰∏ÄË∑ØÔºåÈÅ©ÂêàÂïÜÊ•≠‰ΩøÁî®Ôºõ1Á≠ÜÈù¢Á©ç107Âù™Ôºå‰ΩçÊñº‰ª£Âæ∑‰∏âË°óÔºåËá™Áî®ÊäïË≥áÂÖ©Áõ∏ÂÆú„ÄÇ È´òÈõÑÂ§ßÂ≠∏ÂçÄÊÆµÂæµÊî∂ÂçÄÔºåÁÇ∫ÂåóÈ´òÈõÑÂÑ™Ë≥™ÊñáÊïôÁâπÂçÄÔºåÂÑ™Ë≥™Â±Ö‰ΩèÁí∞Â¢ÉÔºåÂê∏ÂºïÊäïË≥á‰∫∫ÈÄ≤ÈßêÔºåÊú¨Â≠£ÂÜçÊé®Âá∫2Ê®ô2Á≠ÜÂúüÂú∞ÔºåÂÖ∂‰∏≠1Á≠ÜÁ¨¨‰∏âÁ®ÆÂïÜÊ•≠ÂçÄÂúüÂú∞ÔºåÈù¢Á©çÁ¥Ñ639Âù™Ôºå‰ΩçÊñºÂ§ßÂ≠∏26Ë°óÔºåËøëÈ´òÈõÑÂ§ßÂ≠∏Ê≠£ÈñÄÂèäËê¨Âù™ËóçÁî∞ÂÖ¨ÂúíÔºåÂú∞ÂΩ¢ÊñπÊ≠£Ôºå‰ΩøÁî®Âº∑Â∫¶È´òÔºåÈÅ©ÂêàËààÂª∫ÂÑ™Ë≥™‰ΩèÂÆÖÂ§ßÊ®ìÔºõÂè¶1Á≠Ü‰Ωè‰∏âÁî®Âú∞ÔºåÈù¢Á©çÁ¥Ñ379Âù™ÔºåËá®28Á±≥ËóçÊòåË∑ØÔºåËøëÈ´òÈõÑÂ§ßÂ≠∏Âèä‰∏≠Â±±È´ò‰∏≠Ôºå‰∫§ÈÄö‰æøÊç∑„ÄÇ Âè¶Á¨¨37ÊúüÈáçÂäÉÂçÄÂèäÂâçÂ§ßÂØÆËæ≤Âú∞ÈáçÂäÉÂçÄÂêÑÊé®Âá∫1Ëá≥2Á≠ÜÂúüÂú∞ÔºåÂÉπÊ†ºÂêàÁêÜ„ÄÇ Á¨¨4Â≠£ÂúüÂú∞Ê®ôÂîÆ‰ΩúÊ•≠Êñº109Âπ¥12Êúà1Êó•ÂÖ¨ÂëäÔºåÊäïË≥áÂ§ßÁúæÂèØÂâçÂæÄÂú∞ÊîøÂ±ÄÂúüÂú∞ÈñãÁôºËôïÂúüÂú∞ËôïÂàÜÁßëÁ¥¢ÂèñÊ®ôÂîÆÊµ∑Â†±ÂèäÊ®ôÂñÆÔºåÊàñÁõ¥Êé•‰∏äÁ∂≤È´òÈõÑÊàøÂú∞Áî¢ÂÑÑÂπ¥Êó∫Á∂≤Á´ô„ÄÅÂú∞ÊîøÂ±ÄÂèäÂúüÂú∞ÈñãÁôºËôïÁ∂≤Á´ôÊü•Ë©¢‰∏ãËºâÁõ∏ÈóúË≥áÊñôÔºåÂú®ÊúüÈôêÂâçÂÆåÊàêÊäïÊ®ôÔºåÂè¶ÂÜçÊèêÈÜíÊäïÊ®ô‰∫∫ÔºåÊú¨Âπ¥Â∫¶Â∑≤Êõ¥Êñ∞ÊäïÊ®ôÂñÆÊ†ºÂºèÔºåÊäïÊ®ôÂ§ßÁúæË´ãÊ≥®ÊÑèÊáâ‰ª•Êñ∞ÂºèÊäïÊ®ôÂñÆÊäïÊ®ô‰ª•ÂÖçÊäïÊ®ôÁÑ°Êïà‰ΩúÂª¢„ÄÇ ÁÇ∫ÈÖçÂêàÈò≤Áñ´ÈúÄÊ±ÇÔºåÊú¨Â≠£ÈñãÊ®ô‰ΩúÊ•≠Èô§ÊñºÂú∞ÊîøÂ±ÄÁ¨¨‰∏ÄÊúÉË≠∞ÂÆ§Ëæ¶ÁêÜÂ§ñÔºåÂè¶Â∞áÊñºÂú∞ÊîøÂ±ÄFacebookÁ≤âÁµ≤Â∞àÈ†ÅÂêåÊ≠•Áõ¥Êí≠ÔºåË´ãÂ§ßÁúæÂ§öÂä†Âà©Áî®„ÄÇ Ê¥ΩË©¢Â∞àÁ∑öÔºö(07)3373451Êàñ(07)3314942 È´òÈõÑÊàøÂú∞Áî¢ÂÑÑÂπ¥Êó∫Á∂≤Á´ôÔºàÁ∂≤ÂùÄÔºöhttp://eland.kcg.gov.tw/Ôºâ È´òÈõÑÂ∏ÇÊîøÂ∫úÂú∞ÊîøÂ±ÄÁ∂≤Á´ôÔºàÁ∂≤ÂùÄÔºöhttp://landp.kcg.gov.tw/Ôºâ È´òÈõÑÂ∏ÇÊîøÂ∫úÂú∞ÊîøÂ±ÄÂúüÂú∞ÈñãÁôºËôïÁ∂≤Á´ôÔºàÁ∂≤ÂùÄÔºöhttp://landevp.kcg.gov.tw/Ôºâ„ÄÄ &#39;&#39;&#39; raw_text[-300:] . &#39;ÂèäÂúüÂú∞ÈñãÁôºËôïÁ∂≤Á´ôÊü•Ë©¢‰∏ãËºâÁõ∏ÈóúË≥áÊñôÔºåÂú®ÊúüÈôêÂâçÂÆåÊàêÊäïÊ®ôÔºåÂè¶ÂÜçÊèêÈÜíÊäïÊ®ô‰∫∫ÔºåÊú¨Âπ¥Â∫¶Â∑≤Êõ¥Êñ∞ÊäïÊ®ôÂñÆÊ†ºÂºèÔºåÊäïÊ®ôÂ§ßÁúæË´ãÊ≥®ÊÑèÊáâ‰ª•Êñ∞ÂºèÊäïÊ®ôÂñÆÊäïÊ®ô‰ª•ÂÖçÊäïÊ®ôÁÑ°Êïà‰ΩúÂª¢„ÄÇ n n n nÁÇ∫ÈÖçÂêàÈò≤Áñ´ÈúÄÊ±ÇÔºåÊú¨Â≠£ÈñãÊ®ô‰ΩúÊ•≠Èô§ÊñºÂú∞ÊîøÂ±ÄÁ¨¨‰∏ÄÊúÉË≠∞ÂÆ§Ëæ¶ÁêÜÂ§ñÔºåÂè¶Â∞áÊñºÂú∞ÊîøÂ±ÄFacebookÁ≤âÁµ≤Â∞àÈ†ÅÂêåÊ≠•Áõ¥Êí≠ÔºåË´ãÂ§ßÁúæÂ§öÂä†Âà©Áî®„ÄÇ n n n nÊ¥ΩË©¢Â∞àÁ∑öÔºö(07)3373451Êàñ(07)3314942 n nÈ´òÈõÑÊàøÂú∞Áî¢ÂÑÑÂπ¥Êó∫Á∂≤Á´ôÔºàÁ∂≤ÂùÄÔºöhttp://eland.kcg.gov.tw/Ôºâ n nÈ´òÈõÑÂ∏ÇÊîøÂ∫úÂú∞ÊîøÂ±ÄÁ∂≤Á´ôÔºàÁ∂≤ÂùÄÔºöhttp://landp.kcg.gov.tw/Ôºâ n nÈ´òÈõÑÂ∏ÇÊîøÂ∫úÂú∞ÊîøÂ±ÄÂúüÂú∞ÈñãÁôºËôïÁ∂≤Á´ôÔºàÁ∂≤ÂùÄÔºöhttp://landevp.kcg.gov.tw/Ôºâ u3000 n&#39; . I find this lightweight library nlp2 quite handy for text cleaning. The clean_all function removes URL links, HTML elements, and unused tags. . . Note: I want to give a shoutout to Eric Lam, who created nlp2 and other useful NLP tools such as NLPrep, TFkit, and nlp2go. . !pip install nlp2 from nlp2 import clean_all . After cleaning, our sample text looks like this. Notice that all the URL links are gone now. . text = clean_all(raw_text) text[-300:] . &#39;ÂêàÁêÜ„ÄÇ n n n nÁ¨¨4Â≠£ÂúüÂú∞Ê®ôÂîÆ‰ΩúÊ•≠Êñº109Âπ¥12Êúà1Êó•ÂÖ¨ÂëäÔºåÊäïË≥áÂ§ßÁúæÂèØÂâçÂæÄÂú∞ÊîøÂ±ÄÂúüÂú∞ÈñãÁôºËôïÂúüÂú∞ËôïÂàÜÁßëÁ¥¢ÂèñÊ®ôÂîÆÊµ∑Â†±ÂèäÊ®ôÂñÆÔºåÊàñÁõ¥Êé•‰∏äÁ∂≤È´òÈõÑÊàøÂú∞Áî¢ÂÑÑÂπ¥Êó∫Á∂≤Á´ô„ÄÅÂú∞ÊîøÂ±ÄÂèäÂúüÂú∞ÈñãÁôºËôïÁ∂≤Á´ôÊü•Ë©¢‰∏ãËºâÁõ∏ÈóúË≥áÊñôÔºåÂú®ÊúüÈôêÂâçÂÆåÊàêÊäïÊ®ôÔºåÂè¶ÂÜçÊèêÈÜíÊäïÊ®ô‰∫∫ÔºåÊú¨Âπ¥Â∫¶Â∑≤Êõ¥Êñ∞ÊäïÊ®ôÂñÆÊ†ºÂºèÔºåÊäïÊ®ôÂ§ßÁúæË´ãÊ≥®ÊÑèÊáâ‰ª•Êñ∞ÂºèÊäïÊ®ôÂñÆÊäïÊ®ô‰ª•ÂÖçÊäïÊ®ôÁÑ°Êïà‰ΩúÂª¢„ÄÇ n n n nÁÇ∫ÈÖçÂêàÈò≤Áñ´ÈúÄÊ±ÇÔºåÊú¨Â≠£ÈñãÊ®ô‰ΩúÊ•≠Èô§ÊñºÂú∞ÊîøÂ±ÄÁ¨¨‰∏ÄÊúÉË≠∞ÂÆ§Ëæ¶ÁêÜÂ§ñÔºåÂè¶Â∞áÊñºÂú∞ÊîøÂ±ÄFacebookÁ≤âÁµ≤Â∞àÈ†ÅÂêåÊ≠•Áõ¥Êí≠ÔºåË´ãÂ§ßÁúæÂ§öÂä†Âà©Áî®„ÄÇ n n n nÊ¥ΩË©¢Â∞àÁ∑öÔºö 3373451Êàñ 3314942 n nÈ´òÈõÑÊàøÂú∞Áî¢ÂÑÑÂπ¥Êó∫Á∂≤Á´ôÔºàÁ∂≤ÂùÄÔºö Ôºâ n nÈ´òÈõÑÂ∏ÇÊîøÂ∫úÂú∞ÊîøÂ±ÄÁ∂≤Á´ôÔºàÁ∂≤ÂùÄÔºö Ôºâ n nÈ´òÈõÑÂ∏ÇÊîøÂ∫úÂú∞ÊîøÂ±ÄÂúüÂú∞ÈñãÁôºËôïÁ∂≤Á´ôÔºàÁ∂≤ÂùÄÔºö Ôºâ&#39; . Install spacy and ckip-transformers . !pip install -U pip setuptools wheel !pip install -U spacy !python -m spacy download zh_core_web_sm . !pip install -U ckip-transformers . Tokenize texts with ckip-transformers . Let&#39;s create a driver for word segmentation and one for parts of speech. CKIP Transformers also has a built-in driver for named entity recognition, i.e. CkipNerChunker. But we won&#39;t use it here. . . Tip: By default, CPU is used. If you want to use GPU to speed up word segmentation, initialize ws_driver this way instead: ws_driver = CkipWordSegmenter(device=-1) . from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger ws_driver = CkipWordSegmenter() pos_driver = CkipPosTagger() . . Important: Make sure that the input to ws_driver() is a list even if you&#8217;re only dealing with a single text. Otherwise, words won&#8217;t be properly segmented. Notice that the input to pos_driver() is the output of ws_driver(). . ws = ws_driver([text]) pos = pos_driver(ws) . Here&#39;re the segmented tokens. . tokens = ws[0] print(tokens) . [&#39;Â∏ÇÂ∫ú&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;109Âπ¥Â∫¶&#39;, &#39;Á¨¨4&#39;, &#39;Â≠£&#39;, &#39;ÈñãÁôºÂçÄ&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Ôºå&#39;, &#39;ÂÖ±Ë®à&#39;, &#39;Êé®Âá∫&#39;, &#39;8&#39;, &#39;Ê®ô&#39;, &#39;9&#39;, &#39;Á≠Ü&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Âª∫Âú∞&#39;, &#39;Ôºå&#39;, &#39;Ë®Ç&#39;, &#39;Êñº&#39;, &#39;109Âπ¥&#39;, &#39;12Êúà&#39;, &#39;16Êó•&#39;, &#39;ÈñãÊ®ô&#39;, &#39;Ôºå&#39;, &#39;ÂêàË®à&#39;, &#39;Á∏ΩÂ∫ïÂÉπ&#39;, &#39;12 ÂÑÑ&#39;, &#39;4049Ëê¨&#39;, &#39;6164 &#39;, &#39;ÂÖÉ&#39;, &#39;„ÄÇ&#39;, &#39; n n n n&#39;, &#39;Á¨¨93&#39;, &#39;Êúü&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;Âéü&#39;, &#39;ÁÇ∫&#39;, &#39;ÂúãËªç&#39;, &#39;Áú∑Êùë&#39;, &#39;Ôºå&#39;, &#39;Á∑äÈÑ∞&#39;, &#39;ÂúãÂÆö&#39;, &#39;Âè§Ëπü&#39;, &#39;-&#39;, &#39;„Äå&#39;, &#39;Âéü&#39;, &#39;Êó•Êú¨&#39;, &#39;Êµ∑Ëªç&#39;, &#39;È≥≥Â±±&#39;, &#39;ÁÑ°Á∑ö&#39;, &#39;Èõª‰ø°ÊâÄ&#39;, &#39;„Äç&#39;, &#39;Ôºå&#39;, &#39;Â∏ÇÂ∫ú&#39;, &#39;ÁÇ∫&#39;, &#39;‰øùÂ≠ò&#39;, &#39;Âè§Ëπü&#39;, &#39;ÂêåÊôÇ&#39;, &#39;Ê¥ªÂåñ&#39;, &#39;Áú∑Êùë&#39;, &#39;ÈÅ∑Áßª&#39;, &#39;Âæå&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;‰ª•&#39;, &#39;ÈáçÂäÉ&#39;, &#39;ÊñπÂºè&#39;, &#39;Êï¥È´î&#39;, &#39;ÈñãÁôº&#39;, &#39;Ôºå&#39;, &#39;Êñ∞&#39;, &#39;Èó¢&#39;, &#39;‰ΩèÂÆÖÂçÄ&#39;, &#39;„ÄÅ&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;ÂÅúËªäÂ†¥&#39;, &#39;Ôºå&#39;, &#39;‰Ωø&#39;, &#39;Êú¨&#39;, &#39;ÂçÄ&#39;, &#39;ÂÖ∑Êúâ&#39;, &#39;Ê≠∑Âè≤&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÂÖßÊ∂µ&#39;, &#39;Ëàá&#39;, &#39;Á∂†Ëâ≤&#39;, &#39;‰ºëÈñí&#39;, &#39;ÁâπËâ≤&#39;, &#39;Ôºå&#39;, &#39;ÁîüÊ¥ª&#39;, &#39;Ê©üËÉΩ&#39;, &#39;Êõ¥Âä†&#39;, &#39;ÂÅ•ÂÖ®&#39;, &#39;„ÄÇ&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;È¶ñÊ¨°&#39;, &#39;Êé®Âá∫&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Â§ß&#39;, &#39;Èù¢Á©ç&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;2160&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Âú∞ÂΩ¢&#39;, &#39;ÊñπÊï¥&#39;, &#39;Ôºå&#39;, &#39;ÈõôÈù¢&#39;, &#39;Ëá®&#39;, &#39;Ë∑Ø&#39;, &#39;Ôºå&#39;, &#39;Âà©Êñº&#39;, &#39;Ë¶èÂäÉ&#39;, &#39;ËààÂª∫&#39;, &#39;ÊôØËßÄ&#39;, &#39;Â§ßÊ®ì&#39;, &#39;Ôºå&#39;, &#39;ÈôÑËøë&#39;, &#39;Êúâ&#39;, &#39;Â∏ÇÂ†¥&#39;, &#39;„ÄÅ&#39;, &#39;Â≠∏Ê†°&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;Â§ßÊù±&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÂúíÂçÄ&#39;, &#39;Ôºå&#39;, &#39;Ë∑ù&#39;, &#39;Êç∑ÈÅã&#39;, &#39;Â§ßÊù±Á´ô&#39;, &#39;„ÄÅ&#39;, &#39;È≥≥Â±±&#39;, &#39;Âúã‰∏≠Á´ô&#39;, &#39;Âèä&#39;, &#39;È≥≥Â±±&#39;, &#39;ÁÅ´ËªäÁ´ô&#39;, &#39;ÂÉÖ&#39;, &#39;Êï∏&#39;, &#39;ÂàÜÈêò&#39;, &#39;ËªäÁ®ã&#39;, &#39;Ôºå&#39;, &#39;‰∫§ÈÄö&#39;, &#39;ÂõõÈÄöÂÖ´ÈÅî&#39;, &#39;Ôºå&#39;, &#39;Âõ†&#39;, &#39;ÂúüÂú∞&#39;, &#39;Á®ÄÂ∞ëÊÄß&#39;, &#39;Âèä&#39;, &#39;ÂçÄ‰Ωç&#39;, &#39;Ê¢ù‰ª∂&#39;, &#39;Áµï‰Ω≥&#39;, &#39;Ôºå&#39;, &#39;Âã¢ÂøÖ&#39;, &#39;ÊàêÁÇ∫&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;ËøΩÈÄê&#39;, &#39;ÁÑ¶Èªû&#39;, &#39;„ÄÇ&#39;, &#39; n n n n&#39;, &#39;Á¨¨87&#39;, &#39;Êúü&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;‰ΩçÊñº&#39;, &#39;ÁúÅÈÅì&#39;, &#39;Âè∞1Á∑ö&#39;, &#39;ÊóÅ&#39;, &#39;Ôºå&#39;, &#39;ÈÑ∞Ëøë&#39;, &#39;Êç∑ÈÅã&#39;, &#39;Âçó&#39;, &#39;Â≤°Â±±Á´ô&#39;, &#39;Ôºå&#39;, &#39;ÈáçÂäÉ&#39;, &#39;Âæå&#39;, &#39;ÊìÅÊúâ&#39;, &#39;ÂÆåÂñÑ&#39;, &#39;ÁöÑ&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;Á≥ªÁµ±&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Á∂†Âú∞&#39;, &#39;Âèä&#39;, &#39;ÊØóÈÑ∞&#39;, &#39;ÈÜíÊùë&#39;, &#39;Êá∑Ëàä&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÊôØËßÄ&#39;, &#39;Âª∫ÁØâÁæ§&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∑ÂÇô&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Â±Ö‰Ωè&#39;, &#39;Áí∞Â¢É&#39;, &#39;Âèä&#39;, &#39;‰∫§ÈÄö&#39;, &#39;‰æøÊç∑&#39;, &#39;Ë¶Å‰ª∂&#39;, &#39;Ôºå&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;‰∏Ä&#39;, &#39;Êé®Âá∫&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Ôºå&#39;, &#39;Âç≥&#39;, &#39;ÊéÄËµ∑&#39;, &#39;Êê∂Ê®ô&#39;, &#39;ÁÜ±ÊΩÆ&#39;, &#39;Ôºå&#39;, &#39;Êú¨&#39;, &#39;Â≠£&#39;, &#39;ÂÜç&#39;, &#39;ÈáãÂá∫&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;93&#39;, &#39;Âù™&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Ëá®&#39;, &#39;20&#39;, &#39;Á±≥&#39;, &#39;‰ªãÂ£ΩË∑Ø&#39;, &#39;Âèä&#39;, &#39;Èµ¨Á®ãÊù±Ë∑Ø&#39;, &#39;Ôºå&#39;, &#39;ÈôÑËøë&#39;, &#39;Êúâ&#39;, &#39;Â≤°Â±±&#39;, &#39;ÊñáÂåñ&#39;, &#39;‰∏≠ÂøÉ&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖÜÊπò&#39;, &#39;ÂúãÂ∞è&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨13&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨14&#39;, &#39;„ÄÅ&#39;, &#39;ÈôΩÊòé&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;ÂäâÂéù&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Ôºå&#39;, &#39;ÂçÄ‰Ωç&#39;, &#39;Ê¢ù‰ª∂&#39;, &#39;‰Ω≥&#39;, &#39;Ôºå&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;Ê∫ñÂÇô&#39;, &#39;Êê∂ÈÄ≤&#39;, &#39;ÔºÅ&#39;, &#39; n n n n&#39;, &#39;Á¨¨77&#39;, &#39;Êúü&#39;, &#39;Â∏ÇÂú∞&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;‰ΩçÊñº&#39;, &#39;È≥≥Â±±ÂçÄ&#39;, &#39;Âø´ÈÄü&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;ÁúÅÈÅì&#39;, &#39;Âè∞88&#39;, &#39;Á∑ö&#39;, &#39;ÊóÅ&#39;, &#39;Ôºå&#39;, &#39;Ëøë&#39;, &#39;‰∏≠Â±±È´ò&#39;, &#39;‰∫îÁî≤&#39;, &#39;Á≥ªÁµ±&#39;, &#39;‰∫§ÊµÅÈÅì&#39;, &#39;Ôºå&#39;, &#39;ËøëÂπ¥&#39;, &#39;Êé®Âá∫&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;ÁöÜ&#39;, &#39;È†ÜÂà©&#39;, &#39;ÂÆåÈä∑&#39;, &#39;„ÄÇ&#39;, &#39;Êú¨&#39;, &#39;Â≠£&#39;, &#39;ÂÜç&#39;, &#39;Êé®Âá∫&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∂‰∏≠&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;526&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Ëá®&#39;, &#39;‰øùËèØ‰∏ÄË∑Ø&#39;, &#39;Ôºå&#39;, &#39;ÈÅ©Âêà&#39;, &#39;ÂïÜÊ•≠&#39;, &#39;‰ΩøÁî®&#39;, &#39;Ôºõ&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Èù¢Á©ç&#39;, &#39;107&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;‰ΩçÊñº&#39;, &#39;‰ª£Âæ∑‰∏âË°ó&#39;, &#39;Ôºå&#39;, &#39;Ëá™Áî®&#39;, &#39;ÊäïË≥á&#39;, &#39;ÂÖ©&#39;, &#39;Áõ∏ÂÆú&#39;, &#39;„ÄÇ&#39;, &#39; n n n n&#39;, &#39;È´òÈõÑ&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;ÂçÄÊÆµ&#39;, &#39;ÂæµÊî∂ÂçÄ&#39;, &#39;Ôºå&#39;, &#39;ÁÇ∫&#39;, &#39;Âåó&#39;, &#39;È´òÈõÑ&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;ÊñáÊïô&#39;, &#39;ÁâπÂçÄ&#39;, &#39;Ôºå&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Â±Ö‰Ωè&#39;, &#39;Áí∞Â¢É&#39;, &#39;Ôºå&#39;, &#39;Âê∏Âºï&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;ÈÄ≤Èßê&#39;, &#39;Ôºå&#39;, &#39;Êú¨&#39;, &#39;Â≠£&#39;, &#39;ÂÜç&#39;, &#39;Êé®Âá∫&#39;, &#39;2&#39;, &#39;Ê®ô&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∂‰∏≠&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Á¨¨‰∏â&#39;, &#39;Á®Æ&#39;, &#39;ÂïÜÊ•≠ÂçÄ&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;639&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;‰ΩçÊñº&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;26Ë°ó&#39;, &#39;Ôºå&#39;, &#39;Ëøë&#39;, &#39;È´òÈõÑ&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;Ê≠£ÈñÄ&#39;, &#39;Âèä&#39;, &#39;Ëê¨&#39;, &#39;Âù™&#39;, &#39;ËóçÁî∞&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Ôºå&#39;, &#39;Âú∞ÂΩ¢&#39;, &#39;ÊñπÊ≠£&#39;, &#39;Ôºå&#39;, &#39;‰ΩøÁî®&#39;, &#39;Âº∑Â∫¶&#39;, &#39;È´ò&#39;, &#39;Ôºå&#39;, &#39;ÈÅ©Âêà&#39;, &#39;ËààÂª∫&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;‰ΩèÂÆÖ&#39;, &#39;Â§ßÊ®ì&#39;, &#39;Ôºõ&#39;, &#39;Âè¶&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;‰Ωè‰∏â&#39;, &#39;Áî®Âú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;379&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Ëá®&#39;, &#39;28&#39;, &#39;Á±≥&#39;, &#39;ËóçÊòåË∑Ø&#39;, &#39;Ôºå&#39;, &#39;Ëøë&#39;, &#39;È´òÈõÑ&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;Âèä&#39;, &#39;‰∏≠Â±±&#39;, &#39;È´ò‰∏≠&#39;, &#39;Ôºå&#39;, &#39;‰∫§ÈÄö&#39;, &#39;‰æøÊç∑&#39;, &#39;„ÄÇ&#39;, &#39; n n n n&#39;, &#39;Âè¶&#39;, &#39;Á¨¨37&#39;, &#39;Êúü&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Âèä&#39;, &#39;Ââç&#39;, &#39;Â§ßÂØÆ&#39;, &#39;Ëæ≤Âú∞&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;ÂêÑ&#39;, &#39;Êé®Âá∫&#39;, &#39;1&#39;, &#39;Ëá≥&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÉπÊ†º&#39;, &#39;ÂêàÁêÜ&#39;, &#39;„ÄÇ&#39;, &#39; n n n n&#39;, &#39;Á¨¨4&#39;, &#39;Â≠£&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;‰ΩúÊ•≠&#39;, &#39;Êñº&#39;, &#39;109Âπ¥&#39;, &#39;12Êúà&#39;, &#39;1Êó•&#39;, &#39;ÂÖ¨Âëä&#39;, &#39;Ôºå&#39;, &#39;ÊäïË≥á&#39;, &#39;Â§ßÁúæ&#39;, &#39;ÂèØ&#39;, &#39;ÂâçÂæÄ&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;ÂúüÂú∞Ëôï&#39;, &#39;ÂàÜÁßë&#39;, &#39;Á¥¢Âèñ&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Êµ∑Â†±&#39;, &#39;Âèä&#39;, &#39;Ê®ôÂñÆ&#39;, &#39;Ôºå&#39;, &#39;Êàñ&#39;, &#39;Áõ¥Êé•&#39;, &#39;‰∏äÁ∂≤&#39;, &#39;È´òÈõÑ&#39;, &#39;ÊàøÂú∞Áî¢&#39;, &#39;ÂÑÑÂπ¥Êó∫&#39;, &#39;Á∂≤Á´ô&#39;, &#39;„ÄÅ&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;Âèä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Êü•Ë©¢&#39;, &#39;‰∏ãËºâ&#39;, &#39;Áõ∏Èóú&#39;, &#39;Ë≥áÊñô&#39;, &#39;Ôºå&#39;, &#39;Âú®&#39;, &#39;ÊúüÈôê&#39;, &#39;Ââç&#39;, &#39;ÂÆåÊàê&#39;, &#39;ÊäïÊ®ô&#39;, &#39;Ôºå&#39;, &#39;Âè¶&#39;, &#39;ÂÜç&#39;, &#39;ÊèêÈÜí&#39;, &#39;ÊäïÊ®ô‰∫∫&#39;, &#39;Ôºå&#39;, &#39;Êú¨&#39;, &#39;Âπ¥Â∫¶&#39;, &#39;Â∑≤&#39;, &#39;Êõ¥Êñ∞&#39;, &#39;ÊäïÊ®ôÂñÆ&#39;, &#39;Ê†ºÂºè&#39;, &#39;Ôºå&#39;, &#39;ÊäïÊ®ô&#39;, &#39;Â§ßÁúæ&#39;, &#39;Ë´ã&#39;, &#39;Ê≥®ÊÑè&#39;, &#39;Êáâ&#39;, &#39;‰ª•&#39;, &#39;Êñ∞Âºè&#39;, &#39;ÊäïÊ®ôÂñÆ&#39;, &#39;ÊäïÊ®ô&#39;, &#39;‰ª•ÂÖç&#39;, &#39;ÊäïÊ®ô&#39;, &#39;ÁÑ°Êïà&#39;, &#39;‰ΩúÂª¢&#39;, &#39;„ÄÇ&#39;, &#39; n n n n&#39;, &#39;ÁÇ∫&#39;, &#39;ÈÖçÂêà&#39;, &#39;Èò≤Áñ´&#39;, &#39;ÈúÄÊ±Ç&#39;, &#39;Ôºå&#39;, &#39;Êú¨&#39;, &#39;Â≠£&#39;, &#39;ÈñãÊ®ô&#39;, &#39;‰ΩúÊ•≠&#39;, &#39;Èô§&#39;, &#39;Êñº&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;Á¨¨‰∏Ä&#39;, &#39;ÊúÉË≠∞ÂÆ§&#39;, &#39;Ëæ¶ÁêÜ&#39;, &#39;Â§ñ&#39;, &#39;Ôºå&#39;, &#39;Âè¶&#39;, &#39;Â∞á&#39;, &#39;Êñº&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;Facebook&#39;, &#39;Á≤âÁµ≤&#39;, &#39;Â∞àÈ†Å&#39;, &#39;ÂêåÊ≠•&#39;, &#39;Áõ¥Êí≠&#39;, &#39;Ôºå&#39;, &#39;Ë´ã&#39;, &#39;Â§ßÁúæ&#39;, &#39;Â§öÂä†&#39;, &#39;Âà©Áî®&#39;, &#39;„ÄÇ&#39;, &#39; n n n n&#39;, &#39;Ê¥ΩË©¢&#39;, &#39;Â∞àÁ∑ö&#39;, &#39;Ôºö&#39;, &#39; 3373&#39;, &#39;451&#39;, &#39;Êàñ&#39;, &#39; 3314942&#39;, &#39; n&#39;, &#39; n&#39;, &#39;È´òÈõÑ&#39;, &#39;ÊàøÂú∞Áî¢&#39;, &#39;ÂÑÑ&#39;, &#39;Âπ¥&#39;, &#39;Êó∫&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39; &#39;, &#39;Ôºâ&#39;, &#39; n&#39;, &#39; n&#39;, &#39;È´òÈõÑÂ∏Ç&#39;, &#39;ÊîøÂ∫ú&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39; &#39;, &#39;Ôºâ&#39;, &#39; n&#39;, &#39; n&#39;, &#39;È´òÈõÑÂ∏Ç&#39;, &#39;ÊîøÂ∫ú&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39; &#39;, &#39;Ôºâ&#39;] . By contrast, Jieba produced lots of wrongly segmented tokens, which is precisely why we prefer CKIP Transformers. . import jieba print(list(jieba.cut(text))) . Building prefix dict from the default dictionary ... Dumping model to file cache /tmp/jieba.cache Loading model cost 0.958 seconds. Prefix dict has been built successfully. . [&#39;Â∏ÇÂ∫ú&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;109&#39;, &#39;Âπ¥Â∫¶&#39;, &#39;Á¨¨&#39;, &#39;4&#39;, &#39;Â≠£Èñã&#39;, &#39;ÁôºÂçÄ&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Ôºå&#39;, &#39;ÂÖ±Ë®à&#39;, &#39;Êé®Âá∫&#39;, &#39;8&#39;, &#39;Ê®ô&#39;, &#39;9&#39;, &#39;Á≠ÜÂÑ™Ë≥™&#39;, &#39;Âª∫Âú∞&#39;, &#39;Ôºå&#39;, &#39;Ë®Ç&#39;, &#39;Êñº&#39;, &#39;109&#39;, &#39;Âπ¥&#39;, &#39;12&#39;, &#39;Êúà&#39;, &#39;16&#39;, &#39;Êó•&#39;, &#39;ÈñãÊ®ô&#39;, &#39;Ôºå&#39;, &#39;ÂêàË®à&#39;, &#39;Á∏ΩÂ∫ïÂÉπ&#39;, &#39;12&#39;, &#39; &#39;, &#39;ÂÑÑ&#39;, &#39;4049&#39;, &#39;Ëê¨&#39;, &#39;6164&#39;, &#39; &#39;, &#39;ÂÖÉ&#39;, &#39;„ÄÇ&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;Á¨¨&#39;, &#39;93&#39;, &#39;ÊúüÈáç&#39;, &#39;ÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;ÂéüÁÇ∫Âúã&#39;, &#39;Ëªç&#39;, &#39;Áú∑Êùë&#39;, &#39;Ôºå&#39;, &#39;Á∑äÈÑ∞&#39;, &#39;ÂúãÂÆö&#39;, &#39;Âè§&#39;, &#39;Ëπü&#39;, &#39;-&#39;, &#39;„Äå&#39;, &#39;Âéü&#39;, &#39;Êó•Êú¨Êµ∑&#39;, &#39;ËªçÈ≥≥Â±±&#39;, &#39;ÁÑ°Á∑ö&#39;, &#39;Èõª‰ø°&#39;, &#39;ÊâÄ&#39;, &#39;„Äç&#39;, &#39;Ôºå&#39;, &#39;Â∏ÇÂ∫ú&#39;, &#39;ÁÇ∫&#39;, &#39;‰øùÂ≠ò&#39;, &#39;Âè§&#39;, &#39;Ëπü&#39;, &#39;ÂêåÊôÇ&#39;, &#39;Ê¥ªÂåñ&#39;, &#39;Áú∑Êùë&#39;, &#39;ÈÅ∑Áßª&#39;, &#39;Âæå&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;‰ª•&#39;, &#39;ÈáçÂäÉ&#39;, &#39;ÊñπÂºè&#39;, &#39;Êï¥È´î&#39;, &#39;ÈñãÁôº&#39;, &#39;Ôºå&#39;, &#39;Êñ∞Èó¢&#39;, &#39;‰ΩèÂÆÖ&#39;, &#39;ÂçÄ&#39;, &#39;„ÄÅ&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;ÂÅúËªäÂ†¥&#39;, &#39;Ôºå&#39;, &#39;‰ΩøÊú¨ÂçÄ&#39;, &#39;ÂÖ∑Êúâ&#39;, &#39;Ê≠∑Âè≤&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÂÖßÊ∂µ&#39;, &#39;Ëàá&#39;, &#39;Á∂†Ëâ≤&#39;, &#39;‰ºëÈñí&#39;, &#39;ÁâπËâ≤&#39;, &#39;Ôºå&#39;, &#39;ÁîüÊ¥ª&#39;, &#39;Ê©üËÉΩ&#39;, &#39;Êõ¥Âä†&#39;, &#39;ÂÅ•ÂÖ®&#39;, &#39;„ÄÇ&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;È¶ñÊ¨°&#39;, &#39;Êé®Âá∫&#39;, &#39;1&#39;, &#39;Á≠ÜÂ§ßÈù¢Á©ç&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;2160&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Âú∞ÂΩ¢&#39;, &#39;ÊñπÊï¥&#39;, &#39;Ôºå&#39;, &#39;ÈõôÈù¢&#39;, &#39;Ëá®Ë∑Ø&#39;, &#39;Ôºå&#39;, &#39;Âà©&#39;, &#39;Êñº&#39;, &#39;Ë¶èÂäÉ&#39;, &#39;ËààÂª∫ÊôØ&#39;, &#39;ËßÄÂ§ßÊ®ì&#39;, &#39;Ôºå&#39;, &#39;ÈôÑËøë&#39;, &#39;Êúâ&#39;, &#39;Â∏ÇÂ†¥&#39;, &#39;„ÄÅ&#39;, &#39;Â≠∏Ê†°&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;Â§ßÊù±&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÂúíÂçÄ&#39;, &#39;Ôºå&#39;, &#39;Ë∑ùÊç∑&#39;, &#39;ÈÅãÂ§ßÊù±&#39;, &#39;Á´ô&#39;, &#39;„ÄÅ&#39;, &#39;È≥≥Â±±Âúã&#39;, &#39;‰∏≠Á´ô&#39;, &#39;ÂèäÈ≥≥&#39;, &#39;Â±±ÁÅ´&#39;, &#39;ËªäÁ´ô&#39;, &#39;ÂÉÖÊï∏&#39;, &#39;ÂàÜÈêò&#39;, &#39;ËªäÁ®ã&#39;, &#39;Ôºå&#39;, &#39;‰∫§ÈÄö&#39;, &#39;ÂõõÈÄö&#39;, &#39;ÂÖ´ÈÅî&#39;, &#39;Ôºå&#39;, &#39;Âõ†&#39;, &#39;ÂúüÂú∞&#39;, &#39;Á®ÄÂ∞ë&#39;, &#39;ÊÄßÂèä&#39;, &#39;ÂçÄ‰Ωç&#39;, &#39;Ê¢ù‰ª∂&#39;, &#39;Áµï‰Ω≥&#39;, &#39;Ôºå&#39;, &#39;Âã¢ÂøÖÊàê&#39;, &#39;ÁÇ∫&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;ËøΩÈÄê&#39;, &#39;ÁÑ¶Èªû&#39;, &#39;„ÄÇ&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;Á¨¨&#39;, &#39;87&#39;, &#39;ÊúüÈáç&#39;, &#39;ÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;‰Ωç&#39;, &#39;Êñº&#39;, &#39;ÁúÅ&#39;, &#39;ÈÅì&#39;, &#39;Âè∞&#39;, &#39;1&#39;, &#39;Á∑öÊóÅ&#39;, &#39;Ôºå&#39;, &#39;ÈÑ∞Ëøë&#39;, &#39;Êç∑ÈÅã&#39;, &#39;ÂçóÂ≤°Â±±&#39;, &#39;Á´ô&#39;, &#39;Ôºå&#39;, &#39;ÈáçÂäÉ&#39;, &#39;Âæå&#39;, &#39;ÊìÅÊúâ&#39;, &#39;ÂÆåÂñÑ&#39;, &#39;ÁöÑ&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;Á≥ªÁµ±&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Á∂†Âú∞&#39;, &#39;Âèä&#39;, &#39;ÊØó&#39;, &#39;ÈÑ∞ÈÜí&#39;, &#39;ÊùëÊá∑Ëàä&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÊôØËßÄ&#39;, &#39;Âª∫ÁØâÁæ§&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∑ÂÇô&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Â±Ö‰Ωè&#39;, &#39;Áí∞Â¢É&#39;, &#39;Âèä&#39;, &#39;‰∫§ÈÄö&#39;, &#39;‰æøÊç∑&#39;, &#39;Ë¶Å‰ª∂&#39;, &#39;Ôºå&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;‰∏Ä&#39;, &#39;Êé®Âá∫&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Ôºå&#39;, &#39;Âç≥&#39;, &#39;ÊéÄËµ∑&#39;, &#39;Êê∂Ê®ô&#39;, &#39;ÁÜ±ÊΩÆ&#39;, &#39;Ôºå&#39;, &#39;Êú¨Â≠£&#39;, &#39;ÂÜçÈáã&#39;, &#39;Âá∫&#39;, &#39;1&#39;, &#39;Á≠ÜÈù¢&#39;, &#39;Á©çÁ¥Ñ&#39;, &#39;93&#39;, &#39;Âù™&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Ëá®&#39;, &#39;20&#39;, &#39;Á±≥&#39;, &#39;‰ªãÂ£ΩË∑Ø&#39;, &#39;ÂèäÈµ¨Á®ã&#39;, &#39;Êù±Ë∑Ø&#39;, &#39;Ôºå&#39;, &#39;ÈôÑËøë&#39;, &#39;ÊúâÂ≤°Â±±&#39;, &#39;ÊñáÂåñ&#39;, &#39;‰∏≠ÂøÉ&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖÜ&#39;, &#39;ÊπòÂúã&#39;, &#39;Â∞è&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨&#39;, &#39;13&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨&#39;, &#39;14&#39;, &#39;„ÄÅ&#39;, &#39;ÈôΩÊòé&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;ÂäâÂéùÂÖ¨Âúí&#39;, &#39;Ôºå&#39;, &#39;ÂçÄ‰Ωç&#39;, &#39;Ê¢ù‰ª∂&#39;, &#39;‰Ω≥&#39;, &#39;Ôºå&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;Ê∫ñÂÇô&#39;, &#39;Êê∂ÈÄ≤&#39;, &#39;ÔºÅ&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;Á¨¨&#39;, &#39;77&#39;, &#39;ÊúüÂ∏Ç&#39;, &#39;Âú∞&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;‰Ωç&#39;, &#39;Êñº&#39;, &#39;È≥≥Â±±ÂçÄ&#39;, &#39;Âø´ÈÄüÈÅìË∑Ø&#39;, &#39;ÁúÅÈÅì&#39;, &#39;Âè∞&#39;, &#39;88&#39;, &#39;Á∑öÊóÅ&#39;, &#39;Ôºå&#39;, &#39;Ëøë&#39;, &#39;‰∏≠Â±±&#39;, &#39;È´ò&#39;, &#39;‰∫îÁî≤&#39;, &#39;Á≥ªÁµ±&#39;, &#39;‰∫§ÊµÅ&#39;, &#39;ÈÅì&#39;, &#39;Ôºå&#39;, &#39;ËøëÂπ¥&#39;, &#39;Êé®Âá∫&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;ÁöÜ&#39;, &#39;È†ÜÂà©&#39;, &#39;ÂÆåÈä∑&#39;, &#39;„ÄÇ&#39;, &#39;Êú¨Â≠£&#39;, &#39;ÂÜç&#39;, &#39;Êé®Âá∫&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∂‰∏≠&#39;, &#39;1&#39;, &#39;Á≠ÜÈù¢&#39;, &#39;Á©çÁ¥Ñ&#39;, &#39;526&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Ëá®‰øùËèØ&#39;, &#39;‰∏ÄË∑Ø&#39;, &#39;Ôºå&#39;, &#39;ÈÅ©Âêà&#39;, &#39;ÂïÜÊ•≠&#39;, &#39;‰ΩøÁî®&#39;, &#39;Ôºõ&#39;, &#39;1&#39;, &#39;Á≠ÜÈù¢Á©ç&#39;, &#39;107&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;‰Ωç&#39;, &#39;Êñº&#39;, &#39;‰ª£Âæ∑‰∏âË°ó&#39;, &#39;Ôºå&#39;, &#39;Ëá™Áî®&#39;, &#39;ÊäïË≥á&#39;, &#39;ÂÖ©&#39;, &#39;Áõ∏ÂÆú&#39;, &#39;„ÄÇ&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;È´òÈõÑ&#39;, &#39;Â§ßÂ≠∏ÂçÄ&#39;, &#39;ÊÆµ&#39;, &#39;ÂæµÊî∂&#39;, &#39;ÂçÄ&#39;, &#39;Ôºå&#39;, &#39;ÁÇ∫&#39;, &#39;ÂåóÈ´òÈõÑ&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;ÊñáÊïô&#39;, &#39;ÁâπÂçÄ&#39;, &#39;Ôºå&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Â±Ö‰Ωè&#39;, &#39;Áí∞Â¢É&#39;, &#39;Ôºå&#39;, &#39;Âê∏Âºï&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;ÈÄ≤Èßê&#39;, &#39;Ôºå&#39;, &#39;Êú¨Â≠£&#39;, &#39;ÂÜç&#39;, &#39;Êé®Âá∫&#39;, &#39;2&#39;, &#39;Ê®ô&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∂‰∏≠&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Á¨¨‰∏â&#39;, &#39;Á®ÆÂïÜÊ•≠ÂçÄ&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;639&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;‰Ωç&#39;, &#39;Êñº&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;26&#39;, &#39;Ë°ó&#39;, &#39;Ôºå&#39;, &#39;ËøëÈ´òÈõÑ&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;Ê≠£ÈñÄ&#39;, &#39;Âèä&#39;, &#39;Ëê¨Âù™&#39;, &#39;ËóçÁî∞ÂÖ¨Âúí&#39;, &#39;Ôºå&#39;, &#39;Âú∞ÂΩ¢&#39;, &#39;ÊñπÊ≠£&#39;, &#39;Ôºå&#39;, &#39;‰ΩøÁî®&#39;, &#39;Âº∑Â∫¶&#39;, &#39;È´ò&#39;, &#39;Ôºå&#39;, &#39;ÈÅ©Âêà&#39;, &#39;ËààÂª∫&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;‰ΩèÂÆÖ&#39;, &#39;Â§ßÊ®ì&#39;, &#39;Ôºõ&#39;, &#39;Âè¶&#39;, &#39;1&#39;, &#39;Á≠Ü‰Ωè&#39;, &#39;‰∏â&#39;, &#39;Áî®Âú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;379&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Ëá®&#39;, &#39;28&#39;, &#39;Á±≥&#39;, &#39;ËóçÊòåË∑Ø&#39;, &#39;Ôºå&#39;, &#39;ËøëÈ´òÈõÑ&#39;, &#39;Â§ßÂ≠∏Âèä&#39;, &#39;‰∏≠Â±±&#39;, &#39;È´ò‰∏≠&#39;, &#39;Ôºå&#39;, &#39;‰∫§ÈÄö&#39;, &#39;‰æøÊç∑&#39;, &#39;„ÄÇ&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;Âè¶&#39;, &#39;Á¨¨&#39;, &#39;37&#39;, &#39;ÊúüÈáç&#39;, &#39;ÂäÉÂçÄ&#39;, &#39;Âèä&#39;, &#39;Ââç&#39;, &#39;Â§ß&#39;, &#39;ÂØÆ&#39;, &#39;Ëæ≤Âú∞&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;ÂêÑ&#39;, &#39;Êé®Âá∫&#39;, &#39;1&#39;, &#39;Ëá≥&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÉπÊ†º&#39;, &#39;ÂêàÁêÜ&#39;, &#39;„ÄÇ&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;Á¨¨&#39;, &#39;4&#39;, &#39;Â≠£&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;‰ΩúÊ•≠&#39;, &#39;Êñº&#39;, &#39;109&#39;, &#39;Âπ¥&#39;, &#39;12&#39;, &#39;Êúà&#39;, &#39;1&#39;, &#39;Êó•&#39;, &#39;ÂÖ¨Âëä&#39;, &#39;Ôºå&#39;, &#39;ÊäïË≥áÂ§ßÁúæ&#39;, &#39;ÂèØ&#39;, &#39;ÂâçÂæÄ&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ëôï&#39;, &#39;ÂàÜÁßë&#39;, &#39;Á¥¢Âèñ&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Êµ∑Â†±&#39;, &#39;Âèä&#39;, &#39;Ê®ôÂñÆ&#39;, &#39;Ôºå&#39;, &#39;Êàñ&#39;, &#39;Áõ¥Êé•&#39;, &#39;‰∏äÁ∂≤&#39;, &#39;È´òÈõÑÊàø&#39;, &#39;Âú∞Áî¢&#39;, &#39;ÂÑÑÂπ¥&#39;, &#39;Êó∫&#39;, &#39;Á∂≤Á´ô&#39;, &#39;„ÄÅ&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;Âèä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Êü•Ë©¢&#39;, &#39;‰∏ãËºâ&#39;, &#39;Áõ∏Èóú&#39;, &#39;Ë≥áÊñô&#39;, &#39;Ôºå&#39;, &#39;Âú®&#39;, &#39;ÊúüÈôê&#39;, &#39;Ââç&#39;, &#39;ÂÆåÊàê&#39;, &#39;ÊäïÊ®ô&#39;, &#39;Ôºå&#39;, &#39;Âè¶&#39;, &#39;ÂÜç&#39;, &#39;ÊèêÈÜí&#39;, &#39;ÊäïÊ®ô&#39;, &#39;‰∫∫&#39;, &#39;Ôºå&#39;, &#39;Êú¨Âπ¥Â∫¶&#39;, &#39;Â∑≤&#39;, &#39;Êõ¥Êñ∞&#39;, &#39;ÊäïÊ®ô&#39;, &#39;ÂñÆ&#39;, &#39;Ê†ºÂºè&#39;, &#39;Ôºå&#39;, &#39;ÊäïÊ®ô&#39;, &#39;Â§ßÁúæ&#39;, &#39;Ë´ã&#39;, &#39;Ê≥®ÊÑè&#39;, &#39;Êáâ‰ª•&#39;, &#39;Êñ∞Âºè&#39;, &#39;ÊäïÊ®ôÂñÆ&#39;, &#39;ÊäïÊ®ô&#39;, &#39;‰ª•ÂÖç&#39;, &#39;ÊäïÊ®ô&#39;, &#39;ÁÑ°Êïà‰Ωú&#39;, &#39;Âª¢&#39;, &#39;„ÄÇ&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;ÁÇ∫&#39;, &#39;ÈÖçÂêà&#39;, &#39;Èò≤Áñ´&#39;, &#39;ÈúÄÊ±Ç&#39;, &#39;Ôºå&#39;, &#39;Êú¨Â≠£&#39;, &#39;ÈñãÊ®ô&#39;, &#39;‰ΩúÊ•≠&#39;, &#39;Èô§&#39;, &#39;Êñº&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;Á¨¨‰∏Ä&#39;, &#39;ÊúÉË≠∞ÂÆ§&#39;, &#39;Ëæ¶ÁêÜÂ§ñ&#39;, &#39;Ôºå&#39;, &#39;Âè¶Â∞á&#39;, &#39;Êñº&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;Facebook&#39;, &#39;Á≤âÁµ≤&#39;, &#39;Â∞àÈ†Å&#39;, &#39;ÂêåÊ≠•&#39;, &#39;Áõ¥Êí≠&#39;, &#39;Ôºå&#39;, &#39;Ë´ãÂ§ßÁúæ&#39;, &#39;Â§öÂä†&#39;, &#39;Âà©Áî®&#39;, &#39;„ÄÇ&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;Ê¥ΩË©¢&#39;, &#39;Â∞àÁ∑ö&#39;, &#39;Ôºö&#39;, &#39; &#39;, &#39;3373451&#39;, &#39;Êàñ&#39;, &#39; &#39;, &#39;3314942&#39;, &#39; n&#39;, &#39; n&#39;, &#39;È´òÈõÑÊàø&#39;, &#39;Âú∞Áî¢&#39;, &#39;ÂÑÑÂπ¥&#39;, &#39;Êó∫&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39; &#39;, &#39;Ôºâ&#39;, &#39; n&#39;, &#39; n&#39;, &#39;È´òÈõÑÂ∏Ç&#39;, &#39;ÊîøÂ∫ú&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39; &#39;, &#39;Ôºâ&#39;, &#39; n&#39;, &#39; n&#39;, &#39;È´òÈõÑÂ∏Ç&#39;, &#39;ÊîøÂ∫ú&#39;, &#39;Âú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39; &#39;, &#39;Ôºâ&#39;] . Feed tokenized results to spacy using WhitespaceTokenizer . The official website of spaCy describes several ways of adding a custom tokenizer. The simplest is to define the WhitespaceTokenizer class, which tokenizes a text on space characters. The output of tokenization can then be fed into subsequent operations down the pipeline, including tagger for parts-of-speech (POS) tagging, parser for dependency parsing, and ner for named entity recognition. This is possible primarily because tokenizer creates a Doc object whereas the other three steps operate on the Doc object, as illustrated in this graph. . . Note: The original code for words is words = text.split(&quot; &quot;), but it caused an error to my text. So I revised it into words = text.strip().split(). . from spacy.tokens import Doc class WhitespaceTokenizer: def __init__(self, vocab): self.vocab = vocab def __call__(self, text): words = text.strip().split() return Doc(self.vocab, words=words) . Next, let&#39;s load the zh_core_web_sm model for Chinese, which we&#39;ll need for POS tagging. Then here comes the crucial part: nlp.tokenizer = WhitespaceTokenizer(nlp.vocab). This line of code sets the default tokenizer from Jieba to WhitespaceTokenizer, which we just defined above. . import spacy nlp = spacy.load(&#39;zh_core_web_sm&#39;) nlp.tokenizer = WhitespaceTokenizer(nlp.vocab) . Then we join the tokenized result from CKIP Transformers to a single string of space-seperated tokens. . token_str = &quot; &quot;.join(tokens) token_str . &#39;Â∏ÇÂ∫ú Âú∞ÊîøÂ±Ä 109Âπ¥Â∫¶ Á¨¨4 Â≠£ ÈñãÁôºÂçÄ ÂúüÂú∞ Ê®ôÂîÆ Ôºå ÂÖ±Ë®à Êé®Âá∫ 8 Ê®ô 9 Á≠Ü ÂÑ™Ë≥™ Âª∫Âú∞ Ôºå Ë®Ç Êñº 109Âπ¥ 12Êúà 16Êó• ÈñãÊ®ô Ôºå ÂêàË®à Á∏ΩÂ∫ïÂÉπ 12 ÂÑÑ 4049Ëê¨ 6164 ÂÖÉ „ÄÇ n n n n Á¨¨93 Êúü ÈáçÂäÉÂçÄ Ôºå Âéü ÁÇ∫ ÂúãËªç Áú∑Êùë Ôºå Á∑äÈÑ∞ ÂúãÂÆö Âè§Ëπü - „Äå Âéü Êó•Êú¨ Êµ∑Ëªç È≥≥Â±± ÁÑ°Á∑ö Èõª‰ø°ÊâÄ „Äç Ôºå Â∏ÇÂ∫ú ÁÇ∫ ‰øùÂ≠ò Âè§Ëπü ÂêåÊôÇ Ê¥ªÂåñ Áú∑Êùë ÈÅ∑Áßª Âæå ÂúüÂú∞ Ôºå ‰ª• ÈáçÂäÉ ÊñπÂºè Êï¥È´î ÈñãÁôº Ôºå Êñ∞ Èó¢ ‰ΩèÂÆÖÂçÄ „ÄÅ ÈÅìË∑Ø „ÄÅ ÂÖ¨Âúí Âèä ÂÅúËªäÂ†¥ Ôºå ‰Ωø Êú¨ ÂçÄ ÂÖ∑Êúâ Ê≠∑Âè≤ ÊñáÂåñ ÂÖßÊ∂µ Ëàá Á∂†Ëâ≤ ‰ºëÈñí ÁâπËâ≤ Ôºå ÁîüÊ¥ª Ê©üËÉΩ Êõ¥Âä† ÂÅ•ÂÖ® „ÄÇ Âú∞ÊîøÂ±Ä È¶ñÊ¨° Êé®Âá∫ 1 Á≠Ü Â§ß Èù¢Á©ç ÂúüÂú∞ Ôºå Èù¢Á©ç Á¥Ñ 2160 Âù™ Ôºå Âú∞ÂΩ¢ ÊñπÊï¥ Ôºå ÈõôÈù¢ Ëá® Ë∑Ø Ôºå Âà©Êñº Ë¶èÂäÉ ËààÂª∫ ÊôØËßÄ Â§ßÊ®ì Ôºå ÈôÑËøë Êúâ Â∏ÇÂ†¥ „ÄÅ Â≠∏Ê†° „ÄÅ ÂÖ¨Âúí Âèä Â§ßÊù± ÊñáÂåñ ÂúíÂçÄ Ôºå Ë∑ù Êç∑ÈÅã Â§ßÊù±Á´ô „ÄÅ È≥≥Â±± Âúã‰∏≠Á´ô Âèä È≥≥Â±± ÁÅ´ËªäÁ´ô ÂÉÖ Êï∏ ÂàÜÈêò ËªäÁ®ã Ôºå ‰∫§ÈÄö ÂõõÈÄöÂÖ´ÈÅî Ôºå Âõ† ÂúüÂú∞ Á®ÄÂ∞ëÊÄß Âèä ÂçÄ‰Ωç Ê¢ù‰ª∂ Áµï‰Ω≥ Ôºå Âã¢ÂøÖ ÊàêÁÇ∫ ÊäïË≥á‰∫∫ ËøΩÈÄê ÁÑ¶Èªû „ÄÇ n n n n Á¨¨87 Êúü ÈáçÂäÉÂçÄ Ôºå ‰ΩçÊñº ÁúÅÈÅì Âè∞1Á∑ö ÊóÅ Ôºå ÈÑ∞Ëøë Êç∑ÈÅã Âçó Â≤°Â±±Á´ô Ôºå ÈáçÂäÉ Âæå ÊìÅÊúâ ÂÆåÂñÑ ÁöÑ ÈÅìË∑Ø Á≥ªÁµ± „ÄÅ ÂÖ¨Âúí Á∂†Âú∞ Âèä ÊØóÈÑ∞ ÈÜíÊùë Êá∑Ëàä ÊñáÂåñ ÊôØËßÄ Âª∫ÁØâÁæ§ Ôºå ÂÖ∑ÂÇô ÂÑ™Ë≥™ Â±Ö‰Ωè Áí∞Â¢É Âèä ‰∫§ÈÄö ‰æøÊç∑ Ë¶Å‰ª∂ Ôºå Âú∞ÊîøÂ±Ä ‰∏Ä Êé®Âá∫ ÂúüÂú∞ Ê®ôÂîÆ Ôºå Âç≥ ÊéÄËµ∑ Êê∂Ê®ô ÁÜ±ÊΩÆ Ôºå Êú¨ Â≠£ ÂÜç ÈáãÂá∫ 1 Á≠Ü Èù¢Á©ç Á¥Ñ 93 Âù™ ÂúüÂú∞ Ôºå Ëá® 20 Á±≥ ‰ªãÂ£ΩË∑Ø Âèä Èµ¨Á®ãÊù±Ë∑Ø Ôºå ÈôÑËøë Êúâ Â≤°Â±± ÊñáÂåñ ‰∏≠ÂøÉ „ÄÅ ÂÖÜÊπò ÂúãÂ∞è „ÄÅ ÂÖ¨13 „ÄÅ ÂÖ¨14 „ÄÅ ÈôΩÊòé ÂÖ¨Âúí Âèä ÂäâÂéù ÂÖ¨Âúí Ôºå ÂçÄ‰Ωç Ê¢ù‰ª∂ ‰Ω≥ Ôºå ÊäïË≥á‰∫∫ Ê∫ñÂÇô Êê∂ÈÄ≤ ÔºÅ n n n n Á¨¨77 Êúü Â∏ÇÂú∞ ÈáçÂäÉÂçÄ Ôºå ‰ΩçÊñº È≥≥Â±±ÂçÄ Âø´ÈÄü ÈÅìË∑Ø ÁúÅÈÅì Âè∞88 Á∑ö ÊóÅ Ôºå Ëøë ‰∏≠Â±±È´ò ‰∫îÁî≤ Á≥ªÁµ± ‰∫§ÊµÅÈÅì Ôºå ËøëÂπ¥ Êé®Âá∫ ÂúüÂú∞ Ê®ôÂîÆ ÁöÜ È†ÜÂà© ÂÆåÈä∑ „ÄÇ Êú¨ Â≠£ ÂÜç Êé®Âá∫ 2 Á≠Ü ÂúüÂú∞ Ôºå ÂÖ∂‰∏≠ 1 Á≠Ü Èù¢Á©ç Á¥Ñ 526 Âù™ Ôºå Ëá® ‰øùËèØ‰∏ÄË∑Ø Ôºå ÈÅ©Âêà ÂïÜÊ•≠ ‰ΩøÁî® Ôºõ 1 Á≠Ü Èù¢Á©ç 107 Âù™ Ôºå ‰ΩçÊñº ‰ª£Âæ∑‰∏âË°ó Ôºå Ëá™Áî® ÊäïË≥á ÂÖ© Áõ∏ÂÆú „ÄÇ n n n n È´òÈõÑ Â§ßÂ≠∏ ÂçÄÊÆµ ÂæµÊî∂ÂçÄ Ôºå ÁÇ∫ Âåó È´òÈõÑ ÂÑ™Ë≥™ ÊñáÊïô ÁâπÂçÄ Ôºå ÂÑ™Ë≥™ Â±Ö‰Ωè Áí∞Â¢É Ôºå Âê∏Âºï ÊäïË≥á‰∫∫ ÈÄ≤Èßê Ôºå Êú¨ Â≠£ ÂÜç Êé®Âá∫ 2 Ê®ô 2 Á≠Ü ÂúüÂú∞ Ôºå ÂÖ∂‰∏≠ 1 Á≠Ü Á¨¨‰∏â Á®Æ ÂïÜÊ•≠ÂçÄ ÂúüÂú∞ Ôºå Èù¢Á©ç Á¥Ñ 639 Âù™ Ôºå ‰ΩçÊñº Â§ßÂ≠∏ 26Ë°ó Ôºå Ëøë È´òÈõÑ Â§ßÂ≠∏ Ê≠£ÈñÄ Âèä Ëê¨ Âù™ ËóçÁî∞ ÂÖ¨Âúí Ôºå Âú∞ÂΩ¢ ÊñπÊ≠£ Ôºå ‰ΩøÁî® Âº∑Â∫¶ È´ò Ôºå ÈÅ©Âêà ËààÂª∫ ÂÑ™Ë≥™ ‰ΩèÂÆÖ Â§ßÊ®ì Ôºõ Âè¶ 1 Á≠Ü ‰Ωè‰∏â Áî®Âú∞ Ôºå Èù¢Á©ç Á¥Ñ 379 Âù™ Ôºå Ëá® 28 Á±≥ ËóçÊòåË∑Ø Ôºå Ëøë È´òÈõÑ Â§ßÂ≠∏ Âèä ‰∏≠Â±± È´ò‰∏≠ Ôºå ‰∫§ÈÄö ‰æøÊç∑ „ÄÇ n n n n Âè¶ Á¨¨37 Êúü ÈáçÂäÉÂçÄ Âèä Ââç Â§ßÂØÆ Ëæ≤Âú∞ ÈáçÂäÉÂçÄ ÂêÑ Êé®Âá∫ 1 Ëá≥ 2 Á≠Ü ÂúüÂú∞ Ôºå ÂÉπÊ†º ÂêàÁêÜ „ÄÇ n n n n Á¨¨4 Â≠£ ÂúüÂú∞ Ê®ôÂîÆ ‰ΩúÊ•≠ Êñº 109Âπ¥ 12Êúà 1Êó• ÂÖ¨Âëä Ôºå ÊäïË≥á Â§ßÁúæ ÂèØ ÂâçÂæÄ Âú∞ÊîøÂ±Ä ÂúüÂú∞ ÈñãÁôºËôï ÂúüÂú∞Ëôï ÂàÜÁßë Á¥¢Âèñ Ê®ôÂîÆ Êµ∑Â†± Âèä Ê®ôÂñÆ Ôºå Êàñ Áõ¥Êé• ‰∏äÁ∂≤ È´òÈõÑ ÊàøÂú∞Áî¢ ÂÑÑÂπ¥Êó∫ Á∂≤Á´ô „ÄÅ Âú∞ÊîøÂ±Ä Âèä ÂúüÂú∞ ÈñãÁôºËôï Á∂≤Á´ô Êü•Ë©¢ ‰∏ãËºâ Áõ∏Èóú Ë≥áÊñô Ôºå Âú® ÊúüÈôê Ââç ÂÆåÊàê ÊäïÊ®ô Ôºå Âè¶ ÂÜç ÊèêÈÜí ÊäïÊ®ô‰∫∫ Ôºå Êú¨ Âπ¥Â∫¶ Â∑≤ Êõ¥Êñ∞ ÊäïÊ®ôÂñÆ Ê†ºÂºè Ôºå ÊäïÊ®ô Â§ßÁúæ Ë´ã Ê≥®ÊÑè Êáâ ‰ª• Êñ∞Âºè ÊäïÊ®ôÂñÆ ÊäïÊ®ô ‰ª•ÂÖç ÊäïÊ®ô ÁÑ°Êïà ‰ΩúÂª¢ „ÄÇ n n n n ÁÇ∫ ÈÖçÂêà Èò≤Áñ´ ÈúÄÊ±Ç Ôºå Êú¨ Â≠£ ÈñãÊ®ô ‰ΩúÊ•≠ Èô§ Êñº Âú∞ÊîøÂ±Ä Á¨¨‰∏Ä ÊúÉË≠∞ÂÆ§ Ëæ¶ÁêÜ Â§ñ Ôºå Âè¶ Â∞á Êñº Âú∞ÊîøÂ±Ä Facebook Á≤âÁµ≤ Â∞àÈ†Å ÂêåÊ≠• Áõ¥Êí≠ Ôºå Ë´ã Â§ßÁúæ Â§öÂä† Âà©Áî® „ÄÇ n n n n Ê¥ΩË©¢ Â∞àÁ∑ö Ôºö 3373 451 Êàñ 3314942 n n È´òÈõÑ ÊàøÂú∞Áî¢ ÂÑÑ Âπ¥ Êó∫ Á∂≤Á´ô Ôºà Á∂≤ÂùÄ Ôºö Ôºâ n n È´òÈõÑÂ∏Ç ÊîøÂ∫ú Âú∞ÊîøÂ±Ä Á∂≤Á´ô Ôºà Á∂≤ÂùÄ Ôºö Ôºâ n n È´òÈõÑÂ∏Ç ÊîøÂ∫ú Âú∞ÊîøÂ±Ä ÂúüÂú∞ ÈñãÁôºËôï Á∂≤Á´ô Ôºà Á∂≤ÂùÄ Ôºö Ôºâ&#39; . Next, we feed token_str, our tokenized text, to nlp to create a spaCy Doc object. From this point on, we are able to leverage the power of spaCy. For every token in a Doc object, we have access to its text via the attribute .text and its parts-of-speech label via the attribute .pos_. . doc = nlp(token_str) print([token.text for token in doc]) print([token.pos_ for token in doc]) . [&#39;Â∏ÇÂ∫ú&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;109Âπ¥Â∫¶&#39;, &#39;Á¨¨4&#39;, &#39;Â≠£&#39;, &#39;ÈñãÁôºÂçÄ&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Ôºå&#39;, &#39;ÂÖ±Ë®à&#39;, &#39;Êé®Âá∫&#39;, &#39;8&#39;, &#39;Ê®ô&#39;, &#39;9&#39;, &#39;Á≠Ü&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Âª∫Âú∞&#39;, &#39;Ôºå&#39;, &#39;Ë®Ç&#39;, &#39;Êñº&#39;, &#39;109Âπ¥&#39;, &#39;12Êúà&#39;, &#39;16Êó•&#39;, &#39;ÈñãÊ®ô&#39;, &#39;Ôºå&#39;, &#39;ÂêàË®à&#39;, &#39;Á∏ΩÂ∫ïÂÉπ&#39;, &#39;12&#39;, &#39;ÂÑÑ&#39;, &#39;4049Ëê¨&#39;, &#39;6164&#39;, &#39;ÂÖÉ&#39;, &#39;„ÄÇ&#39;, &#39;Á¨¨93&#39;, &#39;Êúü&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;Âéü&#39;, &#39;ÁÇ∫&#39;, &#39;ÂúãËªç&#39;, &#39;Áú∑Êùë&#39;, &#39;Ôºå&#39;, &#39;Á∑äÈÑ∞&#39;, &#39;ÂúãÂÆö&#39;, &#39;Âè§Ëπü&#39;, &#39;-&#39;, &#39;„Äå&#39;, &#39;Âéü&#39;, &#39;Êó•Êú¨&#39;, &#39;Êµ∑Ëªç&#39;, &#39;È≥≥Â±±&#39;, &#39;ÁÑ°Á∑ö&#39;, &#39;Èõª‰ø°ÊâÄ&#39;, &#39;„Äç&#39;, &#39;Ôºå&#39;, &#39;Â∏ÇÂ∫ú&#39;, &#39;ÁÇ∫&#39;, &#39;‰øùÂ≠ò&#39;, &#39;Âè§Ëπü&#39;, &#39;ÂêåÊôÇ&#39;, &#39;Ê¥ªÂåñ&#39;, &#39;Áú∑Êùë&#39;, &#39;ÈÅ∑Áßª&#39;, &#39;Âæå&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;‰ª•&#39;, &#39;ÈáçÂäÉ&#39;, &#39;ÊñπÂºè&#39;, &#39;Êï¥È´î&#39;, &#39;ÈñãÁôº&#39;, &#39;Ôºå&#39;, &#39;Êñ∞&#39;, &#39;Èó¢&#39;, &#39;‰ΩèÂÆÖÂçÄ&#39;, &#39;„ÄÅ&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;ÂÅúËªäÂ†¥&#39;, &#39;Ôºå&#39;, &#39;‰Ωø&#39;, &#39;Êú¨&#39;, &#39;ÂçÄ&#39;, &#39;ÂÖ∑Êúâ&#39;, &#39;Ê≠∑Âè≤&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÂÖßÊ∂µ&#39;, &#39;Ëàá&#39;, &#39;Á∂†Ëâ≤&#39;, &#39;‰ºëÈñí&#39;, &#39;ÁâπËâ≤&#39;, &#39;Ôºå&#39;, &#39;ÁîüÊ¥ª&#39;, &#39;Ê©üËÉΩ&#39;, &#39;Êõ¥Âä†&#39;, &#39;ÂÅ•ÂÖ®&#39;, &#39;„ÄÇ&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;È¶ñÊ¨°&#39;, &#39;Êé®Âá∫&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Â§ß&#39;, &#39;Èù¢Á©ç&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;2160&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Âú∞ÂΩ¢&#39;, &#39;ÊñπÊï¥&#39;, &#39;Ôºå&#39;, &#39;ÈõôÈù¢&#39;, &#39;Ëá®&#39;, &#39;Ë∑Ø&#39;, &#39;Ôºå&#39;, &#39;Âà©Êñº&#39;, &#39;Ë¶èÂäÉ&#39;, &#39;ËààÂª∫&#39;, &#39;ÊôØËßÄ&#39;, &#39;Â§ßÊ®ì&#39;, &#39;Ôºå&#39;, &#39;ÈôÑËøë&#39;, &#39;Êúâ&#39;, &#39;Â∏ÇÂ†¥&#39;, &#39;„ÄÅ&#39;, &#39;Â≠∏Ê†°&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;Â§ßÊù±&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÂúíÂçÄ&#39;, &#39;Ôºå&#39;, &#39;Ë∑ù&#39;, &#39;Êç∑ÈÅã&#39;, &#39;Â§ßÊù±Á´ô&#39;, &#39;„ÄÅ&#39;, &#39;È≥≥Â±±&#39;, &#39;Âúã‰∏≠Á´ô&#39;, &#39;Âèä&#39;, &#39;È≥≥Â±±&#39;, &#39;ÁÅ´ËªäÁ´ô&#39;, &#39;ÂÉÖ&#39;, &#39;Êï∏&#39;, &#39;ÂàÜÈêò&#39;, &#39;ËªäÁ®ã&#39;, &#39;Ôºå&#39;, &#39;‰∫§ÈÄö&#39;, &#39;ÂõõÈÄöÂÖ´ÈÅî&#39;, &#39;Ôºå&#39;, &#39;Âõ†&#39;, &#39;ÂúüÂú∞&#39;, &#39;Á®ÄÂ∞ëÊÄß&#39;, &#39;Âèä&#39;, &#39;ÂçÄ‰Ωç&#39;, &#39;Ê¢ù‰ª∂&#39;, &#39;Áµï‰Ω≥&#39;, &#39;Ôºå&#39;, &#39;Âã¢ÂøÖ&#39;, &#39;ÊàêÁÇ∫&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;ËøΩÈÄê&#39;, &#39;ÁÑ¶Èªû&#39;, &#39;„ÄÇ&#39;, &#39;Á¨¨87&#39;, &#39;Êúü&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;‰ΩçÊñº&#39;, &#39;ÁúÅÈÅì&#39;, &#39;Âè∞1Á∑ö&#39;, &#39;ÊóÅ&#39;, &#39;Ôºå&#39;, &#39;ÈÑ∞Ëøë&#39;, &#39;Êç∑ÈÅã&#39;, &#39;Âçó&#39;, &#39;Â≤°Â±±Á´ô&#39;, &#39;Ôºå&#39;, &#39;ÈáçÂäÉ&#39;, &#39;Âæå&#39;, &#39;ÊìÅÊúâ&#39;, &#39;ÂÆåÂñÑ&#39;, &#39;ÁöÑ&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;Á≥ªÁµ±&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Á∂†Âú∞&#39;, &#39;Âèä&#39;, &#39;ÊØóÈÑ∞&#39;, &#39;ÈÜíÊùë&#39;, &#39;Êá∑Ëàä&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÊôØËßÄ&#39;, &#39;Âª∫ÁØâÁæ§&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∑ÂÇô&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Â±Ö‰Ωè&#39;, &#39;Áí∞Â¢É&#39;, &#39;Âèä&#39;, &#39;‰∫§ÈÄö&#39;, &#39;‰æøÊç∑&#39;, &#39;Ë¶Å‰ª∂&#39;, &#39;Ôºå&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;‰∏Ä&#39;, &#39;Êé®Âá∫&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Ôºå&#39;, &#39;Âç≥&#39;, &#39;ÊéÄËµ∑&#39;, &#39;Êê∂Ê®ô&#39;, &#39;ÁÜ±ÊΩÆ&#39;, &#39;Ôºå&#39;, &#39;Êú¨&#39;, &#39;Â≠£&#39;, &#39;ÂÜç&#39;, &#39;ÈáãÂá∫&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;93&#39;, &#39;Âù™&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Ëá®&#39;, &#39;20&#39;, &#39;Á±≥&#39;, &#39;‰ªãÂ£ΩË∑Ø&#39;, &#39;Âèä&#39;, &#39;Èµ¨Á®ãÊù±Ë∑Ø&#39;, &#39;Ôºå&#39;, &#39;ÈôÑËøë&#39;, &#39;Êúâ&#39;, &#39;Â≤°Â±±&#39;, &#39;ÊñáÂåñ&#39;, &#39;‰∏≠ÂøÉ&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖÜÊπò&#39;, &#39;ÂúãÂ∞è&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨13&#39;, &#39;„ÄÅ&#39;, &#39;ÂÖ¨14&#39;, &#39;„ÄÅ&#39;, &#39;ÈôΩÊòé&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âèä&#39;, &#39;ÂäâÂéù&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Ôºå&#39;, &#39;ÂçÄ‰Ωç&#39;, &#39;Ê¢ù‰ª∂&#39;, &#39;‰Ω≥&#39;, &#39;Ôºå&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;Ê∫ñÂÇô&#39;, &#39;Êê∂ÈÄ≤&#39;, &#39;ÔºÅ&#39;, &#39;Á¨¨77&#39;, &#39;Êúü&#39;, &#39;Â∏ÇÂú∞&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Ôºå&#39;, &#39;‰ΩçÊñº&#39;, &#39;È≥≥Â±±ÂçÄ&#39;, &#39;Âø´ÈÄü&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;ÁúÅÈÅì&#39;, &#39;Âè∞88&#39;, &#39;Á∑ö&#39;, &#39;ÊóÅ&#39;, &#39;Ôºå&#39;, &#39;Ëøë&#39;, &#39;‰∏≠Â±±È´ò&#39;, &#39;‰∫îÁî≤&#39;, &#39;Á≥ªÁµ±&#39;, &#39;‰∫§ÊµÅÈÅì&#39;, &#39;Ôºå&#39;, &#39;ËøëÂπ¥&#39;, &#39;Êé®Âá∫&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;ÁöÜ&#39;, &#39;È†ÜÂà©&#39;, &#39;ÂÆåÈä∑&#39;, &#39;„ÄÇ&#39;, &#39;Êú¨&#39;, &#39;Â≠£&#39;, &#39;ÂÜç&#39;, &#39;Êé®Âá∫&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∂‰∏≠&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;526&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Ëá®&#39;, &#39;‰øùËèØ‰∏ÄË∑Ø&#39;, &#39;Ôºå&#39;, &#39;ÈÅ©Âêà&#39;, &#39;ÂïÜÊ•≠&#39;, &#39;‰ΩøÁî®&#39;, &#39;Ôºõ&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Èù¢Á©ç&#39;, &#39;107&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;‰ΩçÊñº&#39;, &#39;‰ª£Âæ∑‰∏âË°ó&#39;, &#39;Ôºå&#39;, &#39;Ëá™Áî®&#39;, &#39;ÊäïË≥á&#39;, &#39;ÂÖ©&#39;, &#39;Áõ∏ÂÆú&#39;, &#39;„ÄÇ&#39;, &#39;È´òÈõÑ&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;ÂçÄÊÆµ&#39;, &#39;ÂæµÊî∂ÂçÄ&#39;, &#39;Ôºå&#39;, &#39;ÁÇ∫&#39;, &#39;Âåó&#39;, &#39;È´òÈõÑ&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;ÊñáÊïô&#39;, &#39;ÁâπÂçÄ&#39;, &#39;Ôºå&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Â±Ö‰Ωè&#39;, &#39;Áí∞Â¢É&#39;, &#39;Ôºå&#39;, &#39;Âê∏Âºï&#39;, &#39;ÊäïË≥á‰∫∫&#39;, &#39;ÈÄ≤Èßê&#39;, &#39;Ôºå&#39;, &#39;Êú¨&#39;, &#39;Â≠£&#39;, &#39;ÂÜç&#39;, &#39;Êé®Âá∫&#39;, &#39;2&#39;, &#39;Ê®ô&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÖ∂‰∏≠&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;Á¨¨‰∏â&#39;, &#39;Á®Æ&#39;, &#39;ÂïÜÊ•≠ÂçÄ&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;639&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;‰ΩçÊñº&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;26Ë°ó&#39;, &#39;Ôºå&#39;, &#39;Ëøë&#39;, &#39;È´òÈõÑ&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;Ê≠£ÈñÄ&#39;, &#39;Âèä&#39;, &#39;Ëê¨&#39;, &#39;Âù™&#39;, &#39;ËóçÁî∞&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Ôºå&#39;, &#39;Âú∞ÂΩ¢&#39;, &#39;ÊñπÊ≠£&#39;, &#39;Ôºå&#39;, &#39;‰ΩøÁî®&#39;, &#39;Âº∑Â∫¶&#39;, &#39;È´ò&#39;, &#39;Ôºå&#39;, &#39;ÈÅ©Âêà&#39;, &#39;ËààÂª∫&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;‰ΩèÂÆÖ&#39;, &#39;Â§ßÊ®ì&#39;, &#39;Ôºõ&#39;, &#39;Âè¶&#39;, &#39;1&#39;, &#39;Á≠Ü&#39;, &#39;‰Ωè‰∏â&#39;, &#39;Áî®Âú∞&#39;, &#39;Ôºå&#39;, &#39;Èù¢Á©ç&#39;, &#39;Á¥Ñ&#39;, &#39;379&#39;, &#39;Âù™&#39;, &#39;Ôºå&#39;, &#39;Ëá®&#39;, &#39;28&#39;, &#39;Á±≥&#39;, &#39;ËóçÊòåË∑Ø&#39;, &#39;Ôºå&#39;, &#39;Ëøë&#39;, &#39;È´òÈõÑ&#39;, &#39;Â§ßÂ≠∏&#39;, &#39;Âèä&#39;, &#39;‰∏≠Â±±&#39;, &#39;È´ò‰∏≠&#39;, &#39;Ôºå&#39;, &#39;‰∫§ÈÄö&#39;, &#39;‰æøÊç∑&#39;, &#39;„ÄÇ&#39;, &#39;Âè¶&#39;, &#39;Á¨¨37&#39;, &#39;Êúü&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;Âèä&#39;, &#39;Ââç&#39;, &#39;Â§ßÂØÆ&#39;, &#39;Ëæ≤Âú∞&#39;, &#39;ÈáçÂäÉÂçÄ&#39;, &#39;ÂêÑ&#39;, &#39;Êé®Âá∫&#39;, &#39;1&#39;, &#39;Ëá≥&#39;, &#39;2&#39;, &#39;Á≠Ü&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ôºå&#39;, &#39;ÂÉπÊ†º&#39;, &#39;ÂêàÁêÜ&#39;, &#39;„ÄÇ&#39;, &#39;Á¨¨4&#39;, &#39;Â≠£&#39;, &#39;ÂúüÂú∞&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;‰ΩúÊ•≠&#39;, &#39;Êñº&#39;, &#39;109Âπ¥&#39;, &#39;12Êúà&#39;, &#39;1Êó•&#39;, &#39;ÂÖ¨Âëä&#39;, &#39;Ôºå&#39;, &#39;ÊäïË≥á&#39;, &#39;Â§ßÁúæ&#39;, &#39;ÂèØ&#39;, &#39;ÂâçÂæÄ&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;ÂúüÂú∞Ëôï&#39;, &#39;ÂàÜÁßë&#39;, &#39;Á¥¢Âèñ&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;Êµ∑Â†±&#39;, &#39;Âèä&#39;, &#39;Ê®ôÂñÆ&#39;, &#39;Ôºå&#39;, &#39;Êàñ&#39;, &#39;Áõ¥Êé•&#39;, &#39;‰∏äÁ∂≤&#39;, &#39;È´òÈõÑ&#39;, &#39;ÊàøÂú∞Áî¢&#39;, &#39;ÂÑÑÂπ¥Êó∫&#39;, &#39;Á∂≤Á´ô&#39;, &#39;„ÄÅ&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;Âèä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Êü•Ë©¢&#39;, &#39;‰∏ãËºâ&#39;, &#39;Áõ∏Èóú&#39;, &#39;Ë≥áÊñô&#39;, &#39;Ôºå&#39;, &#39;Âú®&#39;, &#39;ÊúüÈôê&#39;, &#39;Ââç&#39;, &#39;ÂÆåÊàê&#39;, &#39;ÊäïÊ®ô&#39;, &#39;Ôºå&#39;, &#39;Âè¶&#39;, &#39;ÂÜç&#39;, &#39;ÊèêÈÜí&#39;, &#39;ÊäïÊ®ô‰∫∫&#39;, &#39;Ôºå&#39;, &#39;Êú¨&#39;, &#39;Âπ¥Â∫¶&#39;, &#39;Â∑≤&#39;, &#39;Êõ¥Êñ∞&#39;, &#39;ÊäïÊ®ôÂñÆ&#39;, &#39;Ê†ºÂºè&#39;, &#39;Ôºå&#39;, &#39;ÊäïÊ®ô&#39;, &#39;Â§ßÁúæ&#39;, &#39;Ë´ã&#39;, &#39;Ê≥®ÊÑè&#39;, &#39;Êáâ&#39;, &#39;‰ª•&#39;, &#39;Êñ∞Âºè&#39;, &#39;ÊäïÊ®ôÂñÆ&#39;, &#39;ÊäïÊ®ô&#39;, &#39;‰ª•ÂÖç&#39;, &#39;ÊäïÊ®ô&#39;, &#39;ÁÑ°Êïà&#39;, &#39;‰ΩúÂª¢&#39;, &#39;„ÄÇ&#39;, &#39;ÁÇ∫&#39;, &#39;ÈÖçÂêà&#39;, &#39;Èò≤Áñ´&#39;, &#39;ÈúÄÊ±Ç&#39;, &#39;Ôºå&#39;, &#39;Êú¨&#39;, &#39;Â≠£&#39;, &#39;ÈñãÊ®ô&#39;, &#39;‰ΩúÊ•≠&#39;, &#39;Èô§&#39;, &#39;Êñº&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;Á¨¨‰∏Ä&#39;, &#39;ÊúÉË≠∞ÂÆ§&#39;, &#39;Ëæ¶ÁêÜ&#39;, &#39;Â§ñ&#39;, &#39;Ôºå&#39;, &#39;Âè¶&#39;, &#39;Â∞á&#39;, &#39;Êñº&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;Facebook&#39;, &#39;Á≤âÁµ≤&#39;, &#39;Â∞àÈ†Å&#39;, &#39;ÂêåÊ≠•&#39;, &#39;Áõ¥Êí≠&#39;, &#39;Ôºå&#39;, &#39;Ë´ã&#39;, &#39;Â§ßÁúæ&#39;, &#39;Â§öÂä†&#39;, &#39;Âà©Áî®&#39;, &#39;„ÄÇ&#39;, &#39;Ê¥ΩË©¢&#39;, &#39;Â∞àÁ∑ö&#39;, &#39;Ôºö&#39;, &#39;3373&#39;, &#39;451&#39;, &#39;Êàñ&#39;, &#39;3314942&#39;, &#39;È´òÈõÑ&#39;, &#39;ÊàøÂú∞Áî¢&#39;, &#39;ÂÑÑ&#39;, &#39;Âπ¥&#39;, &#39;Êó∫&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39;Ôºâ&#39;, &#39;È´òÈõÑÂ∏Ç&#39;, &#39;ÊîøÂ∫ú&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39;Ôºâ&#39;, &#39;È´òÈõÑÂ∏Ç&#39;, &#39;ÊîøÂ∫ú&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;ÂúüÂú∞&#39;, &#39;ÈñãÁôºËôï&#39;, &#39;Á∂≤Á´ô&#39;, &#39;Ôºà&#39;, &#39;Á∂≤ÂùÄ&#39;, &#39;Ôºö&#39;, &#39;Ôºâ&#39;] [&#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;CCONJ&#39;, &#39;NUM&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;DET&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;ADJ&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;ADJ&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;CCONJ&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;CCONJ&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;ADP&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;ADP&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;ADV&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NUM&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;] . The POS tagging is made possible by the zh_core_web_sm model. Notice that spaCy uses coarse labels such as NOUN and VERB. By contrast, CKIP Transformers adopts a more fine-grained tagset, such as Nc for locative nouns and Nd for temporal nouns. Here&#39;re the POS labels for the same text produced by CKIP Transformers. We&#39;ll be using the spaCy&#39;s POS tagging to filter out words that we don&#39;t want in the candicate pool for keywords. . pos_tags = pos[0] print(pos_tags) . [&#39;Nc&#39;, &#39;Nc&#39;, &#39;Nd&#39;, &#39;Neu&#39;, &#39;Nd&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;P&#39;, &#39;Nd&#39;, &#39;Nd&#39;, &#39;Nd&#39;, &#39;VA&#39;, &#39;COMMACATEGORY&#39;, &#39;VG&#39;, &#39;Na&#39;, &#39;Neu&#39;, &#39;Neu&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;D&#39;, &#39;VG&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;DASHCATEGORY&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;A&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;A&#39;, &#39;Nc&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;COMMACATEGORY&#39;, &#39;Nc&#39;, &#39;P&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Nd&#39;, &#39;VHC&#39;, &#39;Nc&#39;, &#39;VC&#39;, &#39;Ng&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;VC&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Na&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VL&#39;, &#39;Nes&#39;, &#39;Nc&#39;, &#39;VJ&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Dfa&#39;, &#39;VHC&#39;, &#39;PERIODCATEGORY&#39;, &#39;Nc&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;A&#39;, &#39;VCL&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VK&#39;, &#39;VC&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nc&#39;, &#39;V_2&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nb&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;Cbb&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;D&#39;, &#39;VG&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Ncd&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VC&#39;, &#39;Ng&#39;, &#39;VJ&#39;, &#39;VH&#39;, &#39;DE&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;VH&#39;, &#39;Nc&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;A&#39;, &#39;VA&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nc&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Nv&#39;, &#39;COMMACATEGORY&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;VD&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nes&#39;, &#39;Nd&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;Nc&#39;, &#39;V_2&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Na&#39;, &#39;PAUSECATEGORY&#39;, &#39;Na&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VF&#39;, &#39;VA&#39;, &#39;EXCLAMATIONCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Nf&#39;, &#39;Ncd&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nd&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Nv&#39;, &#39;D&#39;, &#39;VH&#39;, &#39;VC&#39;, &#39;PERIODCATEGORY&#39;, &#39;Nes&#39;, &#39;Nd&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nep&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;SEMICOLONCATEGORY&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;Neu&#39;, &#39;VH&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;COMMACATEGORY&#39;, &#39;VG&#39;, &#39;Ncd&#39;, &#39;Nc&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;A&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;Na&#39;, &#39;VCL&#39;, &#39;COMMACATEGORY&#39;, &#39;Nes&#39;, &#39;Nd&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nep&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;VC&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;SEMICOLONCATEGORY&#39;, &#39;Nes&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;VCL&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Cbb&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nes&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Caa&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Neu&#39;, &#39;Nd&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;P&#39;, &#39;Nd&#39;, &#39;Nd&#39;, &#39;Nd&#39;, &#39;VE&#39;, &#39;COMMACATEGORY&#39;, &#39;VC&#39;, &#39;Nh&#39;, &#39;D&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;VD&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Caa&#39;, &#39;VH&#39;, &#39;VA&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;Nc&#39;, &#39;VE&#39;, &#39;VC&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Na&#39;, &#39;Ng&#39;, &#39;VC&#39;, &#39;VA&#39;, &#39;COMMACATEGORY&#39;, &#39;Cbb&#39;, &#39;D&#39;, &#39;VE&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nes&#39;, &#39;Na&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VA&#39;, &#39;Nh&#39;, &#39;VF&#39;, &#39;VK&#39;, &#39;D&#39;, &#39;P&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;VA&#39;, &#39;Cbb&#39;, &#39;VA&#39;, &#39;VH&#39;, &#39;VH&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;P&#39;, &#39;VC&#39;, &#39;VA&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nes&#39;, &#39;Nd&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;P&#39;, &#39;P&#39;, &#39;Nc&#39;, &#39;Neu&#39;, &#39;Nc&#39;, &#39;VC&#39;, &#39;Ng&#39;, &#39;COMMACATEGORY&#39;, &#39;Cbb&#39;, &#39;D&#39;, &#39;P&#39;, &#39;Nc&#39;, &#39;FW&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;D&#39;, &#39;COMMACATEGORY&#39;, &#39;VF&#39;, &#39;Nh&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;VE&#39;, &#39;Na&#39;, &#39;COLONCATEGORY&#39;, &#39;FW&#39;, &#39;Neu&#39;, &#39;Caa&#39;, &#39;FW&#39;, &#39;WHITESPACE&#39;, &#39;WHITESPACE&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nb&#39;, &#39;Nf&#39;, &#39;VH&#39;, &#39;Nc&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;Na&#39;, &#39;COLONCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;WHITESPACE&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;Na&#39;, &#39;COLONCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;WHITESPACE&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;Nc&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;Na&#39;, &#39;COLONCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;PARENTHESISCATEGORY&#39;] . Convert stopwords in spaCy from simplified to Taiwanese traditional . spaCy comes with a built-in set of stopwords (basically words that we&#39;d like to ignore), accessible via spacy.lang.zh.stop_words. To make good use of it, let&#39;s convert all the words from simplified characters to traditional ones with the help of OpenCC. . !pip install OpenCC import opencc . OpenCC does not just convert characters mechanically. It has the ability to convert words from simplified characters to their equivalent phrasing in Taiwan Mandarin, which is done by s2twp.json. . from spacy.lang.zh.stop_words import STOP_WORDS converter = opencc.OpenCC(&#39;s2twp.json&#39;) spacy_stopwords_sim = list(STOP_WORDS) print(spacy_stopwords_sim[:5]) spacy_stopwords_tra = [converter.convert(w) for w in spacy_stopwords_sim] print(spacy_stopwords_tra[:5]) . [&#39;Âõ†‰∏∫&#39;, &#39;Â•á&#39;, &#39;ÂòøÂòø&#39;, &#39;ÂÖ∂Ê¨°&#39;, &#39;ÂÅèÂÅè&#39;] [&#39;Âõ†ÁÇ∫&#39;, &#39;Â•á&#39;, &#39;ÂòøÂòø&#39;, &#39;ÂÖ∂Ê¨°&#39;, &#39;ÂÅèÂÅè&#39;] . Define a class for implementing TextRank . If you&#39;re dealing with English texts, you can implement TextRank quite easily with textaCy, the tagline of which is NLP, before and after spaCy. But I couldn&#39;t get it to work for Chinese texts, so I had to implement TextRank from scratch. Luckily, I got a jump-start from this gist, which offers a blueprint for the following definitions. . from collections import OrderedDict import numpy as np class TextRank4Keyword(): &quot;&quot;&quot;Extract keywords from text&quot;&quot;&quot; def __init__(self): self.d = 0.85 # damping coefficient, usually is .85 self.min_diff = 1e-5 # convergence threshold self.steps = 10 # iteration steps self.node_weight = None # save keywords and its weight def set_stopwords(self, custom_stopwords): &quot;&quot;&quot;Set stop words&quot;&quot;&quot; for word in set(spacy_stopwords_tra).union(set(custom_stopwords)): lexeme = nlp.vocab[word] lexeme.is_stop = True def sentence_segment(self, doc, candidate_pos, lower): &quot;&quot;&quot;Store those words only in cadidate_pos&quot;&quot;&quot; sentences = [] for sent in doc.sents: selected_words = [] for token in sent: # Store words only with cadidate POS tag if token.pos_ in candidate_pos and token.is_stop is False: if lower is True: selected_words.append(token.text.lower()) else: selected_words.append(token.text) sentences.append(selected_words) return sentences def get_vocab(self, sentences): &quot;&quot;&quot;Get all tokens&quot;&quot;&quot; vocab = OrderedDict() i = 0 for sentence in sentences: for word in sentence: if word not in vocab: vocab[word] = i i += 1 return vocab def get_token_pairs(self, window_size, sentences): &quot;&quot;&quot;Build token_pairs from windows in sentences&quot;&quot;&quot; token_pairs = list() for sentence in sentences: for i, word in enumerate(sentence): for j in range(i+1, i+window_size): if j &gt;= len(sentence): break pair = (word, sentence[j]) if pair not in token_pairs: token_pairs.append(pair) return token_pairs def symmetrize(self, a): return a + a.T - np.diag(a.diagonal()) def get_matrix(self, vocab, token_pairs): &quot;&quot;&quot;Get normalized matrix&quot;&quot;&quot; # Build matrix vocab_size = len(vocab) g = np.zeros((vocab_size, vocab_size), dtype=&#39;float&#39;) for word1, word2 in token_pairs: i, j = vocab[word1], vocab[word2] g[i][j] = 1 # Get Symmeric matrix g = self.symmetrize(g) # Normalize matrix by column norm = np.sum(g, axis=0) g_norm = np.divide(g, norm, where=norm!=0) # this is to ignore the 0 element in norm return g_norm # I revised this function to return keywords as a list def get_keywords(self, number=10): &quot;&quot;&quot;Print top number keywords&quot;&quot;&quot; node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True)) keywords = [] for i, (key, value) in enumerate(node_weight.items()): keywords.append(key) if i &gt; number: break return keywords def analyze(self, text, candidate_pos=[&#39;NOUN&#39;, &#39;VERB&#39;], window_size=5, lower=False, stopwords=list()): &quot;&quot;&quot;Main function to analyze text&quot;&quot;&quot; # Set stop words self.set_stopwords(stopwords) # Pare text with spaCy doc = nlp(token_str) # Filter sentences sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words # Build vocabulary vocab = self.get_vocab(sentences) # Get token_pairs from windows token_pairs = self.get_token_pairs(window_size, sentences) # Get normalized matrix g = self.get_matrix(vocab, token_pairs) # Initionlization for weight(pagerank value) pr = np.array([1] * len(vocab)) # Iteration previous_pr = 0 for epoch in range(self.steps): pr = (1-self.d) + self.d * np.dot(g, pr) if abs(previous_pr - sum(pr)) &lt; self.min_diff: break else: previous_pr = sum(pr) # Get weight for each node node_weight = dict() for word, index in vocab.items(): node_weight[word] = pr[index] self.node_weight = node_weight . Now we can create an instace of the TextRank4Keyword class and call the set_stopwords function with our CUSTOM_STOPWORDS variable. This created a set of stopwords resulting from the union of both our custom stopwords and spaCy&#39;s built-in stopwords. And only words that meet these two criteria would become candidates for keywords: . they are not in the set of stopwords; | their POS labels are one of those listed in candidate_pos, which includes NOUN and VERB by default. | . tr4w = TextRank4Keyword() tr4w.set_stopwords(CUSTOM_STOPWORDS) . Put it together . Let&#39;s put it all together by defining a main function for keyword extraction. . def extract_keys_from_str(raw_text): text = clean_all(raw_text) #clean the raw text ws = ws_driver([text]) #tokenize the text with CKIP Transformers tokenized_text = &quot; &quot;.join(ws[0]) #join a list into a string tr4w.analyze(tokenized_text) #create a spaCy Doc object with the string and calculate weights for words keys = tr4w.get_keywords(KW_NUM) #get top 10 keywords, as set by the KW_NUM variable return keys . Here&#39;re the top ten keywords for our sample text. The results are quite satisfactory. . keys = extract_keys_from_str(raw_text) keys = [k for k in keys if len(k) &gt; 1] keys . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 221.73it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05&lt;00:00, 5.20s/it] . [&#39;ÂúüÂú∞&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;Âú∞ÊîøÂ±Ä&#39;, &#39;ÊñáÂåñ&#39;, &#39;Êé®Âá∫&#39;, &#39;Èù¢Á©ç&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;ÈÅìË∑Ø&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;ÊäïÊ®ô&#39;] . As a comparison, here&#39;re the top 10 keywords produced by Jieba&#39;s implementation of TextRank, 7 of which are identical to the list above. Although extracting keywords with Jieba is quick and easy, it tends to give rise to wrongly segmented tokens, such as ÊîøÂ±Ä in this example, which should have been Âú∞ÊîøÂ±Ä for Land Administration Bureau. . import jieba.analyse as KE jieba_kw = KE.textrank(text, topK=10) jieba_kw . [&#39;ÂúüÂú∞&#39;, &#39;ÊîøÂ±Ä&#39;, &#39;ÊäïÊ®ô&#39;, &#39;ÂÖ¨Âúí&#39;, &#39;ÊäïË≥á&#39;, &#39;Ê®ôÂîÆ&#39;, &#39;ÊñáÂåñ&#39;, &#39;ÈñãÁôº&#39;, &#39;ÂÑ™Ë≥™&#39;, &#39;Êé®Âá∫&#39;] . Other libraries that failed . textaCy . !pip install textacy . With textaCy, you can load a spaCy language model and then create a spaCy Doc object using that model. . import textacy zh = textacy.load_spacy_lang(&quot;zh_core_web_sm&quot;) doc = textacy.make_spacy_doc(text, lang=zh) doc._.preview . &#39;Doc(612 tokens: &#34;Â∏ÇÂ∫úÂú∞ÊîøÂ±Ä109Âπ¥Â∫¶Á¨¨4Â≠£ÈñãÁôºÂçÄÂúüÂú∞Ê®ôÂîÆÔºåÂÖ±Ë®àÊé®Âá∫8Ê®ô9Á≠ÜÂÑ™Ë≥™Âª∫Âú∞ÔºåË®ÇÊñº109Âπ¥12Êúà16Êó•Èñã...&#34;)&#39; . textaCy implements four algorithms for keyword extraction, including TextRank. But I got useless results by calling the textacy.ke.textrank function with doc. . import textacy.ke as ke ke.textrank(doc) . [(&#39; &#39;, 6.0)] . pyate . !pip install pyate . pyate has a built-in TermExtractionPipeline class for extracting keywords, which can be added to spaCy&#39;s pipeline. But it didn&#39;t work and this error message showed up: TypeError: load() got an unexpected keyword argument &#39;parser&#39;. . from pyate.term_extraction_pipeline import TermExtractionPipeline nlp.add_pipe(TermExtractionPipeline()) . TypeError Traceback (most recent call last) &lt;ipython-input-27-f5a8398fbc3b&gt; in &lt;module&gt;() 1 #collapse-output -&gt; 2 from pyate.term_extraction_pipeline import TermExtractionPipeline 3 nlp.add_pipe(TermExtractionPipeline()) /usr/local/lib/python3.6/dist-packages/pyate/__init__.py in &lt;module&gt;() -&gt; 1 from .term_extraction import TermExtraction, add_term_extraction_method 2 from .basic import basic 3 from .combo_basic import combo_basic 4 from .cvalues import cvalues 5 from .term_extractor import term_extractor /usr/local/lib/python3.6/dist-packages/pyate/term_extraction.py in &lt;module&gt;() 20 21 &gt; 22 class TermExtraction: 23 # TODO: find some way to prevent redundant loading of csv files 24 nlp = spacy.load(&#34;en_core_web_sm&#34;, parser=False, entity=False) /usr/local/lib/python3.6/dist-packages/pyate/term_extraction.py in TermExtraction() 22 class TermExtraction: 23 # TODO: find some way to prevent redundant loading of csv files &gt; 24 nlp = spacy.load(&#34;en_core_web_sm&#34;, parser=False, entity=False) 25 matcher = Matcher(nlp.vocab) 26 language = &#34;en&#34; TypeError: load() got an unexpected keyword argument &#39;parser&#39; . I found on the documentation page that pyate only supports English and Italian, which may account for the error I got. . pytextrank . !pip install pytextrank . To add TextRank to the spaCy pipeline, I followed the instructions found on spaCy&#39;s documentation. But an error popped up. Luckily, ValueError offers possible ways to fix the problem. . import pytextrank tr = pytextrank.TextRank() nlp.add_pipe(tr.PipelineComponent, name=&#39;textrank&#39;, last=True) . ValueError Traceback (most recent call last) &lt;ipython-input-29-cd319957f3b6&gt; in &lt;module&gt;() 2 import pytextrank 3 tr = pytextrank.TextRank() -&gt; 4 nlp.add_pipe(tr.PipelineComponent, name=&#39;textrank&#39;, last=True) /usr/local/lib/python3.6/dist-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate) 746 batch_size=1000, 747 disable=[], --&gt; 748 cleanup=False, 749 component_cfg=None, 750 n_process=1, ValueError: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got &lt;bound method TextRank.PipelineComponent of &lt;pytextrank.pytextrank.TextRank object at 0x7f4fea403550&gt;&gt; (name: &#39;textrank&#39;). - If you created your component with `nlp.create_pipe(&#39;name&#39;)`: remove nlp.create_pipe and call `nlp.add_pipe(&#39;name&#39;)` instead. - If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe(&#39;textcat&#39;)`. - If you&#39;re using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component(&#39;your_name&#39;)`. You can then run `nlp.add_pipe(&#39;your_name&#39;)` to add it to the pipeline. . So I used the @Language.factory decorator to define a TextRank component, and then called the nlp.add_pipe function with textrank. But this didn&#39;t work either. The error message reads: &#39;Chinese&#39; object has no attribute &#39;sents&#39;. . from spacy.language import Language tr = pytextrank.TextRank() @Language.factory(&quot;textrank&quot;) def create_textrank_component(nlp: Language, name: str): return tr.PipelineComponent(nlp) . nlp.add_pipe(&#39;textrank&#39;) . AttributeError Traceback (most recent call last) &lt;ipython-input-31-1c84bbe50472&gt; in &lt;module&gt;() 1 #collapse-output -&gt; 2 nlp.add_pipe(&#39;textrank&#39;) /usr/local/lib/python3.6/dist-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate) 770 if is_python2 and n_process != 1: 771 warnings.warn(Warnings.W023) --&gt; 772 n_process = 1 773 if n_threads != -1: 774 warnings.warn(Warnings.W016, DeprecationWarning) /usr/local/lib/python3.6/dist-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate) 656 link_vectors_to_models(self.vocab) 657 if self.vocab.vectors.data.shape[1]: --&gt; 658 cfg[&#34;pretrained_vectors&#34;] = self.vocab.vectors.name 659 if sgd is None: 660 sgd = create_default_optimizer(Model.ops) /usr/local/lib/python3.6/dist-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate) /usr/local/lib/python3.6/dist-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate) /usr/local/lib/python3.6/dist-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides) &lt;ipython-input-30-fb02aff6bab9&gt; in create_textrank_component(nlp, name) 5 @Language.factory(&#34;textrank&#34;) 6 def create_textrank_component(nlp: Language, name: str): -&gt; 7 return tr.PipelineComponent(nlp) /usr/local/lib/python3.6/dist-packages/pytextrank/pytextrank.py in PipelineComponent(self, doc) 559 Doc.set_extension(&#34;phrases&#34;, force=True, default=[]) 560 Doc.set_extension(&#34;textrank&#34;, force=True, default=self) --&gt; 561 doc._.phrases = self.calc_textrank() 562 563 return doc /usr/local/lib/python3.6/dist-packages/pytextrank/pytextrank.py in calc_textrank(self) 375 t0 = time.time() 376 --&gt; 377 for sent in self.doc.sents: 378 self.link_sentence(sent) 379 AttributeError: &#39;Chinese&#39; object has no attribute &#39;sents&#39; . rake-spacy . I couldn&#39;t even install rake-spacy. . !pip install rake-spacy . ERROR: Could not find a version that satisfies the requirement rake-spacy ERROR: No matching distribution found for rake-spacy . rake-keyword . !pip install rake-keyword . According to the documentation on PYPI, the import is done by from rake import Rake, but it didn&#39;t work. . from rake import Rake . ImportError Traceback (most recent call last) &lt;ipython-input-34-fe043f886018&gt; in &lt;module&gt;() 1 #collapse-output -&gt; 2 from rake import Rake ImportError: cannot import name &#39;Rake&#39; NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the &#34;Open Examples&#34; button below. . However, based on the documentation on GitHub, this is done by from rake import RAKE instead. But it didn&#39;t work either. . from rake import RAKE . ImportError Traceback (most recent call last) &lt;ipython-input-35-59e63adf01ef&gt; in &lt;module&gt;() 1 #collapse-output -&gt; 2 from rake import RAKE ImportError: cannot import name &#39;RAKE&#39; NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the &#34;Open Examples&#34; button below. . . Recap . Integration of CKIP Transformers with spaCy and the TextRank algorithm generates decent results for extracting keywords from texts in traditional Chinese. Although there are many Python libraries out there that implement TextRank, none of them works better than the TextRank4Keyword class crafted from scratch. Until I figure out how to properly add the TextRank component to the spaCy pipeline, I&#39;ll stick with my working pipeline shown here. As a final thought, spaCy recently released v3.0, which supports pretrained transformer models. I can&#39;t wait to give it a try and see how this would change the workflow of extracting keywords or other NLP tasks. But that&#39;ll have to wait until next post. .",
            "url": "https://howard-haowen.github.io/blog.ai/keyword-extraction/spacy/textacy/ckip-transformers/jieba/textrank/rake/2021/02/16/Adding-a-custom-tokenizer-to-spaCy-and-extracting-keywords.html",
            "relUrl": "/keyword-extraction/spacy/textacy/ckip-transformers/jieba/textrank/rake/2021/02/16/Adding-a-custom-tokenizer-to-spaCy-and-extracting-keywords.html",
            "date": " ‚Ä¢ Feb 16, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Many ways to segment Chinese",
            "content": ". Intro . Unlike English, Chinese does not use spaces in its writing system, which can be a pain in the neck (or in the eyes, for that matter) if you&#39;re learning to read Chinese. In a way, it&#39;s like trying to make sense out of long German words like Lebensabschnittspartner, which roughly means &quot;the person I&#39;m with today&quot; (taken from David Sedaris&#39;s language lessons published on the New Yorker). We&#39;ll see how computer models can help us with breaking a stretch of Chinese text into words (called tokenization in NLP jargon). To give computer models a hard time, we&#39;ll test out this text without punctuations. . text = &quot;‰ªäÂπ¥Â•ΩÁÖ©ÊÉ±Â∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈáÄÈÖíÂâõÂâõÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏È§äÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&quot; . This text is challenging not only because it can be segmented multiple ways but also because it could potentially express quite different meanings depending on how you interprete it. For instance, this part ‰ªäÂπ¥Â•ΩÁÖ©ÊÉ±Â∞ë‰∏çÂæóÊâìÂÆòÂè∏ could either mean &quot;This year will be great for you. You&#39;ll have few worries. Don&#39;t file any lawsuit&quot; or &quot;This year, you&#39;ll be very worried. A lawsuit is inevitable&quot;. Either way, it sounds like the kind of aphorism you&#39;d find in fortune cookies. Now that you know the secret to aphorisms being always right is ambiguity, we&#39;ll turn to five Python libraries for doing the hard work for us. . Jieba . Of the five tools to be introduced here, Jieba is perhaps the most widely used one, and it&#39;s even pre-installed on Colab and supported by spaCy. Unfortunately, Jieba told us that a lawsuit is inevitable this year... üò≠ . import jieba tokens = jieba.cut(text) jieba_default = &quot; | &quot;.join(tokens) print(jieba_default) . . Building prefix dict from the default dictionary ... Dumping model to file cache /tmp/jieba.cache Loading model cost 0.741 seconds. Prefix dict has been built successfully. . ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄÈÖí | ÂâõÂâõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏È§ä | Áâõ | Èöª | Èöª | Â§ßÂ¶ÇÂ±± | ËÄÅÈº† | Èöª | Èöª | Ê≠ª . The result is quite satisfying, except for ÈÖ∏È§ä, which is not even a word. Jieba is famouse for being super fast. If we run the segmentation function 1000000 times, top results we got are 256 nanoseconds per loop! . %timeit jieba.cut(text) . . The slowest run took 12.90 times longer than the fastest. This could mean that an intermediate result is being cached. 1000000 loops, best of 3: 256 ns per loop . Let&#39;s write a function for later use. . def Jieba_tokenizer(text): tokens = jieba.cut(text) result = &quot; | &quot;.join(tokens) return result . PKUSeg . As its name suggests, PKUSeg is built by the Language Computing and Machine Learning Group at Peking (aka. Beijing) University. It&#39;s been recently integrated into spaCy. . !pip install -U pkuseg . Collecting pkuseg Downloading https://files.pythonhosted.org/packages/ed/68/2dfaa18f86df4cf38a90ef024e18b36d06603ebc992a2dcc16f83b00b80d/pkuseg-0.0.25-cp36-cp36m-manylinux1_x86_64.whl (50.2MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50.2MB 66kB/s Requirement already satisfied, skipping upgrade: numpy&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from pkuseg) (1.19.5) Requirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.6/dist-packages (from pkuseg) (0.29.21) Installing collected packages: pkuseg Successfully installed pkuseg-0.0.25 . . Here&#39;s the result. . import pkuseg pku = pkuseg.pkuseg() result = pku.cut(text) result = &quot; | &quot;.join(result) result . . &#39;‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄ | ÈÖíÂâõ | Ââõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏È§ä | Áâõ | ÈöªÈöª | Â§ß | Â¶Ç | Â±± | ËÄÅÈº† | ÈöªÈöª | Ê≠ª&#39; . Compared with Jieba, PKUSeg not only got more wrong tokens (ÈÖ∏È§ä and ÈÖíÂâõ) but also ran at a much slower speed. . %timeit pku.cut(text) . . 1000 loops, best of 3: 648 ¬µs per loop . Yet, PKUSeg has one nice feature absent from Jieba. . Users have the option to choose from four domain-specific models, including news, web, medicine, and tourism. . This can be quite helpful if you&#39;re specifically dealing with texts in any of the four domains. Let&#39;s test the news domain with the first paragraph of a news article about Covid-19 published on Yahoo News. . article = &#39;&#39;&#39; Âè∞ÁÅ£Êñ∞ÂÜ†ËÇ∫ÁÇéÈÄ£Á∫åÁ¨¨6Â§©Èõ∂Êú¨ÂúüÁóÖ‰æãÁ†¥ÂäüÔºÅ‰∏≠Â§ÆÊµÅË°åÁñ´ÊÉÖÊåáÊèÆ‰∏≠ÂøÉÊåáÊèÆÂÆòÈô≥ÊôÇ‰∏≠‰ªäÂ§©ÂÆ£Â∏ÉÂúãÂÖßÊñ∞Â¢û4‰æãÊú¨ÂúüÁ¢∫ÂÆöÁóÖ‰æãÔºåÂùáÁÇ∫Ê°ÉÂúíÈÜ´Èô¢ÊÑüÊüì‰∫ã‰ª∂‰πãÁ¢∫Ë®∫ÂÄãÊ°àÁõ∏ÈóúÊé•Ëß∏ËÄÖÔºåÂÖ∂‰∏≠3‰æãÁÇ∫Ê°à863‰πãÂêå‰ΩèÂÆ∂‰∫∫(Ê°à907„ÄÅ909„ÄÅ910)ÔºåÁ†îÂà§ËàáÊ°à863„ÄÅ864„ÄÅ865ÁÇ∫‰∏ÄËµ∑ÂÆ∂Â∫≠Áæ§ËÅöÊ°àÔºåÂÖ∂‰∏≠1‰∫∫ÔºàÊ°à907ÔºâÊ≠ª‰∫°ÔºåÊòØÁõ∏Èöî8ÂÄãÊúà‰ª•‰æÜÂÜçÊ∑ªÊ≠ª‰∫°ÁóÖ‰æãÔºõÂè¶1‰æãÁÇ∫Ê°à889‰πãÂ∞±ÈÜ´Áõ∏ÈóúÊé•Ëß∏ËÄÖ(Ê°à908)„ÄÇÊ≠§Â§ñÔºå‰ªäÂ§©‰πüÊñ∞Â¢û6‰æãÂ¢ÉÂ§ñÁßªÂÖ•Á¢∫ÂÆöÁóÖ‰æãÔºåÂàÜÂà•Ëá™Âç∞Â∞º(Ê°à901)„ÄÅÊç∑ÂÖã(Ê°à902)ÂèäÂ∑¥Ë•ø(Ê°à903Ëá≥906)ÂÖ•Â¢É„ÄÇË°õÁ¶èÈÉ®Ê°ÉÂúíÈÜ´Èô¢ÊÑüÊüìÁ¥ØË®àÈÅî19‰æã(ÂÖ∂‰∏≠1‰∫∫Ê≠ª‰∫°)ÔºåÂÖ®Âè∞ÈÅî909‰æã„ÄÅ8Ê≠ª„ÄÇ &#39;&#39;&#39; . Here&#39;s the result with the default settinng. . pku = pkuseg.pkuseg() result = pku.cut(article) result = &quot; | &quot;.join(result) result . . &#39;Âè∞ÁÅ£ | Êñ∞ÂÜ† | ËÇ∫ÁÇé | ÈÄ£Á∫å | Á¨¨6 | Â§© | Èõ∂ | Êú¨Âúü | ÁóÖ‰æã | Á†¥Âäü | ÔºÅ | ‰∏≠Â§Æ | ÊµÅË°å | Áñ´ÊÉÖ | ÊåáÊèÆ | ‰∏≠ÂøÉ | ÊåáÊèÆÂÆò | Èô≥ÊôÇ | ‰∏≠ | ‰ªäÂ§© | ÂÆ£Â∏É | ÂúãÂÖß | Êñ∞Â¢û | 4 | ‰æã | Êú¨Âúü | Á¢∫ÂÆö | ÁóÖ‰æã | Ôºå | Âùá | ÁÇ∫ | Ê°ÉÂúí | ÈÜ´Èô¢ | ÊÑüÊüì | ‰∫ã‰ª∂ | ‰πã | Á¢∫ | Ë®∫ÂÄãÊ°à | Áõ∏Èóú | Êé•Ëß∏ËÄÖ | Ôºå | ÂÖ∂‰∏≠ | 3 | ‰æã | ÁÇ∫Ê°à | 863 | ‰πã | Âêå | ‰ΩèÂÆ∂‰∫∫ | ( | Ê°à | 907 | „ÄÅ | 909 | „ÄÅ | 910 | ) | Ôºå | Á†îÂà§ | ËàáÊ°à | 863 | „ÄÅ | 864 | „ÄÅ | 865 | ÁÇ∫ | ‰∏ÄËµ∑ | ÂÆ∂Â∫≠ | Áæ§ËÅöÊ°à | Ôºå | ÂÖ∂‰∏≠ | 1 | ‰∫∫ | Ôºà | Ê°à | 907 | Ôºâ | Ê≠ª‰∫° | Ôºå | ÊòØ | Áõ∏Èöî | 8 | ÂÄã | Êúà | ‰ª• | ‰æÜ | ÂÜç | Ê∑ª | Ê≠ª‰∫° | ÁóÖ‰æã | Ôºõ | Âè¶ | 1 | ‰æã | ÁÇ∫Ê°à | 889 | ‰πã | Â∞± | ÈÜ´ | Áõ∏Èóú | Êé•Ëß∏ËÄÖ | ( | Ê°à | 908 | ) | „ÄÇ | Ê≠§Â§ñ | Ôºå | ‰ªäÂ§© | ‰πü | Êñ∞Â¢û | 6‰æã | Â¢ÉÂ§ñ | ÁßªÂÖ• | Á¢∫ÂÆö | ÁóÖ‰æã | Ôºå | ÂàÜÂà• | Ëá™ | Âç∞Â∞º | ( | Ê°à | 901 | ) | „ÄÅ | Êç∑ÂÖã | ( | Ê°à | 902 | ) | Âèä | Â∑¥Ë•ø | ( | Ê°à | 903 | Ëá≥ | 906 | ) | ÂÖ•Â¢É | „ÄÇ | Ë°õÁ¶èÈÉ® | Ê°ÉÂúí | ÈÜ´Èô¢ | ÊÑüÊüì | Á¥ØË®à | ÈÅî | 19 | ‰æã | ( | ÂÖ∂‰∏≠ | 1 | ‰∫∫ | Ê≠ª‰∫° | ) | Ôºå | ÂÖ® | Âè∞ | ÈÅî | 909 | ‰æã | „ÄÅ | 8 | Ê≠ª | „ÄÇ&#39; . Here&#39;s the result with the model_name argument set to news. Both models made some mistakes here and there, but what&#39;s surprising to me is that the news-specific model even made a mistake when parsing Êñ∞ÂÜ†ËÇ∫ÁÇé, which literally means &quot;new coronavirus disease&quot; and refers to Covid-19. . pku = pkuseg.pkuseg(model_name=&#39;news&#39;) result = pku.cut(article) result = &quot; | &quot;.join(result) result . . Downloading: &#34;https://github.com/lancopku/pkuseg-python/releases/download/v0.0.16/news.zip&#34; to /root/.pkuseg/news.zip 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43767759/43767759 [00:00&lt;00:00, 104004889.71it/s] . &#39;Âè∞ÁÅ£ | Êñ∞ | ÂÜ† | ËÇ∫ | ÁÇéÈÄ£ | Á∫å | Á¨¨6Â§© | Èõ∂Êú¨Âúü | ÁóÖ‰æã | Á†¥Âäü | ÔºÅ | ‰∏≠Â§Æ | ÊµÅË°åÁñ´ÊÉÖÊåáÊèÆ‰∏≠ÂøÉ | ÊåáÊèÆ | ÂÆò | Èô≥ | ÊôÇ | ‰∏≠ | ‰ªäÂ§© | ÂÆ£Â∏É | ÂúãÂÖß | Êñ∞Â¢û | 4‰æã | Êú¨Âúü | Á¢∫ÂÆö | ÁóÖ‰æã | Ôºå | Âùá | ÁÇ∫Ê°ÉÂúíÈÜ´Èô¢ | ÊÑüÊüì | ‰∫ã‰ª∂ | ‰πã | Á¢∫ | Ë®∫ | ÂÄã | Ê°à | Áõ∏Èóú | Êé•Ëß∏ | ËÄÖ | Ôºå | ÂÖ∂‰∏≠ | 3‰æã | ÁÇ∫Ê°à | 863 | ‰πã | Âêå | ‰Ωè | ÂÆ∂‰∫∫ | (Ê°à | 907 | „ÄÅ | 909 | „ÄÅ | 910) | Ôºå | Á†îÂà§ | ËàáÊ°à | 863 | „ÄÅ | 864 | „ÄÅ | 865ÁÇ∫ | ‰∏ÄËµ∑ | ÂÆ∂Â∫≠ | Áæ§ | ËÅöÊ°à | Ôºå | ÂÖ∂‰∏≠ | 1 | ‰∫∫ | Ôºà | Ê°à | 907 | Ôºâ | Ê≠ª‰∫° | Ôºå | ÊòØ | Áõ∏Èöî | 8ÂÄãÊúà | ‰ª• | ‰æÜ | ÂÜç | Ê∑ª | Ê≠ª‰∫° | ÁóÖ‰æã | Ôºõ | Âè¶ | 1‰æã | ÁÇ∫Ê°à | 889 | ‰πã | Â∞± | ÈÜ´ | Áõ∏Èóú | Êé•Ëß∏ | ËÄÖ | (Ê°à | 908) | „ÄÇ | Ê≠§Â§ñ | Ôºå | ‰ªäÂ§© | ‰πü | Êñ∞Â¢û | 6‰æã | Â¢ÉÂ§ñ | ÁßªÂÖ• | Á¢∫ÂÆö | ÁóÖ‰æã | Ôºå | ÂàÜ | Âà• | Ëá™ | Âç∞Â∞º | (Ê°à | 901) | „ÄÅ | Êç∑ÂÖã | (Ê°à | 902) | Âèä | Â∑¥Ë•ø | (Ê°à | 903Ëá≥906 | ) | ÂÖ•Â¢É | „ÄÇ | Ë°õ | Á¶èÈÉ®Ê°ÉÂúíÈÜ´Èô¢ | ÊÑüÊüì | Á¥Ø | Ë®àÈÅî | 19‰æã | ( | ÂÖ∂‰∏≠ | 1 | ‰∫∫ | Ê≠ª‰∫° | ) | Ôºå | ÂÖ® | Âè∞ | ÈÅî | 909‰æã | „ÄÅ | 8 | Ê≠ª | „ÄÇ&#39; . Let&#39;s write a function for later use. . def PKU_tokenizer(text): pku = pkuseg.pkuseg() tokens = pku.cut(text) result = &quot; | &quot;.join(tokens) return result . PyHanLP . Next, we&#39;ll try PyHanLP. It&#39;ll take some time to download the model and data files (about 640MB in total). . !pip install pyhanlp . Collecting pyhanlp Downloading https://files.pythonhosted.org/packages/8f/99/13078d71bc9f77705a29f932359046abac3001335ea1d21e91120b200b21/pyhanlp-0.1.66.tar.gz (86kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92kB 9.0MB/s Collecting jpype1==0.7.0 Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.7MB 12.4MB/s Building wheels for collected packages: pyhanlp Building wheel for pyhanlp (setup.py) ... done Created wheel for pyhanlp: filename=pyhanlp-0.1.66-py2.py3-none-any.whl size=29371 sha256=cbe214d3e71b3e4e5692c0570e6eadbafc6845b99409abc5af1d790d9b7ee50f Stored in directory: /root/.cache/pip/wheels/25/8d/5d/6b642484b1abd87474914e6cf0d3f3a15d8f2653e15ff60f9e Successfully built pyhanlp Installing collected packages: jpype1, pyhanlp Successfully installed jpype1-0.7.0 pyhanlp-0.1.66 . from pyhanlp import * . ‰∏ãËΩΩ https://file.hankcs.com/hanlp/hanlp-1.7.8-release.zip Âà∞ /usr/local/lib/python3.6/dist-packages/pyhanlp/static/hanlp-1.7.8-release.zip 100.00%, 1 MB, 187 KB/s, ËøòÊúâ 0 ÂàÜ 0 Áßí ‰∏ãËΩΩ https://file.hankcs.com/hanlp/data-for-1.7.5.zip Âà∞ /usr/local/lib/python3.6/dist-packages/pyhanlp/static/data-for-1.7.8.zip 98.24%, 626 MB, 8117 KB/s, ËøòÊúâ 0 ÂàÜ 1 Áßí . With PyHanLP, we got a similar parsing result, but without the error that Jieba produced. . tokens = HanLP.segment(text) token_list = [res.word for res in tokens] pyhan = &quot; | &quot;.join(token_list) print(pyhan) . ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄ | ÈÖí | ÂâõÂâõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | È§ä | Áâõ | Èöª | Èöª | Â§ß | Â¶ÇÂ±± | ËÄÅÈº† | Èöª | Èöª | Ê≠ª . However, PyHanLP is about 26 times slower than Jieba, as timed below. . %timeit HanLP.segment(text) . The slowest run took 11.80 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 3: 24.6 ¬µs per loop . Let&#39;s write a function for later use. . def PyHan_tokenizer(text): tokens = HanLP.segment(text) token_list = [res.word for res in tokens] result = &quot; | &quot;.join(token_list) return result . SnowNLP . Next is SnowNLP, which I came across only recently. While PyHanLP is about 640MB in size, SnowNLP takes up only less than 40MB. . !pip install snownlp from snownlp import SnowNLP . Collecting snownlp Downloading https://files.pythonhosted.org/packages/3d/b3/37567686662100d3bce62d3b0f2adec18ab4b9ff2b61abd7a61c39343c1d/snownlp-0.12.3.tar.gz (37.6MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37.6MB 86kB/s Building wheels for collected packages: snownlp Building wheel for snownlp (setup.py) ... done Created wheel for snownlp: filename=snownlp-0.12.3-cp36-none-any.whl size=37760957 sha256=7de1997923cd51c8c45b896d9a29792e57652d5f55e3caf088212be684c50b36 Stored in directory: /root/.cache/pip/wheels/f3/81/25/7c197493bd7daf177016f1a951c5c3a53b1c7e9339fd11ec8f Successfully built snownlp Installing collected packages: snownlp Successfully installed snownlp-0.12.3 . SnowNLP gave a similar result, but made two parsing mistakes. Neither ÂÅöÈÜãÊ†º nor Â§ñÈÖ∏ is a legitimate word. . tokens = SnowNLP(text) token_list = [tokens.words][0] snow = &quot; | &quot;.join(token_list) print(snow) . ‰ªä | Âπ¥ | Â•Ω | ÁÖ© | ÊÉ± | Â∞ë‰∏çÂæó | Êâì | ÂÆòÂè∏ | ÈáÄ | ÈÖí | Ââõ | Ââõ | Â•Ω | ÂÅöÈÜãÊ†º | Â§ñÈÖ∏ | È§ä | Áâõ | Èöª | Èöª | Â§ß | Â¶Ç | Â±± | ËÄÅ | Èº† | Èöª | Èöª | Ê≠ª . SnowNLP not only made more mistakes, but also took longer to run. . %timeit SnowNLP(text) . 10000 loops, best of 3: 35.4 ¬µs per loop . But SnowNLP has a convenient feature inspired by TextBlob. Any instance of SnowNLP() has such attributes as words, pinyin (for romanization of words), tags (for parts of speech tags), and even sentiments, which calculates the probability of a text being positive. . print(tokens.words) . [&#39;‰ªä&#39;, &#39;Âπ¥&#39;, &#39;Â•Ω&#39;, &#39;ÁÖ©&#39;, &#39;ÊÉ±&#39;, &#39;Â∞ë‰∏çÂæó&#39;, &#39;Êâì&#39;, &#39;ÂÆòÂè∏&#39;, &#39;ÈáÄ&#39;, &#39;ÈÖí&#39;, &#39;Ââõ&#39;, &#39;Ââõ&#39;, &#39;Â•Ω&#39;, &#39;ÂÅöÈÜãÊ†º&#39;, &#39;Â§ñÈÖ∏&#39;, &#39;È§ä&#39;, &#39;Áâõ&#39;, &#39;Èöª&#39;, &#39;Èöª&#39;, &#39;Â§ß&#39;, &#39;Â¶Ç&#39;, &#39;Â±±&#39;, &#39;ËÄÅ&#39;, &#39;Èº†&#39;, &#39;Èöª&#39;, &#39;Èöª&#39;, &#39;Ê≠ª&#39;] . print(tokens.pinyin) . [&#39;jin&#39;, &#39;nian&#39;, &#39;hao&#39;, &#39;ÁÖ©&#39;, &#39;ÊÉ±&#39;, &#39;shao&#39;, &#39;bu&#39;, &#39;de&#39;, &#39;da&#39;, &#39;guan&#39;, &#39;si&#39;, &#39;ÈáÄ&#39;, &#39;jiu&#39;, &#39;Ââõ&#39;, &#39;Ââõ&#39;, &#39;hao&#39;, &#39;zuo&#39;, &#39;cu&#39;, &#39;ge&#39;, &#39;wai&#39;, &#39;suan&#39;, &#39;È§ä&#39;, &#39;niu&#39;, &#39;Èöª&#39;, &#39;Èöª&#39;, &#39;da&#39;, &#39;ru&#39;, &#39;shan&#39;, &#39;lao&#39;, &#39;shu&#39;, &#39;Èöª&#39;, &#39;Èöª&#39;, &#39;si&#39;] . print(list(tokens.tags)) . [(&#39;‰ªä&#39;, &#39;Tg&#39;), (&#39;Âπ¥&#39;, &#39;q&#39;), (&#39;Â•Ω&#39;, &#39;a&#39;), (&#39;ÁÖ©&#39;, &#39;Rg&#39;), (&#39;ÊÉ±&#39;, &#39;Rg&#39;), (&#39;Â∞ë‰∏çÂæó&#39;, &#39;Rg&#39;), (&#39;Êâì&#39;, &#39;v&#39;), (&#39;ÂÆòÂè∏&#39;, &#39;n&#39;), (&#39;ÈáÄ&#39;, &#39;u&#39;), (&#39;ÈÖí&#39;, &#39;n&#39;), (&#39;Ââõ&#39;, &#39;i&#39;), (&#39;Ââõ&#39;, &#39;Mg&#39;), (&#39;Â•Ω&#39;, &#39;a&#39;), (&#39;ÂÅöÈÜãÊ†º&#39;, &#39;Ag&#39;), (&#39;Â§ñÈÖ∏&#39;, &#39;Ng&#39;), (&#39;È§ä&#39;, &#39;Dg&#39;), (&#39;Áâõ&#39;, &#39;Ag&#39;), (&#39;Èöª&#39;, &#39;Bg&#39;), (&#39;Èöª&#39;, &#39;a&#39;), (&#39;Â§ß&#39;, &#39;a&#39;), (&#39;Â¶Ç&#39;, &#39;v&#39;), (&#39;Â±±&#39;, &#39;n&#39;), (&#39;ËÄÅ&#39;, &#39;a&#39;), (&#39;Èº†&#39;, &#39;Ng&#39;), (&#39;Èöª&#39;, &#39;Ag&#39;), (&#39;Èöª&#39;, &#39;Bg&#39;), (&#39;Ê≠ª&#39;, &#39;a&#39;)] . print(tokens.sentiments) . 0.04306320074116554 . Again, let&#39;s write a function for later use. . def Snow_tokenizer(text): tokens = SnowNLP(text) token_list = [tokens.words][0] result = &quot; | &quot;.join(token_list) return result . CKIP Transformers . While the four models above are primarily trained on simplified Chinese, CKIP Transformers is trained on traditional Chinese. It is created by the CKIP Lab at Academia Sinica. As its name suggests, CKIP Transformers is built on the Transformer architecture, such as BERT and ALBERT. . Note: Read this to find out How Google Changed NLP. . . !pip install -U ckip-transformers from ckip_transformers.nlp import CkipWordSegmenter . Collecting ckip-transformers Downloading https://files.pythonhosted.org/packages/19/53/81d1a8895cbbc02bf32771a7a43d78ad29a8c281f732816ac422bf54f937/ckip_transformers-0.2.1-py3-none-any.whl Collecting transformers&gt;=3.5.0 Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.8MB 22.8MB/s Requirement already satisfied, skipping upgrade: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (4.41.1) Requirement already satisfied, skipping upgrade: torch&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (1.7.0+cu101) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 43.0MB/s Requirement already satisfied, skipping upgrade: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (0.8) Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (20.8) Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (3.0.12) Collecting tokenizers==0.9.4 Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9MB 49.4MB/s Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (2019.12.20) Requirement already satisfied, skipping upgrade: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (3.4.0) Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (1.19.5) Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (2.23.0) Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.1.0-&gt;ckip-transformers) (3.7.4.3) Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.1.0-&gt;ckip-transformers) (0.16.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.15.0) Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (7.1.2) Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.0.0) Requirement already satisfied, skipping upgrade: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2.4.7) Requirement already satisfied, skipping upgrade: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (3.4.0) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2020.12.5) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2.10) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.24.3) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=010fd3e1a8d79574a0b5c323c333d1738886852c4c306fa9d161d1b51f7944b5 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sacremoses, tokenizers, transformers, ckip-transformers Successfully installed ckip-transformers-0.2.1 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2 . CKIP Transformers gives its users the freedom to choose between speed and accuracy. It comes with three levels; the smaller the number, the shorter the running time. All you need to do is pass a number to the level argument of CkipWordSegmenter(). Here&#39;re the models and F1 scores for each level: . Level 1: CKIP ALBERT Tiny, 96.66% | Level 2: CKIP ALBERT Base, 97.33% | Level 3: CKIP BERT Base, 97.60% | . By comparison, the F1 score for Jieba is only 81.18%. For more stats, visit the CKIP Lab&#39;s repo. . ws_driver = CkipWordSegmenter(level=1, device=0) . Here&#39;s the result at Level 1. What&#39;s suprising here is that this big chunk Â§ßÂ¶ÇÂ±±ËÄÅÈº† was not further segmented. But this is not a mistake. It simply means that the model has learned it as an idiom. . tokens = ws_driver([text]) ckip_1 = &quot; | &quot;.join(tokens[0]) print(ckip_1) . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3284.50it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3.98it/s] . ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄÈÖí | ÂâõÂâõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | È§ä | Áâõ | ÈöªÈöª | Â§ßÂ¶ÇÂ±±ËÄÅÈº† | ÈöªÈöª | Ê≠ª . . Of the five libraries covered here, CKIP Transformers by far takes the longest time to run. But where it lags behind in speed (i.e. 17.8 ms per loop for top 3 results), it makes it up in accuracy. . . Warning: Don&#8217;t toggle to show the output unless you really want to see a long list of details. . %timeit ws_driver([text]) . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1721.80it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 97.88it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1529.09it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.06it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1633.93it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 153.22it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4549.14it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 140.57it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1354.75it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 147.18it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1138.52it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 126.70it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2458.56it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 117.60it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1108.43it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 171.43it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1831.57it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 115.85it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3184.74it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.78it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3622.02it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 112.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 605.33it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 127.31it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1614.44it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 127.17it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2353.71it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 72.49it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2058.05it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 103.82it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3847.99it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 117.64it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1375.18it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 148.76it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1582.16it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 76.83it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3248.88it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 114.66it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3141.80it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.91it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2935.13it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 101.42it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2993.79it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 665.87it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 119.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1216.80it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 140.83it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 302.25it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.86it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3276.80it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 84.41it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 388.40it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.69it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4490.69it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 103.00it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4288.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 103.40it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3640.89it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 90.26it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 249.28it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 115.83it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1954.48it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 77.90it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 710.54it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 123.02it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1486.81it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 87.35it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1965.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 72.65it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 505.64it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 101.50it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3070.50it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 102.54it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2706.00it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 75.97it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2582.70it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 130.06it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 500.16it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 102.03it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 484.05it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 166.90it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 570.58it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 108.91it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2185.67it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 94.69it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 335.09it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.52it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 347.93it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 96.33it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3844.46it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 129.77it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 541.41it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 137.98it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2597.09it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 103.02it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4319.57it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 98.25it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4987.28it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 86.25it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 533.56it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 119.71it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 589.09it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.51it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 367.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.34it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4396.55it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 92.23it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 550.22it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 103.53it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3971.88it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 109.92it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 430.89it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 149.38it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2421.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 113.57it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3418.34it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.54it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 923.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 114.52it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1027.01it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 126.54it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 338.96it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 152.83it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3075.00it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 75.36it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1933.75it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 78.64it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4804.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 124.83it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5017.11it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.79it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4116.10it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 66.42it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3788.89it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 65.55it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3785.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 104.02it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5184.55it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 122.87it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 584.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 116.56it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2949.58it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 126.96it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1034.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.94it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3692.17it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 137.48it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 513.94it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 147.20it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1015.82it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.43it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 483.60it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 96.84it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 958.92it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 93.28it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4076.10it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 89.47it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 374.26it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 107.21it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 383.57it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 100.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3360.82it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 174.53it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5289.16it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 116.56it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 505.34it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 136.57it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 371.67it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 160.73it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4279.90it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 91.20it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2314.74it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 100.91it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1760.09it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 86.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2141.04it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 60.61it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2222.74it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 62.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 100.50it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3059.30it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 104.33it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5102.56it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 86.25it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1640.96it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 133.61it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1925.76it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.30it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5769.33it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 125.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4559.03it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 104.11it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1612.57it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 69.70it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2332.76it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 141.38it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3328.81it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 119.13it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4809.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 128.95it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4258.18it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 93.79it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5256.02it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 114.77it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 347.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 116.55it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5540.69it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 92.65it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2531.26it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 144.72it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2322.43it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 125.81it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5866.16it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 114.48it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3581.81it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.74it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3872.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 116.67it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4975.45it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 116.97it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2727.12it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 80.34it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4593.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 102.77it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5461.33it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.51it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3949.44it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 99.60it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4963.67it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 148.21it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2228.64it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.99it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5115.00it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 76.19it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 809.71it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 148.51it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5242.88it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 142.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5184.55it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 147.63it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5777.28it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 136.64it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5159.05it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 137.52it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1851.79it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 112.17it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 910.22it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 58.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1122.97it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.15it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 857.73it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 66.88it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3515.76it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 88.35it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1228.20it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 117.52it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5555.37it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 143.07it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5849.80it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 133.58it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5197.40it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.14it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2364.32it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 162.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 735.84it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 91.59it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4044.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 74.42it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1099.42it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 88.61it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 615.72it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 72.02it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2549.73it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 81.04it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 449.12it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 137.13it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2538.92it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 100.48it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2227.46it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 129.22it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5236.33it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 100.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4132.32it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 72.50it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1465.52it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 83.83it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1186.51it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.21it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1879.17it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 77.25it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2431.48it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 124.57it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3578.76it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 106.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4514.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.38it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4181.76it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 107.34it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5178.15it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 98.54it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4975.45it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 106.69it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4691.62it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 73.17it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2323.71it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 64.70it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2063.11it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 123.22it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 198.49it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.52it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4359.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 84.70it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5133.79it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.82it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1329.41it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 71.47it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1265.25it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 104.74it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 460.15it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.94it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4387.35it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.12it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4040.76it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 117.68it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1589.96it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 117.88it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4249.55it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 126.74it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4452.55it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 48.11it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 393.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 67.30it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 786.19it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 107.90it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 133.06it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 98.22it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 240.72it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 89.24it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2581.11it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 131.50it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5065.58it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 104.19it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3102.30it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 77.82it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4644.85it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 101.58it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4744.69it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 117.11it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2286.97it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 45.18it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2661.36it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 93.13it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1713.36it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 48.73it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 996.04it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 109.81it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2339.27it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.11it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1211.18it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 137.18it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1178.84it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 152.20it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4670.72it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 106.96it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4728.64it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 68.09it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4262.50it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 88.73it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3968.12it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.33it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4614.20it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 119.42it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4194.30it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.82it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4629.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 113.15it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4301.85it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 137.92it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4253.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 108.40it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5035.18it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 113.25it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5336.26it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 95.04it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5035.18it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 99.04it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5077.85it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 98.48it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 496.07it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 40.92it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2598.70it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 101.39it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5562.74it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 44.34it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3695.42it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 115.06it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.84it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4410.41it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 112.68it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5667.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.85it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4144.57it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 113.31it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3688.92it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 84.06it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 40.20it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 513.06it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 72.60it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2792.48it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 76.36it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1015.57it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 62.34it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 551.95it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 86.93it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 940.64it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 74.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 528.72it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 92.79it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4832.15it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 103.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5178.15it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 104.14it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 791.53it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 89.51it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4559.03it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 102.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1060.77it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.65it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 515.14it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 98.23it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 576.54it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 100.07it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4337.44it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 104.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 75.30it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4364.52it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 76.19it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4739.33it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 96.97it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4223.87it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 101.48it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 980.66it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 94.20it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4568.96it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 97.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4514.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 57.58it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3506.94it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 91.81it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4088.02it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 95.46it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4140.48it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 85.03it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4132.32it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 78.31it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4288.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 88.84it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4391.94it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 97.76it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4462.03it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 93.97it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 510.13it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 95.59it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4060.31it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 98.75it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4433.73it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 83.79it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2562.19it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.26it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4946.11it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 106.67it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5384.22it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 101.54it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1106.97it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 69.77it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4563.99it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 110.50it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2968.37it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 134.77it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2319.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 48.43it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5497.12it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.82it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5907.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 136.98it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5940.94it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 95.41it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 675.19it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 120.91it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5540.69it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 102.40it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1164.11it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 129.59it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 604.02it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 125.07it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5932.54it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 112.01it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1723.92it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5907.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 127.97it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6563.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 151.31it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4860.14it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 134.68it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6069.90it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 141.28it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5667.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 136.11it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5683.34it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 135.45it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6204.59it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 129.88it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6114.15it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 134.34it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4815.50it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 60.48it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 653.42it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 52.92it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5745.62it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 134.67it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5637.51it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 134.49it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5592.41it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 123.33it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4837.72it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 145.63it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1220.34it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 71.65it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 420.40it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 94.50it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 282.44it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 96.75it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 742.75it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 88.30it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5584.96it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 106.23it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 161.28it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3139.45it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 87.41it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 623.87it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 151.41it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 586.12it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 176.54it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5203.85it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 142.15it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5295.84it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 157.33it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 672.81it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.38it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6043.67it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 164.57it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 510.82it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 88.54it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5329.48it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 152.61it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3182.32it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 95.88it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6123.07it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 180.26it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5584.96it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 139.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6052.39it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.67it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5497.12it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 152.09it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 518.39it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 100.05it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6213.78it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 161.23it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2792.48it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 94.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 575.03it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 114.97it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 314.51it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 127.32it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 914.79it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 104.90it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5315.97it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 83.18it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 946.58it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 87.64it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 428.12it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.36it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 489.59it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.45it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2451.38it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 172.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2730.67it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 133.67it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 618.81it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 96.37it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 713.20it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 95.27it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1596.61it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 64.02it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 811.59it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.64it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2263.52it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 48.84it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3622.02it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 113.77it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2642.91it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 84.35it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 761.22it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 139.74it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2423.05it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 141.47it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 118.70it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6000.43it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 93.75it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5991.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 135.27it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2798.07it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 108.08it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3246.37it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 117.49it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5223.29it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 126.15it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6017.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 83.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3130.08it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 162.87it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2743.17it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 147.38it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2799.94it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 140.69it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1239.45it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.71it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3276.80it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 146.19it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1399.97it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 129.02it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6061.13it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 103.60it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 762.05it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 141.08it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5426.01it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 89.33it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5874.38it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 127.98it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4771.68it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 143.31it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3170.30it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 94.37it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3587.94it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 107.01it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4969.55it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 112.07it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5817.34it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 126.94it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5991.86it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 89.32it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5622.39it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 93.92it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 836.35it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.30it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5433.04it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 113.07it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1015.82it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 136.56it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5115.00it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 96.23it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 835.35it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 80.33it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2362.99it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 158.80it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2304.56it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 154.69it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6626.07it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 142.10it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5146.39it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 64.62it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 424.91it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.52it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4928.68it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 149.12it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5698.78it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 140.78it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6043.67it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 152.59it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5555.37it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 125.98it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 6842.26it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 162.47it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5675.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 169.06it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5229.81it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 49.94it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3313.04it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 51.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 829.90it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 69.68it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 539.74it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 81.53it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 649.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 129.52it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1151.96it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 136.85it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3731.59it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 118.09it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3518.71it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 143.15it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3008.83it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 184.14it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2641.25it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 153.42it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 559.69it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 125.42it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2803.68it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 166.56it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2931.03it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 168.61it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3084.05it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 155.53it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3826.92it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.72it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 935.18it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 70.14it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2504.06it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 100.86it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2931.03it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 131.09it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2590.68it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 146.09it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5140.08it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 126.99it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1217.50it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 134.68it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1049.89it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 97.22it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1402.78it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 124.37it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 887.12it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 128.88it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1734.62it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 58.15it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4804.47it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 132.32it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3401.71it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 119.64it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3795.75it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 127.07it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4922.89it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 136.14it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2186.81it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 130.60it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5210.32it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.68it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5236.33it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 139.41it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3155.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 119.93it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5753.50it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 737.40it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 125.77it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2498.10it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.55it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 4723.32it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 99.65it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3548.48it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 159.38it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3457.79it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.12it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 964.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 127.45it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1173.89it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 129.79it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2757.60it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 171.26it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3013.15it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 106.74it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2830.16it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 169.58it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3569.62it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 114.50it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1367.11it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 121.74it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2563.76it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 148.67it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5857.97it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 109.42it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1149.44it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 124.47it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5899.16it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5761.41it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 139.26it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5426.01it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 124.69it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 670.98it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.36it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 973.61it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 108.21it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5637.51it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 133.83it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 588.34it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 129.34it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2849.39it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 92.24it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2743.17it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 124.41it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5817.34it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 126.65it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5983.32it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.43it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3045.97it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 147.53it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 501.23it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 105.36it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2514.57it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 148.43it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 5548.02it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 111.28it/s] . 100 loops, best of 3: 17.8 ms per loop . . . Let&#39;s reinstantiate the CkipWordSegmenter() class and set the level to 2 this time. . ws_driver = CkipWordSegmenter(level=2, device=0) . Here&#39;s the result at Level 2, where Â§ßÂ¶ÇÂ±±ËÄÅÈº† was properly segmented into Â§ß, Â¶Ç, and Â±±ËÄÅÈº†. . tokens = ws_driver([text]) ckip_2 = &quot; | &quot;.join(tokens[0]) print(ckip_2) . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 2253.79it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 47.86it/s] . ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄÈÖí | ÂâõÂâõÂ•Ω | ÂÅöÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | È§äÁâõ | ÈöªÈöª | Â§ß | Â¶Ç | Â±±ËÄÅÈº† | ÈöªÈöª | Ê≠ª . . Finally, let&#39;s create an instance of CkipWordSegmenter() at Level 3. . ws_driver = CkipWordSegmenter(level=3, device=0) . However, Level 3 didn&#39;t produce a better result than Level 2. For instance, ÁâõÈöª, though a legitimate token, is not appropriate in this context. . tokens = ws_driver([text]) ckip_3 = &quot; | &quot;.join(tokens[0]) print(ckip_3) . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 976.10it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 59.33it/s] . ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄÈÖí | ÂâõÂâõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | È§ä | ÁâõÈöª | Èöª | Â§ß | Â¶Ç | Â±± | ËÄÅÈº† | ÈöªÈöª | Ê≠ª . . Here&#39;s the function for later use, which takes two arguments instead of one, unlike in previous cases. . def Ckip_tokenizer(text, level): ws_driver = CkipWordSegmenter(level=level, device=0) tokens = ws_driver([text]) result = &quot; | &quot;.join(tokens[0]) return result . Comparison . To compare the five libraries, let&#39;s write a general function. . def Tokenizer(text, style): if style == &#39;jieba&#39;: result = Jieba_tokenizer(text) elif style == &#39;pku&#39;: result = PKU_tokenizer(text) elif style == &#39;pyhan&#39;: result = PyHan_tokenizer(text) elif style == &#39;snow&#39;: result = Snow_tokenizer(text) elif style == &#39;ckip&#39;: res1 = Ckip_tokenizer(text, 1) res2 = Ckip_tokenizer(text, 2) res3 = Ckip_tokenizer(text, 3) result = f&quot;Level 1: {res1} nLevel 2: {res2} nLevel 3: {res3}&quot; output = f&quot;Result tokenized by {style}: n{result}&quot; return output . Now I&#39;m interested in finding out whether simplified or traditional Chinese would have any effect on segmentation results. In addition to the text we&#39;ve been trying (let&#39;s rename it as text_A), we&#39;ll also test another challenging text taken from the PyHanLP repo (let&#39;s call it text_B), which is intended to be ambiguous in multiple places. Given these two texts, two versions of Chinese scripts (simplified and traditional), and five segmentation libraries, we end up having in total 20 combinations of texts and libraries. . import itertools textA_tra = &quot;‰ªäÂπ¥Â•ΩÁÖ©ÊÉ±Â∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈáÄÈÖíÂâõÂâõÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏È§äÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&quot; textA_sim = &quot;‰ªäÂπ¥Â•ΩÁÉ¶ÊÅºÂ∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈÖøÈÖíÂàöÂàöÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏ÂÖªÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&quot; textB_tra = &quot;Â∑•‰ø°ËôïÂ•≥Âππ‰∫ãÊØèÊúàÁ∂ìÈÅé‰∏ãÂ±¨ÁßëÂÆ§ÈÉΩË¶ÅË¶™Âè£‰∫§‰ª£24Âè£‰∫§ÊèõÊ©üÁ≠âÊäÄË°ìÊÄßÂô®‰ª∂ÁöÑÂÆâË£ùÂ∑•‰Ωú&quot; textB_sim = &quot;Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú&quot; texts = [textA_tra, textA_sim, textB_tra, textB_sim] tokenizers = [&#39;jieba&#39;, &#39;pku&#39;, &#39;pyhan&#39;, &#39;snow&#39;,&#39;ckip&#39;] testing_tup = list(itertools.product(texts, tokenizers)) testing_tup . [(&#39;‰ªäÂπ¥Â•ΩÁÖ©ÊÉ±Â∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈáÄÈÖíÂâõÂâõÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏È§äÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;jieba&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÖ©ÊÉ±Â∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈáÄÈÖíÂâõÂâõÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏È§äÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;pku&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÖ©ÊÉ±Â∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈáÄÈÖíÂâõÂâõÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏È§äÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;pyhan&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÖ©ÊÉ±Â∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈáÄÈÖíÂâõÂâõÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏È§äÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;snow&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÖ©ÊÉ±Â∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈáÄÈÖíÂâõÂâõÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏È§äÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;ckip&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÉ¶ÊÅºÂ∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈÖøÈÖíÂàöÂàöÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏ÂÖªÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;jieba&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÉ¶ÊÅºÂ∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈÖøÈÖíÂàöÂàöÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏ÂÖªÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;pku&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÉ¶ÊÅºÂ∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈÖøÈÖíÂàöÂàöÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏ÂÖªÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;pyhan&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÉ¶ÊÅºÂ∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈÖøÈÖíÂàöÂàöÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏ÂÖªÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;snow&#39;), (&#39;‰ªäÂπ¥Â•ΩÁÉ¶ÊÅºÂ∞ë‰∏çÂæóÊâìÂÆòÂè∏ÈÖøÈÖíÂàöÂàöÂ•ΩÂÅöÈÜãÊ†ºÂ§ñÈÖ∏ÂÖªÁâõÈöªÈöªÂ§ßÂ¶ÇÂ±±ËÄÅÈº†ÈöªÈöªÊ≠ª&#39;, &#39;ckip&#39;), (&#39;Â∑•‰ø°ËôïÂ•≥Âππ‰∫ãÊØèÊúàÁ∂ìÈÅé‰∏ãÂ±¨ÁßëÂÆ§ÈÉΩË¶ÅË¶™Âè£‰∫§‰ª£24Âè£‰∫§ÊèõÊ©üÁ≠âÊäÄË°ìÊÄßÂô®‰ª∂ÁöÑÂÆâË£ùÂ∑•‰Ωú&#39;, &#39;jieba&#39;), (&#39;Â∑•‰ø°ËôïÂ•≥Âππ‰∫ãÊØèÊúàÁ∂ìÈÅé‰∏ãÂ±¨ÁßëÂÆ§ÈÉΩË¶ÅË¶™Âè£‰∫§‰ª£24Âè£‰∫§ÊèõÊ©üÁ≠âÊäÄË°ìÊÄßÂô®‰ª∂ÁöÑÂÆâË£ùÂ∑•‰Ωú&#39;, &#39;pku&#39;), (&#39;Â∑•‰ø°ËôïÂ•≥Âππ‰∫ãÊØèÊúàÁ∂ìÈÅé‰∏ãÂ±¨ÁßëÂÆ§ÈÉΩË¶ÅË¶™Âè£‰∫§‰ª£24Âè£‰∫§ÊèõÊ©üÁ≠âÊäÄË°ìÊÄßÂô®‰ª∂ÁöÑÂÆâË£ùÂ∑•‰Ωú&#39;, &#39;pyhan&#39;), (&#39;Â∑•‰ø°ËôïÂ•≥Âππ‰∫ãÊØèÊúàÁ∂ìÈÅé‰∏ãÂ±¨ÁßëÂÆ§ÈÉΩË¶ÅË¶™Âè£‰∫§‰ª£24Âè£‰∫§ÊèõÊ©üÁ≠âÊäÄË°ìÊÄßÂô®‰ª∂ÁöÑÂÆâË£ùÂ∑•‰Ωú&#39;, &#39;snow&#39;), (&#39;Â∑•‰ø°ËôïÂ•≥Âππ‰∫ãÊØèÊúàÁ∂ìÈÅé‰∏ãÂ±¨ÁßëÂÆ§ÈÉΩË¶ÅË¶™Âè£‰∫§‰ª£24Âè£‰∫§ÊèõÊ©üÁ≠âÊäÄË°ìÊÄßÂô®‰ª∂ÁöÑÂÆâË£ùÂ∑•‰Ωú&#39;, &#39;ckip&#39;), (&#39;Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú&#39;, &#39;jieba&#39;), (&#39;Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú&#39;, &#39;pku&#39;), (&#39;Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú&#39;, &#39;pyhan&#39;), (&#39;Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú&#39;, &#39;snow&#39;), (&#39;Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú&#39;, &#39;ckip&#39;)] . Here&#39;re the results for traditional textA. . for sent in testing_tup[:5]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄÈÖí | ÂâõÂâõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏È§ä | Áâõ | Èöª | Èöª | Â§ßÂ¶ÇÂ±± | ËÄÅÈº† | Èöª | Èöª | Ê≠ª Result tokenized by pku: ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄ | ÈÖíÂâõ | Ââõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏È§ä | Áâõ | ÈöªÈöª | Â§ß | Â¶Ç | Â±± | ËÄÅÈº† | ÈöªÈöª | Ê≠ª Result tokenized by pyhan: ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄ | ÈÖí | ÂâõÂâõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | È§ä | Áâõ | Èöª | Èöª | Â§ß | Â¶ÇÂ±± | ËÄÅÈº† | Èöª | Èöª | Ê≠ª Result tokenized by snow: ‰ªä | Âπ¥ | Â•Ω | ÁÖ© | ÊÉ± | Â∞ë‰∏çÂæó | Êâì | ÂÆòÂè∏ | ÈáÄ | ÈÖí | Ââõ | Ââõ | Â•Ω | ÂÅöÈÜãÊ†º | Â§ñÈÖ∏ | È§ä | Áâõ | Èöª | Èöª | Â§ß | Â¶Ç | Â±± | ËÄÅ | Èº† | Èöª | Èöª | Ê≠ª . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1287.78it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 136.95it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1394.38it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 66.44it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 998.41it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 60.47it/s] . Result tokenized by ckip: Level 1: ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄÈÖí | ÂâõÂâõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | È§ä | Áâõ | ÈöªÈöª | Â§ßÂ¶ÇÂ±±ËÄÅÈº† | ÈöªÈöª | Ê≠ª Level 2: ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄÈÖí | ÂâõÂâõÂ•Ω | ÂÅöÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | È§äÁâõ | ÈöªÈöª | Â§ß | Â¶Ç | Â±±ËÄÅÈº† | ÈöªÈöª | Ê≠ª Level 3: ‰ªäÂπ¥ | Â•Ω | ÁÖ©ÊÉ± | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈáÄÈÖí | ÂâõÂâõ | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | È§ä | ÁâõÈöª | Èöª | Â§ß | Â¶Ç | Â±± | ËÄÅÈº† | ÈöªÈöª | Ê≠ª . . Here&#39;re the results for the simplified version of the same text. Notice that the outcome can be quite different simply because a traditional text is converted to its simplified counterpart. . for sent in testing_tup[5:10]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: ‰ªäÂπ¥ | Â•Ω | ÁÉ¶ÊÅº | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈÖøÈÖí | ÂàöÂàö | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | ÂÖªÁâõ | Èöª | Èöª | Â§ßÂ¶ÇÂ±± | ËÄÅÈº† | Èöª | Èöª | Ê≠ª Result tokenized by pku: ‰ªäÂπ¥ | Â•Ω | ÁÉ¶ÊÅº | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈÖøÈÖí | ÂàöÂàö | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ÂÖª | ÁâõÈöª | Èöª | Â§ß | Â¶Ç | Â±± | ËÄÅÈº† | ÈöªÈöª | Ê≠ª Result tokenized by pyhan: ‰ªäÂπ¥ | Â•Ω | ÁÉ¶ÊÅº | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈÖøÈÖí | ÂàöÂàöÂ•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | ÂÖªÁâõ | Èöª | Èöª | Â§ß | Â¶ÇÂ±± | ËÄÅÈº† | Èöª | Èöª | Ê≠ª Result tokenized by snow: ‰ªäÂπ¥ | Â•Ω | ÁÉ¶ÊÅº | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈÖøÈÖí | ÂàöÂàö | Â•Ω | ÂÅöÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | ÂÖª | Áâõ | Èöª | Èöª | Â§ß | Â¶Ç | Â±± | ËÄÅ | Èº† | Èöª | Èöª | Ê≠ª . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 303.61it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 123.89it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 695.69it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 66.45it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 392.84it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 72.00it/s] . Result tokenized by ckip: Level 1: ‰ªäÂπ¥ | Â•Ω | ÁÉ¶ÊÅº | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈÖø | ÈÖí | ÂàöÂàö | Â•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | ÂÖª | ÁâõÈöªÈöª | Â§ßÂ¶ÇÂ±±ËÄÅÈº† | ÈöªÈöª | Ê≠ª Level 2: ‰ªäÂπ¥ | Â•Ω | ÁÉ¶ÊÅº | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈÖøÈÖí | ÂàöÂàö | Â•Ω | ÂÅöÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | ÂÖª | Áâõ | ÈöªÈöª | Â§ß | Â¶Ç | Â±±ËÄÅÈº† | ÈöªÈöª | Ê≠ª Level 3: ‰ªäÂπ¥ | Â•Ω | ÁÉ¶ÊÅº | Â∞ë‰∏çÂæó | ÊâìÂÆòÂè∏ | ÈÖøÈÖí | ÂàöÂàöÂ•Ω | ÂÅö | ÈÜã | Ê†ºÂ§ñ | ÈÖ∏ | ÂÖª | ÁâõÈöª | Èöª | Â§ß | Â¶Ç | Â±± | ËÄÅÈº† | ÈöªÈöª | Ê≠ª . . Here&#39;re the results for traditional textB. Serious mistakes include ËôïÂ•≥ (for &quot;virgin&quot;) and Âè£‰∫§ (for &quot;blowjob&quot;). Both are correct words in Chinese, but not the intended ones in this context. . for sent in testing_tup[10:15]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: Â∑•‰ø° | ËôïÂ•≥ | Âππ‰∫ã | ÊØèÊúà | Á∂ìÈÅé | ‰∏ãÂ±¨ | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | Ë¶™Âè£ | ‰∫§‰ª£ | 24 | Âè£‰∫§ | ÊèõÊ©ü | Á≠â | ÊäÄË°ìÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£ù | Â∑•‰Ωú Result tokenized by pku: Â∑•‰ø° | ËôïÂ•≥ | Âππ‰∫ã | ÊØèÊúà | Á∂ì | ÈÅé‰∏ã | Â±¨ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | Ë¶™Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§ | ÊèõÊ©ü | Á≠â | ÊäÄË°ìÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£ù | Â∑•‰Ωú Result tokenized by pyhan: Â∑• | ‰ø° | ËôïÂ•≥ | Âππ | ‰∫ã | ÊØèÊúà | Á∂ì | ÈÅé | ‰∏ã | Â±¨ | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | Ë¶™ | Âè£ | ‰∫§‰ª£ | 24 | Âè£‰∫§ | ÊèõÊ©ü | Á≠â | ÊäÄ | Ë°ì | ÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâ | Ë£ù | Â∑•‰Ωú Result tokenized by snow: Â∑• | ‰ø° | Ëôï | Â•≥ | Âππ | ‰∫ã | ÊØè | Êúà | Á∂ì | ÈÅé | ‰∏ã | Â±¨ | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | Ë¶™Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§ | Êèõ | Ê©ü | Á≠â | ÊäÄ | Ë°ìÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâ | Ë£ù | Â∑•‰Ωú . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 494.49it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 119.49it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 402.87it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 60.66it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 3942.02it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 60.56it/s] . Result tokenized by ckip: Level 1: Â∑•‰ø° | ËôïÂ•≥ | Âππ‰∫ã | ÊØè | Êúà | Á∂ìÈÅé | ‰∏ãÂ±¨ | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | Ë¶™Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§ÊèõÊ©ü | Á≠â | ÊäÄË°ìÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£ù | Â∑•‰Ωú Level 2: Â∑•‰ø°Ëôï | Â•≥ | Âππ‰∫ã | ÊØè | Êúà | Á∂ìÈÅé | ‰∏ãÂ±¨ | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | Ë¶™Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§ÊèõÊ©ü | Á≠â | ÊäÄË°ìÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£ù | Â∑•‰Ωú Level 3: Â∑•‰ø°Ëôï | Â•≥ | Âππ‰∫ã | ÊØè | Êúà | Á∂ìÈÅé | ‰∏ãÂ±¨ | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | Ë¶™Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§ÊèõÊ©ü | Á≠â | ÊäÄË°ìÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£ù | Â∑•‰Ωú . . Here&#39;re the results for the simplified version of textB. In terms of textB, CKIP Transformers Level 2 and 3 are most stable, giving the same error-free results regardless of the writing sytems. . for sent in testing_tup[15:]: result = Tokenizer(sent[0], sent[1]) print(result) . . Result tokenized by jieba: Â∑•‰ø°Â§Ñ | Â•≥Âπ≤‰∫ã | ÊØèÊúà | ÁªèËøá | ‰∏ãÂ±û | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | ‰∫≤Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§Êç¢Êú∫ | Á≠â | ÊäÄÊúØÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£Ö | Â∑•‰Ωú Result tokenized by pku: Â∑•‰ø° | Â§ÑÂ•≥ | Âπ≤‰∫ã | ÊØèÊúà | ÁªèËøá | ‰∏ãÂ±û | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | ‰∫≤Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§Êç¢Êú∫ | Á≠â | ÊäÄÊúØÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£Ö | Â∑•‰Ωú Result tokenized by pyhan: Â∑•‰ø°Â§Ñ | Â•≥Âπ≤‰∫ã | ÊØèÊúà | ÁªèËøá | ‰∏ãÂ±û | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | ‰∫≤Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§Êç¢Êú∫ | Á≠â | ÊäÄÊúØÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£Ö | Â∑•‰Ωú Result tokenized by snow: Â∑• | ‰ø°Â§ÑÂ•≥ | Âπ≤‰∫ã | ÊØèÊúà | ÁªèËøá | ‰∏ãÂ±û | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | ‰∫≤Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§Êç¢Êú∫ | Á≠â | ÊäÄÊúØÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£Ö | Â∑•‰Ωú . Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1220.69it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 131.83it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 878.39it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 71.48it/s] Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 1254.65it/s] Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 60.75it/s] . Result tokenized by ckip: Level 1: Â∑•‰ø°Â§Ñ | Â•≥Âπ≤ | ‰∫ã | ÊØè | Êúà | ÁªèËøá | ‰∏ã | Â±û | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | ‰∫≤ | Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§Êç¢Êú∫ | Á≠â | ÊäÄÊúØÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£Ö | Â∑•‰Ωú Level 2: Â∑•‰ø°Â§Ñ | Â•≥ | Âπ≤‰∫ã | ÊØè | Êúà | ÁªèËøá | ‰∏ãÂ±û | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | ‰∫≤Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§Êç¢Êú∫ | Á≠â | ÊäÄÊúØÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£Ö | Â∑•‰Ωú Level 3: Â∑•‰ø°Â§Ñ | Â•≥ | Âπ≤‰∫ã | ÊØè | Êúà | ÁªèËøá | ‰∏ãÂ±û | ÁßëÂÆ§ | ÈÉΩ | Ë¶Å | ‰∫≤Âè£ | ‰∫§‰ª£ | 24 | Âè£ | ‰∫§Êç¢Êú∫ | Á≠â | ÊäÄÊúØÊÄß | Âô®‰ª∂ | ÁöÑ | ÂÆâË£Ö | Â∑•‰Ωú . . Recap . This post has tested five word segmentation libraries against two challenging Chinese texts. Here&#39;re the takeaways: . If you value speed more than anything, Jieba is definitely the top choice. If you&#39;re dealing with traditional Chinese, it is a good practice to first convert your texts to simplified Chinese before feeding them to Jieba. Doing this may produce better results. . | If you care more about accuracy instead, it&#39;s best to use CKIP Transformers. Its Level 2 and 3 produce consistent results whether your texts are in traditional or simplified Chinese. . | Finally, if you hope to levarage the power of NLP libraries such as spaCy and Texthero (by the way, their slogan is really awesome: from zero to hero), you&#39;ll have to go for Jieba or PKUSeg. I hope spaCy will also add CKIP to its inventory of tokenizers in the near future. . | .",
            "url": "https://howard-haowen.github.io/blog.ai/tokenization/jieba/pkuseg/pyhanlp/snownlp/ckip-transformers/2021/01/29/Many-ways-to-segment-Chinese.html",
            "relUrl": "/tokenization/jieba/pkuseg/pyhanlp/snownlp/ckip-transformers/2021/01/29/Many-ways-to-segment-Chinese.html",
            "date": " ‚Ä¢ Jan 29, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Visualizing real estate prices with Altair",
            "content": "Intro . Several months into the journey of Python programming, I was already aware of visualization tools like Matplotlib Seaborn, and Plotly, which are commonly discussed on Medium. But I&#39;d never heard of Altair until I came across fastpages. Since I plan to keep writing on this fastpages-powered blog, I did some experiments with Altair. For illustration purprose, I&#39;ll be using the dataset of real estate prices in Kaohsiung, TW, which I&#39;ve cleaned and put together in my GitHub repo. For those of you who don&#39;t know Kaohsiung, it&#39;s selected by the New York Times as one of the 52 places to love in 2021. Maybe you&#39;ll consider buying an apartment in Kaohsiung after reading this post. Who knows? . Import dependencies . Altair is alrealdy pre-installed on Colab. So there&#39;s no need to pip-install it if you&#39;re doing this on Colab. . import pandas as pd import altair as alt from altair import datum . Load the dataset . The first thing to do is to git-clone the dataset into your environment. . !git clone -l -s https://github.com/howard-haowen/kh-real-estate cloned-repo %cd cloned-repo !ls . Cloning into &#39;cloned-repo&#39;... warning: --local is ignored remote: Enumerating objects: 100, done. remote: Counting objects: 100% (100/100), done. remote: Compressing objects: 100% (100/100), done. remote: Total 100 (delta 46), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (100/100), 3.30 MiB | 1.03 MiB/s, done. Resolving deltas: 100% (46/46), done. /content/cloned-repo catboost-model-feature-importance.png catboost-model-residuals.png catboost-model-feature-importance-shap-value.png compare-models.png catboost-model-learning-curve.png kh-house-prices.csv catboost-model-outliers.png kh-house-prices.pkl catboost-model.png LICENSE catboost-model-prediction-errors.png README.md . . Let&#39;s take a look at 5 random observations. . df = pd.read_pickle(&#39;kh-house-prices.pkl&#39;) df.sample(5) . . purpose trading_target land_area property_type living_room bedroom bathroom partition property_area is_managed total_floor parking_area parking_price parking_type land_use district trading_date trading_year built_date built_year price_per_sqm . 25204 ‰ΩèÂÆ∂Áî® | ÊàøÂú∞(ÂúüÂú∞+Âª∫Áâ©)+Ëªä‰Ωç | 13.53 | ‰ΩèÂÆÖÂ§ßÊ®ì(11Â±§Âê´‰ª•‰∏äÊúâÈõªÊ¢Ø) | 2 | 4 | 2 | Êúâ | 129.39 | Êúâ | 13 | 0.00 | 0 | Âù°ÈÅìÂπ≥Èù¢ | ÂïÜ | Ê•†Ê¢ìÂçÄ | 2017-01-20 | 2017 | 1995-01-26 | 1995 | 33233.0 | . 19272 ‰ΩèÂÆ∂Áî® | ÊàøÂú∞(ÂúüÂú∞+Âª∫Áâ©)+Ëªä‰Ωç | 18.24 | ‰ΩèÂÆÖÂ§ßÊ®ì(11Â±§Âê´‰ª•‰∏äÊúâÈõªÊ¢Ø) | 0 | 0 | 0 | ÁÑ° | 360.51 | Êúâ | 36 | 61.10 | 0 | Âù°ÈÅìÂπ≥Èù¢ | ÂïÜ | ÈºìÂ±±ÂçÄ | 2016-05-20 | 2016 | 2014-06-26 | 2014 | 62717.0 | . 12575 ‰ΩèÂÆ∂Áî® | ÊàøÂú∞(ÂúüÂú∞+Âª∫Áâ©)+Ëªä‰Ωç | 13.12 | ‰ΩèÂÆÖÂ§ßÊ®ì(11Â±§Âê´‰ª•‰∏äÊúâÈõªÊ¢Ø) | 2 | 3 | 2 | Êúâ | 145.90 | Êúâ | 15 | 12.66 | 840000 | Âù°ÈÅìÊ©üÊ¢∞ | ‰Ωè | ÈºìÂ±±ÂçÄ | 2015-07-14 | 2015 | 2014-05-15 | 2014 | 73101.0 | . 15299 ‰ΩèÂÆ∂Áî® | ÊàøÂú∞(ÂúüÂú∞+Âª∫Áâ©)+Ëªä‰Ωç | 15.42 | ‰ΩèÂÆÖÂ§ßÊ®ì(11Â±§Âê´‰ª•‰∏äÊúâÈõªÊ¢Ø) | 2 | 3 | 2 | Êúâ | 125.39 | Êúâ | 15 | 11.24 | 0 | Âù°ÈÅìÊ©üÊ¢∞ | ‰Ωè | Â∑¶ÁáüÂçÄ | 2015-11-08 | 2015 | 2007-01-12 | 2007 | 43066.0 | . 31446 ‰ΩèÂÆ∂Áî® | ÊàøÂú∞(ÂúüÂú∞+Âª∫Áâ©)+Ëªä‰Ωç | 13.91 | ‰ΩèÂÆÖÂ§ßÊ®ì(11Â±§Âê´‰ª•‰∏äÊúâÈõªÊ¢Ø) | 2 | 3 | 2 | Êúâ | 177.61 | Êúâ | 13 | 0.00 | 0 | Âù°ÈÅìÊ©üÊ¢∞ | ÂïÜ | ÈºìÂ±±ÂçÄ | 2017-12-12 | 2017 | 1996-04-05 | 1996 | 44479.0 | . The dataset includes 45717 observations and 21 columns. . df.shape . (45717, 21) . Most of the column names should be self-explanatory since I&#39;ve translated them from the original Chinese to English. . columns = df.columns.tolist() columns . [&#39;purpose&#39;, &#39;trading_target&#39;, &#39;land_area&#39;, &#39;property_type&#39;, &#39;living_room&#39;, &#39;bedroom&#39;, &#39;bathroom&#39;, &#39;partition&#39;, &#39;property_area&#39;, &#39;is_managed&#39;, &#39;total_floor&#39;, &#39;parking_area&#39;, &#39;parking_price&#39;, &#39;parking_type&#39;, &#39;land_use&#39;, &#39;district&#39;, &#39;trading_date&#39;, &#39;trading_year&#39;, &#39;built_date&#39;, &#39;built_year&#39;, &#39;price_per_sqm&#39;] . Here&#39;re some basic stats. . df.describe() . land_area living_room bedroom bathroom property_area total_floor parking_area parking_price trading_year built_year price_per_sqm . count 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 4.571700e+04 | 45717.000000 | 45717.000000 | 4.571700e+04 | . mean 24.949719 | 1.739987 | 2.921058 | 1.907540 | 145.261129 | 13.729947 | 6.606456 | 9.966087e+04 | 2016.760702 | 1999.837938 | 5.222278e+04 | . std 32.301563 | 0.583373 | 1.299294 | 1.084739 | 89.910644 | 7.810174 | 81.029070 | 5.323162e+05 | 1.699207 | 11.445783 | 2.236209e+04 | . min 0.010000 | 0.000000 | 0.000000 | 0.000000 | 0.020000 | 1.000000 | 0.000000 | 0.000000e+00 | 2012.000000 | 1913.000000 | 0.000000e+00 | . 25% 10.450000 | 2.000000 | 2.000000 | 1.000000 | 89.080000 | 8.000000 | 0.000000 | 0.000000e+00 | 2015.000000 | 1994.000000 | 3.849700e+04 | . 50% 16.630000 | 2.000000 | 3.000000 | 2.000000 | 128.440000 | 14.000000 | 0.000000 | 0.000000e+00 | 2017.000000 | 1999.000000 | 4.829400e+04 | . 75% 26.200000 | 2.000000 | 3.000000 | 2.000000 | 171.200000 | 15.000000 | 0.000000 | 0.000000e+00 | 2018.000000 | 2009.000000 | 6.233000e+04 | . max 2140.100000 | 22.000000 | 52.000000 | 50.000000 | 4119.900000 | 85.000000 | 17098.000000 | 1.000000e+07 | 2020.000000 | 2020.000000 | 1.048343e+06 | . MaxRowsError is the first trouble I got! It turns out that by default Altair only allows you to plot a dataset with a maximum of 5000 rows. . alt.Chart(df).mark_point().encode( x=&#39;trading_year&#39;, y=&#39;price_per_sqm&#39;, color=&#39;district&#39;, ).interactive() . MaxRowsError Traceback (most recent call last) /usr/local/lib/python3.6/dist-packages/altair/vegalite/v4/api.py in to_dict(self, *args, **kwargs) 361 copy = self.copy(deep=False) 362 original_data = getattr(copy, &#34;data&#34;, Undefined) --&gt; 363 copy.data = _prepare_data(original_data, context) 364 365 if original_data is not Undefined: /usr/local/lib/python3.6/dist-packages/altair/vegalite/v4/api.py in _prepare_data(data, context) 82 # convert dataframes or objects with __geo_interface__ to dict 83 if isinstance(data, pd.DataFrame) or hasattr(data, &#34;__geo_interface__&#34;): &gt; 84 data = _pipe(data, data_transformers.get()) 85 86 # convert string input to a URLData /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /usr/local/lib/python3.6/dist-packages/altair/vegalite/data.py in default_data_transformer(data, max_rows) 17 @curried.curry 18 def default_data_transformer(data, max_rows=5000): &gt; 19 return curried.pipe(data, limit_rows(max_rows=max_rows), to_values) 20 21 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /usr/local/lib/python3.6/dist-packages/altair/utils/data.py in limit_rows(data, max_rows) 82 &#34;than the maximum allowed ({}). &#34; 83 &#34;For information on how to plot larger datasets &#34; &gt; 84 &#34;in Altair, see the documentation&#34;.format(max_rows) 85 ) 86 return data MaxRowsError: The number of rows in your dataset is greater than the maximum allowed (5000). For information on how to plot larger datasets in Altair, see the documentation . alt.Chart(...) . . The limitation can be lifted by calling this function. . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . According to the official documentation, this is not a good solution. But I did it anyway because I didn&#39;t know better. I was then able to make a plot, but it only took seconds for my Colab notebook to crash. So the lesson learned is this: . . Warning: Never disable the restriction for max rows if you&#8217;re dealing with a huge amount of data! . A better way to deal with this is to pass data by URL, which only supports json and csv files. So I converted my dataframe to csv and then uploaded it to my GitHub repo. Then all that&#39;s needed to start using Altair is the URL to that file. . with open(&#39;kh-house-prices.csv&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as file: df.to_csv(file, index=False) . . Tip: For Altair to load your dataset properly, make sure the dataset is viewable by entering the URL in your browser. If your dataset is stored on GitHub, that means the URL has to start with https://raw.githubusercontent.com rather than https://github.com. . This URL is the data source from which we&#39;ll be making all the charts. . url= &quot;https://raw.githubusercontent.com/howard-haowen/kh-real-estate/main/kh-house-prices.csv&quot; . Simple charts . After we got the data loading and performance issue taken care of, let&#39;s break down the syntax of Altair. . I&#39;m a visual learner, so I personally think the easiest way to get started is to go to the Example Gallery and pick the kind of charts that you&#39;d like to draw. Most of the time, all you need to do is copy-paste the codes and change the data source as well as column names. . All fancy charts start with something simple.In the case of Altair, it&#39;s alt.Chart(), which takes either URL or a pandas DataFrame object (like df in our failed example above) as its argument. . Then you decide what kinds of marks you&#39;d like to draw on the chart by calling the .mark_X() function, where X could be circle if you want to represent an observation with a circle. Other types of marks used in this post include point, line, bar, and area. . Finally, you need to call the encode() function in order to map the properties of your dataset onto the chart you&#39;re making. In this example below, the function takes three arguments: . x for which column to be mapped to the x axis | y for which column to be mapped to the y axis | color for which column to be colored on the chart | . Once you pass url to alt.Chart() and the column names in your dataset to encode(), you&#39;ll get this chart. . alt.Chart(url).mark_circle().encode( x=&#39;built_date:T&#39;, y=&#39;price_per_sqm:Q&#39;, color=&#39;district:N&#39;,) . . . Note: If your data source is a dataframe, then column names are sufficient. But if your data source is an URL as is the case here, you have to specify your data types with :X right after the column names, where X can be one of these: . Q for quantitative data | O for ordinal data | N for nominal data | T for temporal data | G for geographic data | . And one thing that I like about Altair is that there&#39;re lots of predefined aggregate functions that you can use on the fly. For instance, you can pass temporal data to the function yearmonth(), which aggreates data points in terms of year and month. Or you can pass quantitative data to average(), which calculates the mean for you. This way, you won&#39;t have to create additional columns using pandas and keep your raw data as minimal as possible. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,) . . In pandas, we&#39;d filter data using df[FILTER]. In Altair, this is done by .transform_filter(). In the chart above, we see that the majority of data points gather in the lower right corner. So one way to zoom in is to set a range for built_year on the x axis, which represents the year a property was built. Suppose we want built_year to fall within 1950 and 2020, we do alt.FieldRangePredicate(field=&#39;built_year&#39;, range=[1950, 2020]). . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( alt.FieldRangePredicate(field=&#39;built_year&#39;, range=[1950, 2020]) ) . . Similarly, if we want price_per_sqm on the y axis, which represents property prices per square meter (in NT$ of course!) to be in the range of 10k and 300k, then we do alt.FieldRangePredicate(field=&#39;price_per_sqm&#39;, range=[10000, 300000]). . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( alt.FieldRangePredicate(field=&#39;price_per_sqm&#39;, range=[10000, 300000]) ) . . But what if we want to filter data from multiple columns? I found that an easy way to do that is to use datum.X, where X is a column name. Then the syntax is just like what you&#39;d see in pandas. Suppose we want built_year to be greater than 1950 and price_per_sqm less than 300k, then we do (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000). . . Important: It took me a while to figure what what kind of object datum is. It turns out that Altair is smart enough to take care of everything for you as long as you import datum. So be sure to do this: from altair import datum. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000) ) . . Finally, if you want to give viewers of your chart the liberty to zoom in and out, you can make an interactive chart simply by adding .interactive() to the end of your syntax. To see the effect, click on any grid of the following chart and then scroll your mouse or move two of your fingers up and down on your Magic Trackpad. . . Warning: Try not to make too many interactive charts if your dataset is huge because they can cause serious performance issues. That&#8217;s why I only made one interactive chart in this post. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000) ).interactive() . . I think that&#39;s enough for the basics and for you to keep the ball rolling. Coming up are some of the numerous fancy charts that you can make with Altair. . Complex charts . Suppose we want to create a scatter plot where viewers can focus on data points from a particular district of their choice, the .add_selection() function can be quite handy. Let&#39;s first check out the unique districts in the datasets. (Btw, there&#39;re more districts in Kaohsiung. These are simply more densely populated areas.) . districts = df.district.unique().tolist() districts . [&#39;ÈºìÂ±±ÂçÄ&#39;, &#39;ÂâçÈáëÂçÄ&#39;, &#39;ÂâçÈéÆÂçÄ&#39;, &#39;‰∏âÊ∞ëÂçÄ&#39;, &#39;Ê•†Ê¢ìÂçÄ&#39;, &#39;Â∑¶ÁáüÂçÄ&#39;, &#39;È≥≥Â±±ÂçÄ&#39;, &#39;Êñ∞ËààÂçÄ&#39;, &#39;ËãìÈõÖÂçÄ&#39;] . We first create a variable selection, which we&#39;ll pass to .add_selection() later. The selection itself is a built-in function called alt.selection_single(), which takes the following arguments: . name for the name you want to display in the selection area | fields for a list of column names that views can choose from | init for a dictionary specifying the default value for each selectable column | bind for a dictionary specifying the way a column is to be selected (in this case, alt.binding_select() for a drop down box) and its possible values (indicated by the argument options) | . Additionally, if we want to display information about a data point upon mouseover, we can pass a list of column names to the argument tooltip of the .encode() function. . Importantly, for the interaction to work, we have to add .add_selection(selection) right before the .encode() function. . selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;district&#39;, ], init={&#39;district&#39;: &#39;Â∑¶ÁáüÂçÄ&#39;, }, bind={&#39;district&#39;: alt.binding_select(options=districts), } ) alt.Chart(url).mark_circle().add_selection(selection).encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;price_per_sqm:Q&#39;, color=alt.condition(selection, &#39;district:N&#39;, alt.value(&#39;lightgray&#39;)), tooltip=[&#39;property_type:N&#39;,&#39;property_area:Q&#39;,&#39;parking_area:Q&#39;, &#39;built_date:T&#39;,&#39;tradinng_date:T&#39;,&#39;price_per_sqm:Q&#39;], ).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 200000) ) # add &quot;.interactive()&quot; at the end to make the chart interactive . . We can also make two charts and then concatenat them vertically by calling the function alt.vconcat(), which takes chart objects and data as its arguments. . selection = alt.selection_multi(fields=[&#39;district&#39;]) top = alt.Chart().mark_line().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;mean(price_per_sqm):Q&#39;, color=&#39;district:N&#39; ).properties( width=600, height=200 ).transform_filter( selection ) bottom = alt.Chart().mark_bar().encode( x=&#39;yearmonth(trading_date):T&#39;, y=&#39;mean(price_per_sqm):Q&#39;, color=alt.condition(selection, alt.value(&#39;steelblue&#39;), alt.value(&#39;lightgray&#39;)) ).properties( width=600, height=100 ).add_selection( selection ) alt.vconcat( top, bottom, data=url ) . . We can make one chart respond to another chart based on selection on the second one. This can be useful when we want to have both a global and detailed view of the same chart. The key function we need is alt.Scale(). Watch the top chart change as you select different areas of the bottom chart. . brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(url).mark_area().encode( x = &#39;yearmonth(built_date):T&#39;, y = &#39;price_per_sqm:Q&#39; ).properties( width=600, height=200 ) upper = base.encode( alt.X(&#39;yearmonth(built_date):T&#39;, scale=alt.Scale(domain=brush)) ) lower = base.properties( height=60 ).add_selection(brush) upper &amp; lower . . Finally, you can also pick three random variables from your dataset and make a 3 times 3 grid of charts, with each varing in the x and y axis combination. To do that, we&#39;ll need to specify repetition in two places: once in the argument of the x and y axis (i.e. alt.repeat() within alt.X and alt.Y) and the other time in the outmost layer of the syntax (i.e. .repeat() at the very end). . alt.Chart(url).mark_circle().encode( alt.X(alt.repeat(&quot;column&quot;), type=&#39;quantitative&#39;), alt.Y(alt.repeat(&quot;row&quot;), type=&#39;quantitative&#39;), color=&#39;district:N&#39; ).properties( width=150, height=150 ).repeat( row=[&#39;property_area&#39;, &#39;price_per_sqm&#39;, &#39;built_year&#39;], column=[&#39;built_year&#39;, &#39;price_per_sqm&#39;, &#39;property_area&#39;] ) . . Recap . Altair is a Python library worth looking into if you want to show interactive charts on your websites and give your visitors some freedom to play with the outcome. This post only shows what I&#39;ve tried. If you wish to dig deeper into this library, uwdata/visualization-curriculum seems like a great resource, aside from the official documentation. Now that you know the average price of real estate in Kaohsiung, TW, would you consider moving down here? üë®‚Äçüíª .",
            "url": "https://howard-haowen.github.io/blog.ai/visualization/real-estate-prices/altair/2021/01/24/Visualizing-real-estate-prices-with-Altair.html",
            "relUrl": "/visualization/real-estate-prices/altair/2021/01/24/Visualizing-real-estate-prices-with-Altair.html",
            "date": " ‚Ä¢ Jan 24, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "fastText embeddings for traditional Chinese",
            "content": ". Intro . This video explains to you what fastText is all about as if you were five years old. If the video doesn&#39;t load, click on this link. Basically, a fastText model maps a word to a series of numbers (called vectors or embeddings in NLP jargon) so that word similarity can be calcuated based on those numbers. . fastText cbow 300 dimensions from Facebook . Here&#39;re the simple steps for loading the Chinese model released by Facebook, abbreviated here as ft. . !pip install fasttext import fasttext . Collecting fasttext Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 4.4MB/s Requirement already satisfied: pybind11&gt;=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1) Requirement already satisfied: setuptools&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (51.1.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.19.5) Building wheels for collected packages: fasttext Building wheel for fasttext (setup.py) ... done Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3039122 sha256=5aa81e1045293ebc74315d2013c28cd0018ec96b8868502d535b71438f1faa0c Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592 Successfully built fasttext Installing collected packages: fasttext Successfully installed fasttext-0.9.2 . import fasttext.util fasttext.util.download_model(&#39;zh&#39;, if_exists=&#39;ignore&#39;) # zh = Chinese . Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.bin.gz . &#39;cc.zh.300.bin&#39; . ft = fasttext.load_model(&#39;cc.zh.300.bin&#39;) . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . The ft model covers a whopping great number of words, 2000000 to be exact, because it&#39;s trained on a HUGE corpus. . len(ft.words) . 2000000 . Let&#39;s check out the top 10 words most similar to &quot;Áñ´ÊÉÖ&quot; (meaning &quot;pandemic situation&quot;) according to the ft model. The numbers indicate the degree of similarity. The larger the number, the greater the similarity. . ft.get_nearest_neighbors(&quot;Áñ´ÊÉÖ&quot;) . [(0.7571706771850586, &#39;Á¶ΩÊµÅÊÑü&#39;), (0.6940484046936035, &#39;Áî≤ÊµÅ&#39;), (0.6807129383087158, &#39;ÊµÅÊÑü&#39;), (0.6670429706573486, &#39;Áñ´ÁóÖ&#39;), (0.6640030741691589, &#39;Èò≤Áñ´&#39;), (0.6531218886375427, &#39;Ëê®ÊñØÁóÖ&#39;), (0.6506668329238892, &#39;H1N1&#39;), (0.6495682001113892, &#39;Áñ´Áóá&#39;), (0.6432098150253296, &#39;Ôº≥Ôº°Ôº≤Ôº≥&#39;), (0.642063319683075, &#39;Áñ´Âå∫&#39;)] . . The results are pretty good. But the downside is that the ft model is huge in size. After being unzipped, the model file is about 6.74G. . fastText cbow 300 dimensions from ToastyNews in Cantonese . This article is what inpired me to write this post. The author trained a fastText model on articles written in Cantonese, which uses traditional characters. Here&#39;re the simple steps for loading his model, abbreviated here as hk. . Since his model is stored on GDrive, I find it more convenient to use the gdown library to download the model. . import gdown . url = &#39;https://drive.google.com/u/0/uc?export=download&amp;confirm=4g-b&amp;id=1kmZ8NKYDngKtA_-1f3ZdmbLV0CDBy1xA&#39; output = &#39;toasty_news.bin.gz&#39; gdown.download(url, output, quiet=False) . Downloading... From: https://drive.google.com/u/0/uc?export=download&amp;confirm=4g-b&amp;id=1kmZ8NKYDngKtA_-1f3ZdmbLV0CDBy1xA To: /content/toasty_news.bin.gz 2.77GB [00:26, 106MB/s] . &#39;toasty_news.bin.gz&#39; . The file needs to be first unzipped to be loaded as a fastText model. An easy way to do that is the command !gunzip plus a file name. . !gunzip toasty_news.bin.gz . hk = fasttext.load_model(&#39;/content/toasty_news.bin&#39;) hk.get_dimension() . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . 300 . The hk model covers 222906 words in total. . len(hk.words) . 222906 . fastText cbow 100 dimensions from Taiwan news in traditional Chinese . I trained a fastText model on 5816 articles of Taiwan news in traditional Chinese, most of them related to health and diseases. . tw = fasttext.load_model(path) # &quot;path&quot; is where my model is stored. tw.get_dimension() . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . 100 . The tw model covers only 11089 words in total because it&#39;s trained on a much smaller corpus than the hk model. . len(tw.words) . 11089 . Comparison . My original plan was to compare all the three models and see what similar words they come up with given the same keyword. But the ft model is huge so I can&#39;t load all of them into RAM. The RAM limit on Colab is about 12G. So we&#39;ll just compare the tw and hk model. . Since we&#39;re not concerned with the degree of similarity, let&#39;s write a simple function to show just similar words. . def similar_words(keyword, model): top_10 = model.get_nearest_neighbors(keyword) top_10 = [w[1] for w in top_10] return top_10 . Then, calling the function similar_word(), with a keyword and a fastText model as the required arguments, shows the top ten words most similar to the keyword. . similar_words(&quot;Áñ´ÊÉÖ&quot;, hk) . [&#39;Áñ´Áóá&#39;, &#39;ÁóÖÁñ´ÊÉÖ&#39;, &#39;Ê≠¶Êº¢ËÇ∫ÁÇé&#39;, &#39;Áñ´ÊΩÆ&#39;, &#39;Áñ´&#39;, &#39;Êñ∞ÂÜ†ËÇ∫ÁÇé&#39;, &#39;Áñ´Ë™ø&#39;, &#39;Áñ´Â∏Ç&#39;, &#39;Êñ∞ÂûãÂÜ†ÁãÄÁóÖÊØí&#39;, &#39;Áñ´ÁóÖ&#39;] . Now let&#39;s write a function to show the results of the two models side by side in a dataframe. . import pandas as pd models = {&#39;hk&#39;: hk, &#39;tw&#39;: tw} def compare_models(keyword, **models): hk_results = similar_words(keyword, models[&#39;hk&#39;]) tw_results = similar_words(keyword, models[&#39;tw&#39;]) data = {&#39;HKNews_&#39;+keyword: hk_results, &#39;TWNews_&#39;+keyword: tw_results} df = pd.DataFrame(data) return df . Let&#39;s test it out with the keyword &quot;Áñ´ÊÉÖ&quot;. . test = compare_models(&quot;Áñ´ÊÉÖ&quot;, **models) test . HKNews_Áñ´ÊÉÖ TWNews_Áñ´ÊÉÖ . 0 Áñ´Áóá | Áñ´ÊÉÖÂúã | . 1 ÁóÖÁñ´ÊÉÖ | Âõ†Êáâ | . 2 Ê≠¶Êº¢ËÇ∫ÁÇé | Èò≤Â†µ | . 3 Áñ´ÊΩÆ | ÂàáË®ò | . 4 Áñ´ | Êì¥Êï£ | . 5 Êñ∞ÂÜ†ËÇ∫ÁÇé | Â±¨Âú∞ | . 6 Áñ´Ë™ø | Áñ´ÊÉÖËôï | . 7 Áñ´Â∏Ç | ÂçáÊ∫´ | . 8 Êñ∞ÂûãÂÜ†ÁãÄÁóÖÊØí | Ë≠¶Ë®ä | . 9 Áñ´ÁóÖ | Âö¥Â≥ª | . It&#39;s interesting that similar words of &quot;Á∏ΩÁµ±&quot; (meaning &quot;the president&quot;) include &quot;Ëî°Á∏ΩÁµ±&quot; (meaning &quot;President Tsai&quot;, referring to Taiwan&#39;s incumbent president) according to the hk model but not the tw model. I&#39;d expect the opposite. . test = compare_models(&quot;Á∏ΩÁµ±&quot;, **models) test . HKNews_Á∏ΩÁµ± TWNews_Á∏ΩÁµ± . 0 ‰ª£Á∏ΩÁµ± | ‰∏ªÊåÅ | . 1 ÁæéÂúãÁ∏ΩÁµ± | Á∏ΩÁµ±Â∫ú | . 2 ÂâçÁ∏ΩÁµ± | ÈÉ®Èï∑ | . 3 Ê∞ëÈÅ∏Á∏ΩÁµ± | Ë¶™Ëá® | . 4 ÊùéÁ∏ΩÁµ± | Â±ÄÈï∑ | . 5 ÂâØÁ∏ΩÁµ± | ËòáÁõä‰ªÅ | . 6 ‰∏ã‰ªªÁ∏ΩÁµ± | Âππ‰∫ãÈï∑ | . 7 Á∏ΩÁêÜ | ÂâØÈô¢Èï∑ | . 8 È¶ñÁõ∏ | ÊùéÊòé‰∫Æ | . 9 Ëî°Á∏ΩÁµ± | Â∫ßË´áÊúÉ | . Again, it is the hk model, not the tw model, that knows &quot;Ëî°Ëã±Êñá&quot; (meaning &quot;Tsai Ing-wen&quot;) is most similar to &quot;Ëî°Á∏ΩÁµ±&quot; (meaning &quot;President Tsai&quot;). The two linguistic terms have the same reference. . test = compare_models(&quot;Ëî°Á∏ΩÁµ±&quot;, **models) test . HKNews_Ëî°Á∏ΩÁµ± TWNews_Ëî°Á∏ΩÁµ± . 0 Ëî°Ëã±Êñá | Á∏ΩÁµ± | . 1 Ë≥¥Ê∏ÖÂæ∑ | ‰∏ªÊåÅ | . 2 È¶¨Ëã±‰πù | ÈÉ®Èï∑ | . 3 ÊùéÁ∏ΩÁµ± | Ë¶™Ëá® | . 4 ÊûóÂÖ® | Â±ÄÈï∑ | . 5 Ê∞ëÈÄ≤Èª® | Èô≥Âª∫‰ªÅ | . 6 ÊüØÊñáÂì≤ | Â∫ßË´áÊúÉ | . 7 Á∏ΩÁµ± | Âê≥ | . 8 Â∑ùÊôÆ | ÂâØÈô¢Èï∑ | . 9 Á∏ΩÁµ±Â∫ú | Á∏ΩÁµ±Â∫ú | . Finally, let&#39;s write a function to quickly compare a list of keywords. . def concat_dfs(keyword_list): dfs = [] for word in keyword_list: df = compare_models(word, **models) dfs.append(df) results = pd.concat(dfs, axis=1) return results . keywords = &quot;Áñ´ÊÉÖ Áñ´Ëãó ÁóÖÊØí ËÇ∫ÁÇé Ê™¢Áñ´ ÊµÅÊÑü Âè∞ÁÅ£&quot; key_list = keywords.split() concat_dfs(key_list) . HKNews_Áñ´ÊÉÖ TWNews_Áñ´ÊÉÖ HKNews_Áñ´Ëãó TWNews_Áñ´Ëãó HKNews_ÁóÖÊØí TWNews_ÁóÖÊØí HKNews_ËÇ∫ÁÇé TWNews_ËÇ∫ÁÇé HKNews_Ê™¢Áñ´ TWNews_Ê™¢Áñ´ HKNews_ÊµÅÊÑü TWNews_ÊµÅÊÑü HKNews_Âè∞ÁÅ£ TWNews_Âè∞ÁÅ£ . 0 Áñ´Áóá | Áñ´ÊÉÖÂúã | ÊµÅÊÑüÁñ´Ëãó | Êé•Á®Æ | Ëº™ÁãÄÁóÖÊØí | ÁóÖÊØíÂûã | Ê≠¶Êº¢ËÇ∫ÁÇé | Ë±¨ÈèàÁêÉËèå | Ê™¢Áñ´ÊâÄ | Ê™¢Áñ´ÂÆò | ÊµÅÊÑüÁóÖÊØí | Êñ∞ÊµÅÊÑü | Ëá∫ÁÅ£ | Ëá∫ÁÅ£ | . 1 ÁóÖÁñ´ÊÉÖ | Âõ†Êáâ | ÂÖçÁñ´Èáù | Êé•Á®ÆÂú∞ | Âê´ÁóÖÊØí | ËÖ∫ÁóÖÊØí | Ê≠¶ËÇ∫ | ÈèàÁêÉËèå | Ê™¢Áñ´‰∏≠ÂøÉ | Ê™¢Áñ´Á´ô | ÊµÅË°åÊÄßÊÑüÂÜí | Èò≤ÊµÅÊÑü | Âè∞ÁÅ£Âúã | Ê†πÈô§ | . 2 Ê≠¶Êº¢ËÇ∫ÁÇé | Èò≤Â†µ | ÊäóÈ´î | Êé•Á®ÆÁÇ∫ | ÂÜ†ÁãÄÁóÖÊØí | ÁóÖÊØíÊ†™ | Êñ∞ÂÜ†ËÇ∫ÁÇé | ÁñæÊÇ£ | Ê™¢Áñ´Â±Ä | Ê™¢Áñ´Â±Ä | Á¶ΩÊµÅÊÑü | ÊâìÊµÅÊÑü | Âè∞ÁÅ£ÊîøÂ∫ú | Ê≠∑Âè≤ | . 3 Áñ´ÊΩÆ | ÂàáË®ò | Ëó•Áâ© | Êé•Á®ÆÈªû | Êñ∞ÁóÖÊØí | ÁóÖÊØíÂ≠∏ | ÁóÖÁñ´ | ÈõôÁêÉËèå | Ê™¢Áñ´Á´ô | Ëà™Ê©ü | ÊµÅË°åÁóÖ | Â∞çÊµÅÊÑü | ‰∏≠ÂúãÂ§ßÈô∏ | ‰∏ÄÁõ¥ | . 4 Áñ´ | Êì¥Êï£ | Âç°‰ªãËãó | Êé•Á®ÆÂç° | ËÖ∫ÁóÖÊØí | ÂûãÂà• | ÁóÖÁñ´ÊÉÖ | ÂøÉÂåÖËÜúÁÇé | ÈöîÈõ¢ | Ê©üÂ†¥ | Áñ´Áóá | Ë±¨ÊµÅÊÑü | ‰∏≠Âúã | ‰∫ûÂ§™ | . 5 Êñ∞ÂÜ†ËÇ∫ÁÇé | Â±¨Âú∞ | ÊäóÁîüÁ¥† | Áñ´ËãóÈáè | ÊÆ∫ÁóÖÊØí | ÊµÅË°åÊ†™ | Áñ´Áóá | ÁâπÊÆä | Ëá™ÊàëÈöîÈõ¢ | ÂÖ•Â¢É | ÁóÖÁñ´ÊÉÖ | ÊäóÊµÅÊÑü | Âè∞ÁÅ£‰∫∫ | Ë´∏Â§ö | . 6 Áñ´Ë™ø | Áñ´ÊÉÖËôï | Ëº™ÁãÄÁóÖÊØí | Âç°‰ªãËãó | È∫ªÁñπÁóÖÊØí | Ê†™ÂûãÂà• | ÈùûÂÖ∏ÂûãËÇ∫ÁÇé | ‰æµË•≤ÊÄß | ÈöîÈõ¢ËÄÖ | Ë™øÊü•Ë°® | È∫ªÁñπ | ÊµÅÊÑüÁñ´ | ‰∏≠ÂúãÂè∞ÁÅ£ | ‰∏ñÁ¥Ä | . 7 Áñ´Â∏Ç | ÂçáÊ∫´ | Êé•Á®Æ | ÂÉπ | Èò≤ÁóÖÊØí | ËÖ∏ | Áñ´ÊÉÖ | ÂÜ†ÁãÄÂãïËÑà | ÁóÖÊØíÊ™¢Ê∏¨ | Ê∏ØÂè£ | ÊµÅË°åÊÄßËÖÆËÖ∫ÁÇé | Â≠£ÁØÄÊÄß | Âè∞ÁÅ£Áç®Á´ã | Ë∑®ÂúãÊÄß | . 8 Êñ∞ÂûãÂÜ†ÁãÄÁóÖÊØí | Ë≠¶Ë®ä | È†êÈò≤Êé•Á®Æ | Â§öÂêà‰∏Ä | ÂÜ†Áä∂ÁóÖÊØí | ÈáçÁµÑ | Âª¢ÁÇé | ÁóáÂÄôÁæ§ | ÂÅ•Â∫∑Áî≥Â†± | ÁôªÊ©ü | ÊµÅÊÑüÁñ´Ëãó | ÊµÅÊÑüÁóÖ | Âè∞ÁÅ£Á§æ | Èù¢Ëá® | . 9 Áñ´ÁóÖ | Âö¥Â≥ª | È∫ªÁñπ | Âª†Áâå | ÁóÖÂéüÈ´î | ÊØíÊ†™ | Áñ´ÁóÖ | ÂÜ†ÁãÄÁóÖÊØí | Ê™¢Ê∏¨ | ËÅ≤ÊòéÂç° | ÁôªÈù©ÁÜ± | Êñ∞Âûã | ‰∏≠ËèØÊ∞ëÂúã | ‰πã‰∏≠ | . . keywords = &quot;È†≠Áóõ ÁôºÁáí ÊµÅÈºªÊ∞¥ &quot; key_list = keywords.split() concat_dfs(key_list) . HKNews_È†≠Áóõ TWNews_È†≠Áóõ HKNews_ÁôºÁáí TWNews_ÁôºÁáí HKNews_ÊµÅÈºªÊ∞¥ TWNews_ÊµÅÈºªÊ∞¥ . 0 ÂÅèÈ†≠Áóõ | ËÇåËÇâÁóõ | Âí≥ÂóΩ | Âá∫Áèæ | ÈºªÊ∞¥ | ÈºªÊ∞¥ | . 1 È†≠Áñº | È™®È†≠Áóõ | ÁóÖÂæµ | ÁóáÁãÄ | ÊµÅÈºªÊ∂ï | ÊµÅ | . 2 ËÉÉÁóõ | ÂôÅÂøÉ | ÁôºÈ´òÁáí | ÂñâÂö®Áóõ | Âí≥ÂóΩ | ÈºªÂ°û | . 3 Áó†Áóõ | È™®È†≠ | ÁôºÁóÖ | ÂóÖË¶∫ | ÂñâÂö®Áóõ | ÂñâÂö® | . 4 ÈÖ∏Áóõ | ËÇåËÇâ | ÂñâÂö®Áóõ | Âë≥Ë¶∫ | Âá∫Áñπ | ÂñâÂö®Áô¢ | . 5 ÁµûÁóõ | ÂæåÁúº | ÊµÅÈºªÊ∞¥ | ÈºªÊ∞¥ | ÁôºÁáí | ÂñâÂö®Áóõ | . 6 ËÖ´Áóõ | ÁïèÂØí | ÁóáÁãÄ | Âí≥ÂóΩ | ÁöÆÁñπ | ÂóÖË¶∫ | . 7 È†≠Êöà | ÂÄ¶ÊÄ† | ÂæµÁãÄ | ÂñâÂö® | ÊµÅÈºªË°Ä | Âë≥Ë¶∫ | . 8 ÂøÉÁµûÁóõ | Á™©Áóõ | Âá∫Áñπ | Áñ≤ÂÄ¶ | ËÇöÁÄâ | Á¥ÖÁñπ | . 9 ËÖ∞ËÉåÁóõ | ÁµêËÜú | ÂëºÂê∏Âõ∞Èõ£ | ÊµÅ | Âí≥Ë°Ä | ÂÄ¶ÊÄ† | . Recap . You can easily find out words most similar to a keyword that you&#39;re interested in just by loading a fastText model. And for it to work pretty well, you don&#39;t even need to have a huge corpus at hand. Even if you don&#39;t know how to train a model from scratch, you can still make good use of fastText by loading pretrained models, like those released by Facebook. In total, 157 languages are covered, including even Malay and Malayalam! (Btw, check out this Malayalam grammar that I wrote and is now archived on Semantic Scholar.) . Note: This is my first post written in Jupyter notebook. After I uploaded the .ipynb file to GitHub, the post didn&#8217;t show up automatically and I got a CI failing warning in my repo. Listed in the tip section below is what I did to fix the problem, though I&#8217;m not sure which of them was the key. . Tip: 1. requested an automatic update by following the instructions in the troubleshooting guide 2. deleted the backtick symbol in the summary section of the front matter 3. uploaded the .ipynb file straight from Colab to GitHub instead of doing this manually .",
            "url": "https://howard-haowen.github.io/blog.ai/fasttext/embeddings/similar-words/2021/01/22/fastText-embeddings-for-traditional-Chinese.html",
            "relUrl": "/fasttext/embeddings/similar-words/2021/01/22/fastText-embeddings-for-traditional-Chinese.html",
            "date": " ‚Ä¢ Jan 22, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "My first post",
            "content": ". There‚Äôs always an awesome list for X. . Don&#39;t reinvent the wheel is something you‚Äôll hear a lot in the field of technology. That means, whenever possible, try to use existing tools out there instead of building from scratch. If you use GitHub long enough, you probably already know this: for almost everything you‚Äôd like to learn, there‚Äôs an awesome list for that, which collects all sorts of awesome resources, including tools and tutorials. All you need to do is search for awesome X on GitHub. Or better yet, there‚Äôs even a meta list of awesome lists on various topics, like sindresorhus/awesome. Included in it is an awesome list for linguistics, theimpossibleastronaut/awesome-linguistics, which is a good place to start learning about natural language processing (NLP) if you are a humanities major who knows nothing about programming, as I was about one year ago. To take one more example, I wish I had discovered keon/awesome-nlp much earlier, which could‚Äôve saved me lots of time when I was still fumbling around and trying to wrap my head around how a tool, say gensim, fits into the broader picture of NLP. But sometimes the name of an awesome list doesn‚Äôt have the keyword awesome in it, such as this gem ivan-bilan/The-NLP-Pandect. In my opinion, although an awesome list by any other name would be as awesome, the The-NLP-Pandect repo would have got much more stars if awesome were in its name. . But the hardest part is to get the ball rolling. . However, awesome as they are, awesome lists can be quite intimidating to go through and easy to get lost in. And even when you are lucky enough to finally come across an awesome tool that you wanna try out, it sometimes takes lots of trials and errors to figure out the right way to get the ball rolling, especially when you are a fresh programmer. So on this blog, I plan to add my personal touch to various tools, documenting not only what I did right to get the ball rolling, but also what I did wrong to save you (or even future me) from abysmal frustration. For almost every tool, there is already a wide specturum of instructional documents available online, ranging from hardcore official documentations to professional posts on platforms like Medium. And this blog is meant to be a friendly complement to those. I‚Äôll be writing in plain language because I was not trained for computer science or programming anyway. . The fast stack . I‚Äôd like to start with a series of tools that I dub the fast stack, including fastpages, fast.ai, fastText, and fastAPI. First of all, fastpages , designed by the awesome fast.ai team, is basically a template for building blogs (like this one!) within seconds. It does lots of awesome things for you hehind the scenes. Features that I like about it include: . automatically converts .md and .ipynb files on GitHub to posts on your blog | automatically adds links to Colab and GitHub | shows interactive visualizations of your data with the help of Altair | supports comments, tags, and fast search (super fast at that!) | is free from end to end | . Truth be told that I actually failed twice before I successfully set up this blog. The lesson learned is this: do exactly what‚Äôs said in the instructions! Humanities majors like me are often taught to be creative, but be sure to leave your creativity at the door when you set up computer programs. This then concludes my first post. Nothing is super technical here since it‚Äôs just a warm-up. I‚Äôll save other tools in the fast stack for another day. . . I was clueless when I read PR in the instructions. It turns out to mean &#39;pull requests&#39;. Click on the tab that says &#39;pull requests&#39; when you are done forking the original repo. Then you&#39;re good to go by following the instructions there.",
            "url": "https://howard-haowen.github.io/blog.ai/awesome-list/fastpages/2020/01/17/my-first-post.html",
            "relUrl": "/awesome-list/fastpages/2020/01/17/my-first-post.html",
            "date": " ‚Ä¢ Jan 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". . My name is Haowen Jiang. I obtained a PhD of linguistics from Rice University and spent most of my early career in academia doing research on indigenous languages of Taiwan, all of them related to languages such as Tagalog and Malay/Indonesian. Then along came COVID-19, which disrupted most people&#39;s lives, including mine. Somehow I became hooked on machine learning and Python programming. After several months of self-learning, I transitioned from an IT muggle to an AI engineer. My current job primarily focuses on Natural Language Processing, including text classification, clustering, topic modeling, document similarity, and information retrieval. View my certificates in Data Science .",
          "url": "https://howard-haowen.github.io/blog.ai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://howard-haowen.github.io/blog.ai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}