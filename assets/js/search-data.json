{
  
    
        "post0": {
            "title": "What I did to pass the Microsoft Exam DP-100 for data scientists",
            "content": ". What I did to pass the Microsoft Exam DP-100 for data scientists . It’s been a while since I posted anything here. And it’s primarily because I was busy preparing for the Microsoft DP-100 Exam, which is an associate (i.e. intermediate) level for designing and implementing data science solutions on Azure. I spent about 2 months preparing for it and got my certificate today! Here’s what it looks like. . . The certificate also comes with a dedicated Credly webpage, where people can verify its validity. In this post, I’ll share the resources that I perused for the prep work. They come from three sources: a book, two Udemy courses, and three kinds of official materials from Microsoft. . One book . . The book is entitled Data Science Solutions on Azure Tools and Techniques Using Databricks and MLOps, which is a good start if you don’t know anything about Azure or other cloud tools that Microsoft offers. One nice thing about this book is that it includes lots of snapshots of the Azure Machine Learning interface, which gives you a clear mental map of the workflow even if you don’t have an Azure account. . Two Udemy courses . Here’re the two Udemy courses that I took. . DP-100 Microsoft Azure Data Scientist Complete Exam Prep taught by Scott Duffy | DP-100: A-Z Machine Learning using Azure Machine Learning taught by Jitesh Khurkhuriya | . The first one is about 2.5 hours long and the second one has a staggering length of 32 hours, but they cost the same (i.e. NT$2,990 last time I checked)! The second one is long primarily because it covers a lot of the classic Azure ML Studio, which is gradually replaced by the updated Azure ML Designer. Also included is a primer on Python programming. I skipped this part, which is why I didn’t finish the second course, as indicated in my Udemy learning history. . . I don’t remember much from the first course while I find it hard to say the same to the second one. Its instructor, Jitesh, has a very personalized style of intonation, which makes even the dullest stuff sound interesting. He is not only pretty good at explaining complex concepts in simple and intuitive ways, but also meticulous about every bit of details regarding the exam. My only complaint about his course is the low speed at which he talks. I had to set the playback speed to x1.5 faster to stay awake. . Three kinds of official materials from Microsoft . Microsoft did an incredibly good job of making high-quality learning materials accessible to the public, which makes me feel that Microsoft really wants exam-takers to pass rather than fail. Here’re the three kinds of official materials that I went through. . 4 online learning paths Create machine learning models (5 Modules) | Use visual tools to create machine learning models with Azure Machine Learning (4 Modules) | Build and operate machine learning solutions with Azure Machine Learning (14 Modules) | Perform data science with Azure Databricks (12 Modules) | . | 17 ipynb files from the MicrosoftLearning/mslearn-dp100 repo | Documentation for Algorithm &amp; module reference for Azure Machine Learning designer | . The learning paths give you a high-level overview of what Azure ML products can do. The ipynb files cover pretty much all you need to know about Azure Machine Learning SDK for Python as far as the exam is concerned. They are so helpful that I forked the whole repo into this one, to which I added a note of crucial codes taken from all the 17 ipynb files. The last material is specfically about Azure ML Designer, and I made a summary note of the entire documentation. . . To navigate this long note, press the button with three bars and dots at the upper left corner of the markdown page. Some final notes about the exam. In hindsight, I believe I could still pass the exam even without taking the Udemy courses, considering the official materials from Microsoft already cover the great majority of questions. Next, the price for taking the exam varies depending on the country you choose even if you take it online. I did it online (who wouldn’t during the pandemic!), and the price for Taiwan is $125 USD (but $165 USD instead for the USA). Finally, you have a few options for the exam language, which can be English, Japanese, Chinese, or some others. But the exam proctor only speaks English or Japanese. I wonder why Microsoft offers Japanese as the only non-English language for proctors. .",
            "url": "https://howard-haowen.github.io/blog.ai/azure/certificate/2021/06/27/How-I-passed-Microsoft-DP100-Exam.html",
            "relUrl": "/azure/certificate/2021/06/27/How-I-passed-Microsoft-DP100-Exam.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Classifying customer reviews with spaCy v3.0",
            "content": ". Intro . Text classification is a very common NLP task. Given enough training data, it&#39;s relatively easy to build a model that can automatically classify previously unseen texts in a way that follows the logic of the training data. In this post, I&#39;ll go through the steps for building such a model. Specifically, I&#39;ll leverage the power of the recently released spaCy v3.0 to train two classification models, one for identifying the sentiment of customer reviews in Chinese as being positive or negative (i.e. binary classification) and the other for predicting their product categories in a list of five (i.e. multiclass classification). If you can&#39;t wait to see how spaCy v3.0 has made the training process an absolute breeze, feel free to jump to the training the textcat component with CLI section. If not, bear with me on this long journey. All the datasets and models created in this post are hosted in this repo of mine. . Preparing the dataset . Getting the dataset . I&#39;m hoping to build classification models that can take traditional Chinese texts as input, but I can&#39;t find any publicly available datasets of customer reviews in traditional Chinese. So I had to make do with reviews in simplified Chinese. Let&#39;s first download the dataset using !wget. . !wget https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip . --2021-03-07 14:08:42-- https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 4084428 (3.9M) [application/zip] Saving to: ‘online_shopping_10_cats.zip’ online_shopping_10_ 100%[===================&gt;] 3.89M --.-KB/s in 0.1s 2021-03-07 14:08:43 (37.1 MB/s) - ‘online_shopping_10_cats.zip’ saved [4084428/4084428] . Then we unzip the downloaded file online_shopping_10_cats.zip with, surprisingly, !unzip. . !unzip online_shopping_10_cats.zip . Archive: online_shopping_10_cats.zip inflating: online_shopping_10_cats.csv . The dataset has three columns: review for review texts, label for sentiment , and cat for product categories. Here&#39;s a random sample of five reviews. . import pandas as pd file_path = &#39;/content/online_shopping_10_cats.csv&#39; df = pd.read_csv(file_path) df.sample(5) . cat label review . 35479 洗发水 | 0 | 买了两套说好的赠品吹风机没给！ | . 35477 洗发水 | 0 | 抢购降价一半？坑，爹？没赶上时候？ | . 53299 酒店 | 1 | 碰上酒店做活动，加了40元给升级到行政房。房间还不错，比较新。服务员是实习生，不熟练但态度认... | . 14367 手机 | 1 | 1）外观新颖2）拥有强大的多媒体功能和卓越的性能，同时将电池的消耗减到最小，方便了更多的用户... | . 12549 平板 | 0 | 分辨率太低，买的后悔了. | . There&#39;re in total 62774 reviews. . df.shape . (62774, 3) . The label column has only two unique values, 1 for positive reviews and 0 for negative ones. . df.label.unique() . array([1, 0]) . The cat column has nine unique values. . df.cat.unique() . array([&#39;书籍&#39;, &#39;平板&#39;, &#39;手机&#39;, &#39;水果&#39;, &#39;洗发水&#39;, &#39;热水器&#39;, &#39;蒙牛&#39;, &#39;衣服&#39;, &#39;计算机&#39;, &#39;酒店&#39;], dtype=object) . Before moving on, let&#39;s save the raw dataset to Google Drive. The dest variable can be any GDrive path you like. . dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/&quot; !cp {file_path} {dest} . Filtering the datases . Now let&#39;s do some data filtering. The groupby function from pandas is very useful, and here&#39;s how to get the counts of each of the unique values in the cat column. . df.groupby(by=&#39;cat&#39;).size() . cat 书籍 3851 平板 10000 手机 2323 水果 10000 洗发水 10000 热水器 575 蒙牛 2033 衣服 10000 计算机 3992 酒店 10000 dtype: int64 . To create a balanced dataset, I decided to keep categories whose counts are 10,000. So we&#39;re left with five product categories, 平板 for tablets, 水果 for fruits, 洗发水 for shampoo, 衣服 for clothing, and finally 酒店 for hotels. . There&#39;re many ways to filter data in pandas, and my favorite is to first create a filt variable that holds a list of True and False, which in this particular case is whether the value in the cat volumn is in the cat_list variable for the categories to be kept. Then we can simply filter data with df[filt]. After filtering, the dataset is reduced to 50,000 reviews. . cat_list = [&#39;平板&#39;, &#39;水果&#39;, &#39;洗发水&#39;, &#39;衣服&#39;, &#39;酒店&#39;] filt = df[&#39;cat&#39;].isin(cat_list) df = df[filt] df.shape . (50000, 3) . Now, the dataset is balanced in terms of both the cat and label columnn. There&#39;re 10,000 reviews for each product category. . df.groupby(by=&#39;cat&#39;).size() . cat 平板 10000 水果 10000 洗发水 10000 衣服 10000 酒店 10000 dtype: int64 . And there&#39;re 25,000 for either of the two sentiments. . df.groupby(by=&#39;label&#39;).size() . label 0 25000 1 25000 dtype: int64 . Having made sure the filtered dataset is balanced, we can now reset the index, and save the dataset as online_shopping_5_cats_sim.csv. . df.reset_index(inplace=True, drop=True) df.to_csv(dest+&quot;online_shopping_5_cats_sim.csv&quot;, sep=&quot;,&quot;, index=False) . Converting the dataset to traditional Chinese . Let&#39;s load back the file we just saved to make sure the dataset is accessible for later use. . df = pd.read_csv(dest+&quot;online_shopping_5_cats_sim.csv&quot;) df.tail() . cat label review . 49995 酒店 | 0 | 我们去盐城的时候那里的最低气温只有4度，晚上冷得要死，居然还不开空调，投诉到酒店客房部，得到... | . 49996 酒店 | 0 | 房间很小，整体设施老化，和四星的差距很大。毛巾太破旧了。早餐很简陋。房间隔音很差，隔两间房间... | . 49997 酒店 | 0 | 我感觉不行。。。性价比很差。不知道是银川都这样还是怎么的！ | . 49998 酒店 | 0 | 房间时间长，进去有点异味！服务员是不是不够用啊！我在一楼找了半个小时以上才找到自己房间，想找... | . 49999 酒店 | 0 | 老人小孩一大家族聚会，选在吴宫泛太平洋，以为新加坡品牌一定很不错，没想到11点30分到前台，... | . Next, I converted the reviews from simplified Chinese to traditional Chinese using the OpenCC library. . !pip install OpenCC . Collecting OpenCC Downloading https://files.pythonhosted.org/packages/d5/b4/24e677e135df130fc6989929dc3990a1ae19948daf28beb8f910b4f7b671/OpenCC-1.1.1.post1-py2.py3-none-manylinux1_x86_64.whl (1.3MB) |████████████████████████████████| 1.3MB 8.0MB/s Installing collected packages: OpenCC Successfully installed OpenCC-1.1.1.post1 . OpenCC has many conversion methods. I specifically used s2twp, which converts simplified Chinese to traditional Chinese adpated to Taiwanese vocabulary. The adaptation is not optimal, but it&#39;s better than mechanic simplified-to-traditional conversion. Here&#39;s a random review in the two writing systems. . from opencc import OpenCC cc = OpenCC(&#39;s2twp&#39;) test = df.loc[49995, &#39;review&#39;] print(test) test_tra = cc.convert(test) print(test_tra) . 我们去盐城的时候那里的最低气温只有4度，晚上冷得要死，居然还不开空调，投诉到酒店客房部，得到的答复是现在还没有领导指示需要开暖气，如果冷到话可以多给一床被子，太可怜了。。。 我們去鹽城的時候那裡的最低氣溫只有4度，晚上冷得要死，居然還不開空調，投訴到酒店客房部，得到的答覆是現在還沒有領導指示需要開暖氣，如果冷到話可以多給一床被子，太可憐了。。。 . Having made sure the conversion is correct, we can now go ahead and convert all reviews. . df.loc[ : , &#39;review&#39;] = df[&#39;review&#39;].apply(lambda x: cc.convert(x)) . Let&#39;s make the same change to the cat column. . df.loc[ : , &#39;cat&#39;] = df[&#39;cat&#39;].apply(lambda x: cc.convert(x)) . And then we save the converted dataset as online_shopping_5_cats_tra.csv. . df.to_csv(dest+&#39;online_shopping_5_cats_tra.csv&#39;, sep=&quot;,&quot;, index=False) . Inspecting the dataset . Let&#39;s load back the file just saved to make sure it&#39;s accessible in the future. . df = pd.read_csv(dest+&#39;online_shopping_5_cats_tra.csv&#39;) df.tail() . cat label review . 49995 酒店 | 0 | 我們去鹽城的時候那裡的最低氣溫只有4度，晚上冷得要死，居然還不開空調，投訴到酒店客房部，得到... | . 49996 酒店 | 0 | 房間很小，整體設施老化，和四星的差距很大。毛巾太破舊了。早餐很簡陋。房間隔音很差，隔兩間房間... | . 49997 酒店 | 0 | 我感覺不行。。。價效比很差。不知道是銀川都這樣還是怎麼的！ | . 49998 酒店 | 0 | 房間時間長，進去有點異味！服務員是不是不夠用啊！我在一樓找了半個小時以上才找到自己房間，想找... | . 49999 酒店 | 0 | 老人小孩一大家族聚會，選在吳宮泛太平洋，以為新加坡品牌一定很不錯，沒想到11點30分到前臺，... | . Before building models, I would normally inspect the dataset. There&#39;re many ways to do so. I recently learned that there&#39;s a trick on Colab which allows you to filter a dataset in an interactive manner. All it takes is three lines of code. . %load_ext google.colab.data_table from google.colab import data_table data_table.DataTable(df, include_index=False, num_rows_per_page=10) . Warning: total number of rows (50000) exceeds max_rows (20000). Limiting to first max_rows. . cat label review . 0 平板 | 1 | ﻿很不錯。。。。。。很好的平板 | . 1 平板 | 1 | 幫同學買的，同學說感覺挺好，質量也不錯 | . 2 平板 | 1 | 東西不錯，一看就是正品包裝，還沒有開機，相信京東，都是老顧客，還是京東值得信賴，給五星好評 | . 3 平板 | 1 | 總體而言，產品還是不錯的。 | . 4 平板 | 1 | 好，不錯，真的很好不錯 | . ... ... | ... | ... | . 49995 酒店 | 0 | 我們去鹽城的時候那裡的最低氣溫只有4度，晚上冷得要死，居然還不開空調，投訴到酒店客房部，得到... | . 49996 酒店 | 0 | 房間很小，整體設施老化，和四星的差距很大。毛巾太破舊了。早餐很簡陋。房間隔音很差，隔兩間房間... | . 49997 酒店 | 0 | 我感覺不行。。。價效比很差。不知道是銀川都這樣還是怎麼的！ | . 49998 酒店 | 0 | 房間時間長，進去有點異味！服務員是不是不夠用啊！我在一樓找了半個小時以上才找到自己房間，想找... | . 49999 酒店 | 0 | 老人小孩一大家族聚會，選在吳宮泛太平洋，以為新加坡品牌一定很不錯，沒想到11點30分到前臺，... | . 50000 rows × 3 columns . Alternatively, if you&#39;d like to see some sample reviews from all the categories, the groupby function is quite handy. The trick here is to feed pd.DataFrame.sample to the apply function so that you can specify the number of reviews to inspect from each product category. . df.groupby(&#39;cat&#39;).apply(pd.DataFrame.sample, n=3)[[&#39;label&#39;, &#39;review&#39;]] . label review . cat . 平板 6247 0 | 這個平板真的是3G的嗎？你們有沒有忽悠唉，為什麼我下了一個百度影片，就卡的要死要活的，跟我以... | . 1081 1 | 看網頁玩王者榮耀都很流暢，音質畫面都不錯，就是稍微重了點，綜合性價比還是很好的 | . 1042 1 | 我覺得還可以，就是把膜貼上了之後，有點滑不動，打遊戲的時候就很煩了 | . 水果 10468 1 | 還不錯，這個價格比我在外面買的划算，以後還會經常來，個頭不是很大，還可以吧 | . 10986 1 | 蘋果味道好，就是小了一點，比想象的要小量了一下，好像基本上都沒到70毫米。快遞還是挺快的包裝... | . 18062 0 | 這是我在京東消費這麼多年來買到唯一次最爛的東西，還是自營一斤6塊錢的就這貨色還有一個是壞的，... | . 洗髮水 23271 1 | 很好，很舒服，清揚就是好用，謝謝老闆，希望一直好用，好好好好好好好好好，快樂 | . 21992 1 | 一如既往的好 京東速度快 值得信賴 優惠多多 | . 21867 1 | 京東購物 多快好省 寫評論真的很累 有木有 每次商品很滿意就用這個 各位大佬請放心購買 | . 衣服 30157 1 | 褲子質量不錯，貨真價實，和店家介紹的基本相符，大小合適，樣式也很滿意，穿上褲子走路感到很輕便，舒服 | . 33190 1 | 做工精細穿起來很舒服，質量很好 | . 35326 0 | 質量很差，沒有想象的那麼好 | . 酒店 42243 1 | 房間是新裝修的,用的是淺色調,感覺很溫馨,佈局較合理,顯的比較寬敞.酒店選用的布草很講究,放... | . 48082 0 | 實在忍無可忍。1、水。缺水現象——洗澡至中途，突然斷水，望著滿身的肥皂泡欲哭無淚；多水現象—... | . 49552 0 | 下雨天冷，想洗熱水澡，可惜開了半小時水還是冷的 | . Finally, one of the most powerful ways of exploring a dataset is to use the facets-overview library. Let&#39;s first create a column for the length of review texts. . df[&#39;len&#39;] = df[&#39;review&#39;].apply(len) df.tail() . cat label review len . 49995 酒店 | 0 | 我們去鹽城的時候那裡的最低氣溫只有4度，晚上冷得要死，居然還不開空調，投訴到酒店客房部，得到... | 86 | . 49996 酒店 | 0 | 房間很小，整體設施老化，和四星的差距很大。毛巾太破舊了。早餐很簡陋。房間隔音很差，隔兩間房間... | 102 | . 49997 酒店 | 0 | 我感覺不行。。。價效比很差。不知道是銀川都這樣還是怎麼的！ | 29 | . 49998 酒店 | 0 | 房間時間長，進去有點異味！服務員是不是不夠用啊！我在一樓找了半個小時以上才找到自己房間，想找... | 64 | . 49999 酒店 | 0 | 老人小孩一大家族聚會，選在吳宮泛太平洋，以為新加坡品牌一定很不錯，沒想到11點30分到前臺，... | 455 | . Then we install the library. . !pip install facets-overview . Collecting facets-overview Downloading https://files.pythonhosted.org/packages/df/8a/0042de5450dbd9e7e0773de93fe84c999b5b078b1f60b4c19ac76b5dd889/facets_overview-1.0.0-py2.py3-none-any.whl Requirement already satisfied: protobuf&gt;=3.7.0 in /usr/local/lib/python3.7/dist-packages (from facets-overview) (3.12.4) Requirement already satisfied: pandas&gt;=0.22.0 in /usr/local/lib/python3.7/dist-packages (from facets-overview) (1.1.5) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from facets-overview) (1.19.5) Requirement already satisfied: six&gt;=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf&gt;=3.7.0-&gt;facets-overview) (1.15.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf&gt;=3.7.0-&gt;facets-overview) (53.0.0) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.22.0-&gt;facets-overview) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.22.0-&gt;facets-overview) (2018.9) Installing collected packages: facets-overview Successfully installed facets-overview-1.0.0 . In order to render an interative visualization of the dataset, we first convert the DataFrame object df to the json format and then add it to an HTML template, as shown below. If you choose len for Binning | X-Axis, cat for Binning | Y-Axis, and finally review for Label By, you&#39;ll see all the reviews are beautifully arranged in term of text length along the X axis and product categories along the Y axis. They&#39;re also color-coded with respect to sentiment, blue for positive and red for negative. Clicking on a point of either color shows the values of that particular datapoint. Feel free to play around. . from IPython.core.display import display, HTML jsonstr = df.to_json(orient=&#39;records&#39;) HTML_TEMPLATE = &quot;&quot;&quot; &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;import&quot; href=&quot;https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html&quot;&gt; &lt;facets-dive id=&quot;elem&quot; height=&quot;600&quot;&gt;&lt;/facets-dive&gt; &lt;script&gt; var data = {jsonstr}; document.querySelector(&quot;#elem&quot;).data = data; &lt;/script&gt;&quot;&quot;&quot; html = HTML_TEMPLATE.format(jsonstr=jsonstr) display(HTML(html)) . Training spaCy models . Instantiating a pretrained spaCy model . spaCy supports many pretrained models in multiple languages, and offers a convenient widget for working out the code for downloading a particular model for a particular language. I specifically picked zh_core_web_md for Chinese. . !pip install -U pip setuptools wheel !pip install -U spacy !python -m spacy download zh_core_web_md . Collecting pip Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB) |████████████████████████████████| 1.5MB 8.9MB/s Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (54.0.0) Requirement already up-to-date: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2) Installing collected packages: pip Found existing installation: pip 19.3.1 Uninstalling pip-19.3.1: Successfully uninstalled pip-19.3.1 Successfully installed pip-21.0.1 Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4) Collecting spacy Downloading spacy-3.0.3-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB) |████████████████████████████████| 12.7 MB 313 kB/s Requirement already satisfied: numpy&gt;=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5) Requirement already satisfied: typing-extensions&gt;=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3) Collecting typer&lt;0.4.0,&gt;=0.3.0 Downloading typer-0.3.2-py3-none-any.whl (21 kB) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1) Requirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5) Collecting thinc&lt;8.1.0,&gt;=8.0.0 Downloading thinc-8.0.1-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 53.7 MB/s Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.0.0) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5) Collecting pathy Downloading pathy-0.4.0-py3-none-any.whl (36 kB) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.0) Collecting srsly&lt;3.0.0,&gt;=2.4.0 Downloading srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456 kB) |████████████████████████████████| 456 kB 50.6 MB/s Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1) Collecting pydantic&lt;1.8.0,&gt;=1.7.1 Downloading pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB) |████████████████████████████████| 9.1 MB 60.5 MB/s Collecting spacy-legacy&lt;3.1.0,&gt;=3.0.0 Downloading spacy_legacy-3.0.1-py2.py3-none-any.whl (7.0 kB) Collecting catalogue&lt;2.1.0,&gt;=2.0.1 Downloading catalogue-2.0.1-py3-none-any.whl (9.6 kB) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;spacy) (3.4.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;spacy) (2.4.7) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2020.12.5) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.24.3) Requirement already satisfied: click&lt;7.2.0,&gt;=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer&lt;0.4.0,&gt;=0.3.0-&gt;spacy) (7.1.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;spacy) (1.1.1) Collecting smart-open&lt;4.0.0,&gt;=2.2.0 Downloading smart_open-3.0.0.tar.gz (113 kB) |████████████████████████████████| 113 kB 67.7 MB/s Building wheels for collected packages: smart-open Building wheel for smart-open (setup.py) ... done Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107097 sha256=806c280d458aada53b7a5891f07eb7158ebc0fa2261a2415a0bd4e89c5015af9 Stored in directory: /root/.cache/pip/wheels/83/a6/12/bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870 Successfully built smart-open Installing collected packages: catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-legacy, pathy, spacy Attempting uninstall: catalogue Found existing installation: catalogue 1.0.0 Uninstalling catalogue-1.0.0: Successfully uninstalled catalogue-1.0.0 Attempting uninstall: srsly Found existing installation: srsly 1.0.5 Uninstalling srsly-1.0.5: Successfully uninstalled srsly-1.0.5 Attempting uninstall: smart-open Found existing installation: smart-open 4.2.0 Uninstalling smart-open-4.2.0: Successfully uninstalled smart-open-4.2.0 Attempting uninstall: thinc Found existing installation: thinc 7.4.0 Uninstalling thinc-7.4.0: Successfully uninstalled thinc-7.4.0 Attempting uninstall: spacy Found existing installation: spacy 2.2.4 Uninstalling spacy-2.2.4: Successfully uninstalled spacy-2.2.4 Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.3 spacy-legacy-3.0.1 srsly-2.4.0 thinc-8.0.1 typer-0.3.2 2021-03-05 01:24:40.728642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 Collecting zh-core-web-md==3.0.0 Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.0.0/zh_core_web_md-3.0.0-py3-none-any.whl (80.1 MB) |████████████████████████████████| 80.1 MB 56 kB/s Requirement already satisfied: spacy&lt;3.1.0,&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-md==3.0.0) (3.0.3) Collecting spacy-pkuseg&lt;0.1.0,&gt;=0.0.27 Downloading spacy_pkuseg-0.0.28-cp37-cp37m-manylinux2014_x86_64.whl (2.4 MB) |████████████████████████████████| 2.4 MB 8.8 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (3.7.4.3) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (20.9) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (1.0.5) Requirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (8.0.1) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (0.8.2) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (4.41.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (2.11.3) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (54.0.0) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (2.0.1) Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (0.4.0) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (3.0.5) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (3.0.1) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (0.4.1) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (2.4.0) Requirement already satisfied: typer&lt;0.4.0,&gt;=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (0.3.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (2.0.5) Requirement already satisfied: numpy&gt;=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (1.19.5) Requirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (2.23.0) Requirement already satisfied: pydantic&lt;1.8.0,&gt;=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (1.7.3) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (3.7.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (3.4.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (2.4.7) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (2020.12.5) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (2.10) Requirement already satisfied: cython&gt;=0.25 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg&lt;0.1.0,&gt;=0.0.27-&gt;zh-core-web-md==3.0.0) (0.29.22) Requirement already satisfied: click&lt;7.2.0,&gt;=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer&lt;0.4.0,&gt;=0.3.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (7.1.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (1.1.1) Requirement already satisfied: smart-open&lt;4.0.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;zh-core-web-md==3.0.0) (3.0.0) Installing collected packages: spacy-pkuseg, zh-core-web-md Successfully installed spacy-pkuseg-0.0.28 zh-core-web-md-3.0.0 ✔ Download and installation successful You can now load the package via spacy.load(&#39;zh_core_web_md&#39;) . . Everthing in spaCy starts with loading a model. The model we downloaded has five built-in components, accessible via the pipe_names attribute. . import spacy nlp=spacy.load(&quot;zh_core_web_md&quot;) nlp.pipe_names . [&#39;tok2vec&#39;, &#39;tagger&#39;, &#39;parser&#39;, &#39;ner&#39;, &#39;attribute_ruler&#39;] . Let&#39;s call the nlp object with a sample review text and print out its tokens to make sure the model is working. . test = df.loc[22, &#39;review&#39;] doc = nlp(test) for tok in doc: print(tok.text, tok.pos_) . 買 VERB 了 PART 送人 NOUN 的 PART ， PUNCT 沒有 VERB 拆開 NOUN ， PUNCT 包裝 VERB 高大上 NOUN ， PUNCT 以前 NOUN 自用 VERB 買 VERB 了 PART 一 NUM 個 NUM ， PUNCT 這次 ADJ 活動 NOUN 便宜 VERB 好 VERB 幾百 VERB ， PUNCT 價格 NOUN 好 ADV 划算 VERB 啊 PART ， PUNCT 以後 NOUN 還會 VERB 降價 NOUN 嗎 VERB ？？？ PUNCT . Converting the dataset to spaCy format for training . To train a classification model wtih spaCy, we have to convert our dataset to spaCy format. Before that, we&#39;ll create a directory called data under the current working directory cwd. This is where we&#39;ll save the data in spaCy format. . . Tip: I find the os.makedirs function much more useful than the more commonly seen os.mkdir() function because the former creates all the intermediate directories for you if they don&#8217;t exist yet. . import os def create_dir(dir_name): cwd = os.getcwd() project_dir = os.path.join(cwd, dir_name) os.makedirs(project_dir) return project_dir project_dir = create_dir(&quot;data&quot;) project_dir . &#39;/content/data&#39; . The first step for the conversion is to create a list of tuples with two elements, one for the text and the other for the text class label. Let&#39;s start with the binary classification for sentiment first and generalize to multiclass classification for product categories later. . The easiest way to generate such a list is to create a new column called tuples (or whatever), whose values are derived by applying a lambda function to review for text and label for text class. I learned this trick from this article. Here&#39;re the first 10 tuples in the newly created dataset list. . df[&#39;tuples&#39;] = df.apply(lambda row: (row[&#39;review&#39;], row[&#39;label&#39;]), axis=1) dataset = df[&#39;tuples&#39;].tolist() dataset[:10] . [(&#39; ufeff很不錯。。。。。。很好的平板&#39;, 1), (&#39;幫同學買的，同學說感覺挺好，質量也不錯&#39;, 1), (&#39;東西不錯，一看就是正品包裝，還沒有開機，相信京東，都是老顧客，還是京東值得信賴，給五星好評&#39;, 1), (&#39;總體而言，產品還是不錯的。&#39;, 1), (&#39;好，不錯，真的很好不錯&#39;, 1), (&#39;很好，音響效果不錯。挺喜歡的，用了一段時間才來評價的。&#39;, 1), (&#39;包裝不是很好，裡面太空了我不塞多點汽泡袋，其它的還可以&#39;, 1), (&#39;之前一直用華為手機，覺得不錯，想試一下平板，前天下單，昨天到貨，試了一下，感覺還不錯，家裡也有ipad，兩者不能比較，各有各的好，只是近年一直都用華為，手機，盒子用起來都不錯，現在是老媽用蘋果手機用ipad，我都用華為，用習慣了。大小也正合適，挺喜歡的，唯一欠缺的一點就是沒有耳機。後面多用幾次再追評吧。&#39;, 1), (&#39;說實話，非常喜歡，這個是送給客戶的，之前送的蘋果派的，但是不能拷檔案，自從找到這款，物美價廉，經濟實惠，很喜歡！！！買了好幾個了&#39;, 1), (&#39;續航能力也太差了吧，看影片1個小時就25%了&#39;, 1)] . Then I split the dataset using train_test_split from scikit-learn. The train_data and valid_data hold 80% and 20% of the dataset, respectively. . from sklearn.model_selection import train_test_split train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1) . Then we need to turn the dataset list into spaCy Doc objects and assign text classes to each of them so that the model can start learning. Assigning text classes is the trickiest part and took me lots of trials and errors to figure out. Unfortunately, official documentation of spaCy is not very clear about this part. At first, I used this make_docs function from p-sodmann/Spacy3Textcat to create Doc objects. That was successful, but the trained model gave weird results. Then I realized that the values of text classes need to be either True or False. So here&#39;s my revised version of the make_docs function. The trick here is to use bool(label), which will be True if the value of the label is 1 and False if its value is 0. . from tqdm.auto import tqdm from spacy.tokens import DocBin def make_docs(data): &quot;&quot;&quot; this will take a list of texts and labels and transform them in spacy documents texts: List(str) labels: List(labels) returns: List(spacy.Doc.doc) &quot;&quot;&quot; docs = [] # nlp.pipe([texts]) is way faster than running nlp(text) for each text # as_tuples allows us to pass in a tuple, the first one is treated as text # the second one will get returned as it is. for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)): # we need to set the (text)cat(egory) for each document doc.cats[&quot;POSITIVE&quot;] = bool(label) doc.cats[&quot;NEGATIVE&quot;] = not bool(label) # put them into a nice list docs.append(doc) return docs . spaCy v3.0 introduces the DocBin class, which is the recommended container for serializing a list of Doc objects, much like the pickle format, but better. After we create a list of Doc objects with the make_docs function, we can then generate an instance of the DocBin class for holding that list and save the serialized object to disk by calling the to_disk function. We first do this to valid_data and save the serialized file as valid.spacy in the data directory. . valid_docs = make_docs(valid_data) doc_bin = DocBin(docs=valid_docs) doc_bin.to_disk(&quot;./data/valid.spacy&quot;) . . . Then we do the same thing to train_data and save it as train.spacy. This will take much more time to run since train_data is much larger than valid_data. . train_docs = make_docs(train_data) doc_bin = DocBin(docs=train_docs) doc_bin.to_disk(&quot;./data/train.spacy&quot;) . . . And that was the end of the converting process! Now I&#39;d like to save the serialized data for later use, so I copied it to dest, which is a directory in my Google Drive. . . Note: Remember to use the -R flag when copying whatever in a directory to another. . source = &quot;/content/data&quot; dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-sentiment-model/&quot; !cp -R {source} {dest} . Training the textcat component with CLI . spaCy v3.0 offers a nice and easy way to train spaCy&#39;s textcat component with a command line interfact (CLI). The first step is to get the base_config.cfg file, which is generated for you if you use spaCy&#39;s quickstart widget. The only thing that needs to changed is train and dev under [paths], which indicate where the serialized files are stored. . %%writefile base_config.cfg # This is an auto-generated partial config. To use it with &#39;spacy train&#39; # you can run spacy init fill-config to auto-fill all default settings: # python -m spacy init fill-config ./base_config.cfg ./config.cfg [paths] train = &quot;data/train.spacy&quot; dev = &quot;data/valid.spacy&quot; [system] gpu_allocator = null [nlp] lang = &quot;zh&quot; pipeline = [&quot;tok2vec&quot;,&quot;textcat&quot;] batch_size = 1000 [components] [components.tok2vec] factory = &quot;tok2vec&quot; [components.tok2vec.model] @architectures = &quot;spacy.Tok2Vec.v2&quot; [components.tok2vec.model.embed] @architectures = &quot;spacy.MultiHashEmbed.v1&quot; width = ${components.tok2vec.model.encode.width} attrs = [&quot;ORTH&quot;, &quot;SHAPE&quot;] rows = [5000, 2500] include_static_vectors = false [components.tok2vec.model.encode] @architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot; width = 96 depth = 4 window_size = 1 maxout_pieces = 3 [components.textcat] factory = &quot;textcat&quot; [components.textcat.model] @architectures = &quot;spacy.TextCatBOW.v1&quot; exclusive_classes = true ngram_size = 1 no_output_layer = false [corpora] [corpora.train] @readers = &quot;spacy.Corpus.v1&quot; path = ${paths.train} max_length = 2000 [corpora.dev] @readers = &quot;spacy.Corpus.v1&quot; path = ${paths.dev} max_length = 0 [training] dev_corpus = &quot;corpora.dev&quot; train_corpus = &quot;corpora.train&quot; [training.optimizer] @optimizers = &quot;Adam.v1&quot; [training.batcher] @batchers = &quot;spacy.batch_by_words.v1&quot; discard_oversize = false tolerance = 0.2 [training.batcher.size] @schedules = &quot;compounding.v1&quot; start = 100 stop = 1000 compound = 1.001 [initialize] vectors = null . Writing base_config.cfg . Then we create the config.cfg configuration file with base_config.cfg by using this command. . !python -m spacy init fill-config ./base_config.cfg ./config.cfg . 2021-03-05 01:43:52.105583: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 ✔ Auto-filled config with all values ✔ Saved config config.cfg You can now add your data and train your pipeline: python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy . Here comes the fun part, where we can see how a computer model learns over time! The initial score was 43, which is worse than chance. But only after 200 iterations, the score already skyrocketed to 83! By the end of the training, we got a score of 92, which is pretty awesome, considering that we didn&#39;t do any text preprocessing. . !python -m spacy train ./config.cfg --output ./output . 2021-03-05 01:44:00.345586: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 ✔ Created output directory: output ℹ Using CPU =========================== Initializing pipeline =========================== Set up nlp object from config Pipeline: [&#39;tok2vec&#39;, &#39;textcat&#39;] Created vocabulary Finished initializing nlp object Initialized pipeline components: [&#39;tok2vec&#39;, &#39;textcat&#39;] ✔ Initialized pipeline ============================= Training pipeline ============================= ℹ Pipeline: [&#39;tok2vec&#39;, &#39;textcat&#39;] ℹ Initial learn rate: 0.001 E # LOSS TOK2VEC LOSS TEXTCAT CATS_SCORE SCORE - 0 0 0.00 0.06 43.32 0.43 0 200 0.00 34.18 83.25 0.83 0 400 0.00 21.43 86.60 0.87 0 600 0.00 12.77 87.25 0.87 0 800 0.00 15.29 89.00 0.89 0 1000 0.00 11.78 89.71 0.90 0 1200 0.00 5.92 89.62 0.90 0 1400 0.00 4.52 89.98 0.90 0 1600 0.00 1.51 90.40 0.90 0 1800 0.00 4.41 90.82 0.91 0 2000 0.00 0.80 90.76 0.91 0 2200 0.00 1.53 90.97 0.91 0 2400 0.00 0.16 91.06 0.91 0 2600 0.00 0.37 91.34 0.91 0 2800 0.00 0.13 91.33 0.91 0 3000 0.00 0.16 91.40 0.91 0 3200 0.00 0.20 91.56 0.92 0 3400 0.00 0.38 91.57 0.92 0 3600 0.00 0.10 91.70 0.92 1 3800 0.00 0.12 91.41 0.91 1 4000 0.00 0.17 91.65 0.92 1 4200 0.00 0.10 91.54 0.92 1 4400 0.00 0.14 91.58 0.92 1 4600 0.00 0.15 91.58 0.92 1 4800 0.00 0.19 91.59 0.92 1 5000 0.00 0.09 91.66 0.92 1 5200 0.00 2.16 91.91 0.92 1 5400 0.00 0.12 91.86 0.92 1 5600 0.00 0.21 91.68 0.92 1 5800 0.00 0.10 91.69 0.92 2 6000 0.00 2.18 91.22 0.91 2 6200 0.00 0.15 91.50 0.92 2 6400 0.00 0.16 91.61 0.92 2 6600 0.00 0.10 91.67 0.92 2 6800 0.00 0.11 91.57 0.92 ✔ Saved pipeline to output directory output/model-last . . spaCy automatically saves two models to the path specified by the --output argument. We&#39;ll use the best model for testing. Here&#39;s the py file adapted from p-sodmann/Spacy3Textcat. . %%writefile test_input.py import spacy # load the best model from training nlp = spacy.load(&quot;output/model-best&quot;) text = &quot;&quot; print(&quot;type : &#39;quit&#39; to exit&quot;) # predict the sentiment until someone writes quit while text != &quot;quit&quot;: text = input(&quot;Please enter a review here: &quot;) doc = nlp(text) print(doc.cats) . Writing test_input.py . Let&#39;s print out some reviews in valid_data for the sake of testing the model. . valid_data[:10] . [(&#39;和以前買的不一樣！以前用的特別滑！用的也少！這次買的不僅用的東西多！而且還頭皮癢！有頭屑！&#39;, 0), (&#39;不好意思我貨未收到，我想起訴你們！&#39;, 0), (&#39;寶貝收到了，是我想要的版型，質量十分好，這條褲子的顏色跟圖片上一樣，是我想要的顏色，總之，是一次很愉快的購物&#39;, 1), (&#39;一點也不甜不清脆，只能榨汁了。&#39;, 0), (&#39;蘋果小，但味道不錯，給個好評吧&#39;, 1), (&#39;平板收到，很滿意和想像的一樣，沒失望第一時間體驗中，稍後再評&#39;, 1), (&#39;說好的樂視會員一年 送哪去了 差評&#39;, 0), (&#39;相當完美啊 好東西&#39;, 1), (&#39;連生產生日都沒有，真不知道是不是真的。&#39;, 0), (&#39;蘋果一如既往得好，脆、甜、水分足，推薦購買哦奧～&#39;, 1)] . Now let&#39;s run test_input.py to start grilling the model about the sentiment of reviews. The results are quite satisfactory. I even intentionally asked about a mixed review, 他們家的香蕉好吃，但是蘋果卻一點也不甜! (Their bananas are delicious, but their apples are not sweet at all!). And the model gave almost equal scores to the two sentiments! . !python test_input.py . . 2021-03-05 05:16:18.092297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 type : &#39;quit&#39; to exit Please enter a review here: 和以前買的不一樣！以前用的特別滑！用的也少！這次買的不僅用的東西多！而且還頭皮癢！有頭屑！ {&#39;POSITIVE&#39;: 0.022647695615887642, &#39;NEGATIVE&#39;: 0.9773523211479187} Please enter a review here: 平板收到，很滿意和想像的一樣，沒失望第一時間體驗中，稍後再評 {&#39;POSITIVE&#39;: 0.43043994903564453, &#39;NEGATIVE&#39;: 0.5695600509643555} Please enter a review here: 說好的樂視會員一年 送哪去了 差評 {&#39;POSITIVE&#39;: 0.20790322124958038, &#39;NEGATIVE&#39;: 0.792096734046936} Please enter a review here: 連生產生日都沒有，真不知道是不是真的。 {&#39;POSITIVE&#39;: 0.04043734818696976, &#39;NEGATIVE&#39;: 0.9595626592636108} Please enter a review here: 蘋果一如既往得好，脆、甜、水分足，推薦購買哦奧～ {&#39;POSITIVE&#39;: 0.9967644214630127, &#39;NEGATIVE&#39;: 0.0032355356961488724} Please enter a review here: 他們家的香蕉好吃，但是 {&#39;POSITIVE&#39;: 0.7071358561515808, &#39;NEGATIVE&#39;: 0.2928641438484192} Please enter a review here: 他們家的香蕉好吃，但是蘋果卻一點也不甜! {&#39;POSITIVE&#39;: 0.5702691674232483, &#39;NEGATIVE&#39;: 0.4297308325767517} Please enter a review here: quit {&#39;POSITIVE&#39;: 0.46642377972602844, &#39;NEGATIVE&#39;: 0.533576250076294} . Finally, I saved the best trained model to Google Drive. . source = &quot;/content/output/model-best&quot; dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-sentiment-model/&quot; !cp -R {source} {dest} . Going from binary to multiclass classification . Next, we&#39;ll go over pretty much the same steps to train a multiclass classification model. This time, the dataset is a list of tuples with review texts and product categories. . df[&#39;tuples&#39;] = df.apply(lambda row: (row[&#39;review&#39;], row[&#39;cat&#39;]), axis=1) dataset = df[&#39;tuples&#39;].tolist() dataset[-10:] . [(&#39;非常一般的酒店，房裡設施很舊，房間送餐竟然要加多50%的送餐費。總之找不到好的方面來說，有其他選擇就不要去了&#39;, &#39;酒店&#39;), (&#39;房間沒窗戶，攜程網竟然沒有說明！&#39;, &#39;酒店&#39;), (&#39;1、中午快一點到店，說房間沒有收拾出來，只好寄存行李出去玩，晚上回到房間滿屋煙味，打電話後給調了同型房間。明知道入住的是一家三口親子游，還這樣安排，初始印象分-1 2、電視很多頻道都是雪花，調了幾個臺都是如此，一點看電視的興致都沒了，-0.5毫不過分 3、房間設施一點不人性化，先說燈，除了總控，還要開燈具開關，晚上起夜，前前後後摁了五六個開關才把燈開啟，一點睡意都沒了，-0.5不冤吧；再說洗澡，浴缸很淺，站在裡面淋浴完後外面地上全是水，只能墊個浴巾之類的跳到房間穿衣，還好我和娃她媽還沒老到跳不動。第二天晚上一家三口沒一個人有洗澡的興致，這個-1算客氣了吧 4、最後來說早餐，剛進去服務員一把攔住，娃身高超一米二，要全價一百九，雖然肉痛，一想五星級飯店早餐應該值這個價，就去付了，一個男領班模樣的帶到一邊，讓付九十現金就可以了。付完了還竊喜佔了便宜，等端了盤子找吃的時才發現品種少的可憐。娃之前在四星都能吃到的冰激淋這根本找不到。還沒吃完娃就和我們嘟噥，明天打死也不來吃。-1算理智了吧 5、我要這麼給您減完了，您估計會以為我挑剔，故意找碴。這麼來說吧，稍微讓我們滿意的就是空調溫度了，還有禮賓部寄存行李的服務員了。讓我暫時忘卻你們是五星級飯店吧，這裡給您+0.5 6、沒有比較就沒有差距，遠的不說，這次來北京住五晚，前兩天是紅杉假日酒店，我給評了4.8，和您比起來，除了地段外，您就沒哪項能落了好的。人家酒店剛入住時也說了小孩半價早餐，可覺得過年早餐品種不夠豐富，就免了娃的費用。諷刺的是，我們一家尤其是娃覺得紅杉的早餐比您這要豐盛可口，唯一遺憾的是那也沒冰淇淋。 7、您要是就憑地段就能拉紅杉一晚三百的差距，您自個也覺得心安理得。那我提醒您一句：且行且珍惜&#39;, &#39;酒店&#39;), (&#39;酒店很舊 房間很小。入住體驗非常不好。服務還可以。下面是盤門景區，這點不錯。到各景區交通比較方便。&#39;, &#39;酒店&#39;), (&#39;開始以為應該不錯，結果大失所望！！頂多跟100塊的小旅館差不多！各種設施都老舊的要命！連淋雨花灑壞的！最坑的是網速還限制每個房間幾百K!!開個網頁都要五分鐘好嗎？真破酒店！&#39;, &#39;酒店&#39;), (&#39;我們去鹽城的時候那裡的最低氣溫只有4度，晚上冷得要死，居然還不開空調，投訴到酒店客房部，得到的答覆是現在還沒有領導指示需要開暖氣，如果冷到話可以多給一床被子，太可憐了。。。&#39;, &#39;酒店&#39;), (&#39;房間很小，整體設施老化，和四星的差距很大。毛巾太破舊了。早餐很簡陋。房間隔音很差，隔兩間房間的聲音都聽得見。晚上有人在走廊裡大聲喧譁很久，也沒有人來勸止。比不上以前入住的附近的經濟型酒店。下次不會入住了。&#39;, &#39;酒店&#39;), (&#39;我感覺不行。。。價效比很差。不知道是銀川都這樣還是怎麼的！&#39;, &#39;酒店&#39;), (&#39;房間時間長，進去有點異味！服務員是不是不夠用啊！我在一樓找了半個小時以上才找到自己房間，想找個服務員問一下都找不到，總之不推薦！&#39;, &#39;酒店&#39;), (&#39;老人小孩一大家族聚會，選在吳宮泛太平洋，以為新加坡品牌一定很不錯，沒想到11點30分到前臺，大堂裡擠滿了人，我們所有人都託著大大的行李箱站著，還有的抱著小嬰兒，排隊辦理入住，足足等了1個多小時，期間還被前臺張悅（自稱是當天的值班經理，和另一位自稱值班經理有重複，不知誰是誰非）冷嘲熱諷，總之等待的客人都很不高興，前臺速度太慢，一會兒進裡面問問，一會兒去那裡問問，很混亂的感覺！我們的房間等到4點還沒拿到房！ 房間裡的床品有發黴潮溼的味道，絕對不像5星級，家人們都說不如3星的！ 排隊吃早飯排了半小時，也算是醉了，而且沒什麼東西吃，問有沒有小餛飩，煎荷包蛋的廚師說有，但要點單45元一碗，沒聽說過自助餐廳裡有收費的專案，要麼說沒有，再說一碗小餛飩要45元？邊上聽著的客人直搖頭！ 沒辦法承接這麼多客流，就不要接，入住體驗很不好！勸上海的朋友不要被宣傳圖片和文字忽悠了，起碼旺季真心不咋的，出入不方便，電梯不是直達的，必須經過前臺才能到底，總之體驗非常差，每次去蘇州都住園區那裡，老城區第一次體驗，不好，不會再去！&#39;, &#39;酒店&#39;)] . In the make_doc function above, we hardcoded the string names of text classes, that is, POSITIVE and NEGATIVE. That was not a good option since the function cannot be reused in other cases. So I&#39;d like a general function that works just as well even if we don&#39;t know in advance how many text classes there are in the dataset and what their string names are. After doing some experiments, I finally came up with the make_docs_multiclass function, which does the job. The trick here is to create the label_dict dictionary with every unique class name as keys and False as their default values for every Doc object in the for-loop. Then we assign label_dict to the cats attribute of every Doc object, that is, doc.cats = label_dict. At last, we update the value of a class in label_dict to True only when that is the class of the Doc object in question. . unique_labels = df.cat.unique().tolist() def make_docs_multiclass(data): &quot;&quot;&quot; this will take a list of texts and labels and transform them in spacy documents texts: List(str) labels: List(labels) returns: List(spacy.Doc.doc) &quot;&quot;&quot; docs = [] # nlp.pipe([texts]) is way faster than running nlp(text) for each text # as_tuples allows us to pass in a tuple, the first one is treated as text # the second one will get returned as it is. for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)): label_dict = {label: False for label in unique_labels} # we need to set the (text)cat(egory) for each document doc.cats = label_dict doc.cats[label] = True # put them into a nice list docs.append(doc) return docs . Then we split the dataset, convert it to sapCy format, save the converted data, and train a model with CLI, just like we did earlier. . from sklearn.model_selection import train_test_split train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1) . valid_docs = make_docs_multiclass(valid_data) doc_bin = DocBin(docs=valid_docs) doc_bin.to_disk(&quot;./data/valid.spacy&quot;) . . train_docs = make_docs_multiclass(train_data) doc_bin = DocBin(docs=train_docs) doc_bin.to_disk(&quot;./data/train.spacy&quot;) . . source = &quot;/content/data&quot; dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-product-cat-model/&quot; !cp -R {source} {dest} . After training, we got an overall score of 89, which is pretty awesome. . !python -m spacy train ./config.cfg --output ./output-multiclass . 2021-03-05 03:12:00.200786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 ✔ Created output directory: output-multiclass ℹ Using CPU =========================== Initializing pipeline =========================== Set up nlp object from config Pipeline: [&#39;tok2vec&#39;, &#39;textcat&#39;] Created vocabulary Finished initializing nlp object Initialized pipeline components: [&#39;tok2vec&#39;, &#39;textcat&#39;] ✔ Initialized pipeline ============================= Training pipeline ============================= ℹ Pipeline: [&#39;tok2vec&#39;, &#39;textcat&#39;] ℹ Initial learn rate: 0.001 E # LOSS TOK2VEC LOSS TEXTCAT CATS_SCORE SCORE - 0 0 0.00 0.09 0.00 0.00 0 200 0.00 45.82 37.83 0.38 0 400 0.00 14.59 63.48 0.63 0 600 0.00 12.88 71.83 0.72 0 800 0.00 12.18 76.29 0.76 0 1000 0.00 5.34 78.87 0.79 0 1200 0.00 3.11 80.73 0.81 0 1400 0.00 4.12 82.13 0.82 0 1600 0.00 1.41 82.99 0.83 0 1800 0.00 0.76 83.86 0.84 0 2000 0.00 0.58 84.57 0.85 0 2200 0.00 0.40 85.13 0.85 0 2400 0.00 0.22 85.54 0.86 0 2600 0.00 0.24 86.10 0.86 0 2800 0.00 0.17 86.51 0.87 0 3000 0.00 0.30 86.82 0.87 0 3200 0.00 0.20 87.06 0.87 0 3400 0.00 0.26 87.13 0.87 0 3600 0.00 0.14 87.48 0.87 1 3800 0.00 0.13 87.73 0.88 1 4000 0.00 0.13 87.80 0.88 1 4200 0.00 0.31 87.80 0.88 1 4400 0.00 0.12 88.07 0.88 1 4600 0.00 0.25 88.07 0.88 1 4800 0.00 0.26 87.97 0.88 1 5000 0.00 0.10 88.25 0.88 1 5200 0.00 0.20 88.27 0.88 1 5400 0.00 0.15 88.39 0.88 1 5600 0.00 0.13 88.51 0.89 1 5800 0.00 0.11 88.53 0.89 2 6000 0.00 0.16 88.77 0.89 2 6200 0.00 0.15 88.39 0.88 2 6400 0.00 0.15 88.67 0.89 2 6600 0.00 0.11 88.77 0.89 2 6800 0.00 0.11 88.65 0.89 2 7000 0.00 0.14 88.73 0.89 2 7200 0.00 0.11 88.69 0.89 2 7400 0.00 0.17 88.67 0.89 2 7600 0.00 0.17 88.58 0.89 2 7800 0.00 0.11 88.68 0.89 2 8000 0.00 0.28 88.96 0.89 3 8200 0.00 0.21 88.93 0.89 3 8400 0.00 0.11 88.91 0.89 3 8600 0.00 0.23 88.86 0.89 3 8800 0.00 0.15 88.80 0.89 3 9000 0.00 0.11 88.95 0.89 3 9200 0.00 0.30 88.90 0.89 3 9400 0.00 0.11 89.11 0.89 3 9600 0.00 0.12 89.14 0.89 3 9800 0.00 0.12 89.09 0.89 3 10000 0.00 0.16 88.95 0.89 3 10200 0.00 0.17 89.01 0.89 4 10400 0.00 0.10 89.07 0.89 4 10600 0.00 0.12 89.14 0.89 4 10800 0.00 0.12 89.15 0.89 4 11000 0.00 0.11 89.09 0.89 4 11200 0.00 0.13 89.03 0.89 4 11400 0.00 0.09 89.11 0.89 4 11600 0.00 0.11 89.28 0.89 4 11800 0.00 0.18 89.15 0.89 4 12000 0.00 0.14 89.33 0.89 4 12200 0.00 0.09 89.22 0.89 4 12400 0.00 0.15 89.22 0.89 5 12600 0.00 0.10 89.08 0.89 5 12800 0.00 0.11 89.17 0.89 5 13000 0.00 0.15 89.24 0.89 5 13200 0.00 0.12 89.20 0.89 5 13400 0.00 0.12 89.08 0.89 5 13600 0.00 0.15 89.18 0.89 ✔ Saved pipeline to output directory output-multiclass/model-last . . Here&#39;s the py file for testing the multiclass classification model. . %%writefile test_input_multiclass.py import spacy # load the best model from training nlp = spacy.load(&quot;output-multiclass/model-best&quot;) text = &quot;&quot; print(&quot;type : &#39;quit&#39; to exit&quot;) # predict the product category until someone writes quit while text != &quot;quit&quot;: text = input(&quot;Please enter a review here: &quot;) doc = nlp(text) print(doc.cats) . Writing test_input_multiclass.py . Again, let&#39;s print out 10 reviews in valid_data for the purpose of testing. . valid_data[:10] . [(&#39;和以前買的不一樣！以前用的特別滑！用的也少！這次買的不僅用的東西多！而且還頭皮癢！有頭屑！&#39;, &#39;洗髮水&#39;), (&#39;不好意思我貨未收到，我想起訴你們！&#39;, &#39;衣服&#39;), (&#39;寶貝收到了，是我想要的版型，質量十分好，這條褲子的顏色跟圖片上一樣，是我想要的顏色，總之，是一次很愉快的購物&#39;, &#39;衣服&#39;), (&#39;一點也不甜不清脆，只能榨汁了。&#39;, &#39;水果&#39;), (&#39;蘋果小，但味道不錯，給個好評吧&#39;, &#39;水果&#39;), (&#39;平板收到，很滿意和想像的一樣，沒失望第一時間體驗中，稍後再評&#39;, &#39;平板&#39;), (&#39;說好的樂視會員一年 送哪去了 差評&#39;, &#39;平板&#39;), (&#39;相當完美啊 好東西&#39;, &#39;平板&#39;), (&#39;連生產生日都沒有，真不知道是不是真的。&#39;, &#39;洗髮水&#39;), (&#39;蘋果一如既往得好，脆、甜、水分足，推薦購買哦奧～&#39;, &#39;水果&#39;)] . The model works as expected. I intentionally tested the ambiguous review, 這個牌子值得信賴 (This brand is trustworthy.), which could have been a review for tablets, clothing, or shampoo. And our model gave top three scores to precisely these three classes! . !python test_input_multiclass.py . . 2021-03-05 05:07:28.215017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 type : &#39;quit&#39; to exit Please enter a review here: 和以前買的不一樣！以前用的特別滑！用的也少！這次買的不僅用的東西多！而且還頭皮癢！有頭屑！ {&#39;平板&#39;: 2.058545214822516e-05, &#39;水果&#39;: 1.1623805221461225e-05, &#39;洗髮水&#39;: 0.9999675750732422, &#39;衣服&#39;: 2.6233973926537146e-07, &#39;酒店&#39;: 7.811570945648327e-09} Please enter a review here: 蘋果小，但味道不錯，給個好評吧 {&#39;平板&#39;: 0.0010689852060750127, &#39;水果&#39;: 0.9964427351951599, &#39;洗髮水&#39;: 0.00197225296869874, &#39;衣服&#39;: 0.00019674153008963913, &#39;酒店&#39;: 0.0003193040902260691} Please enter a review here: 脆、甜、水分足，推薦購買哦奧～ {&#39;平板&#39;: 0.00028378094430081546, &#39;水果&#39;: 0.9972990155220032, &#39;洗髮水&#39;: 0.0011949185281991959, &#39;衣服&#39;: 0.0006593622965738177, &#39;酒店&#39;: 0.0005628817598335445} Please enter a review here: 收到，很滿意和想像的一樣，沒失望第一時間體驗中，稍後再評 {&#39;平板&#39;: 0.5766726732254028, &#39;水果&#39;: 0.07788817584514618, &#39;洗髮水&#39;: 0.16239948570728302, &#39;衣服&#39;: 0.16160377860069275, &#39;酒店&#39;: 0.0214359350502491} Please enter a review here: 連生產生日都沒有，真不知道是不是真的。 {&#39;平板&#39;: 0.23529699444770813, &#39;水果&#39;: 0.032520271837711334, &#39;洗髮水&#39;: 0.6238041520118713, &#39;衣服&#39;: 0.017157811671495438, &#39;酒店&#39;: 0.09122073650360107} Please enter a review here: 這個牌子值得信賴， {&#39;平板&#39;: 0.19572772085666656, &#39;水果&#39;: 0.10571669787168503, &#39;洗髮水&#39;: 0.362021267414093, &#39;衣服&#39;: 0.29673027992248535, &#39;酒店&#39;: 0.03980398178100586} Please enter a review here: quit {&#39;平板&#39;: 0.25125113129615784, &#39;水果&#39;: 0.061232421547174454, &#39;洗髮水&#39;: 0.27915307879447937, &#39;衣服&#39;: 0.12709283828735352, &#39;酒店&#39;: 0.2812705338001251} . Now I can rest assured and save the trained model to Google Drive. . source = &quot;/content/output-multiclass/model-best&quot; dest = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-product-cat-model/&quot; !cp -R {source} {dest} . Checking model performance . I thought I&#39;d have to write custom functions to evaluate the performance of our classification models. But it turns out that spaCy, our unfailingly considerate friend, has done it for us under the hood. Performance metrics are hidden in the meta.json file under the model-best directory. Here&#39;s the content of the meta.json file for our multiclass classification model. . import json meta_path = &quot;/content/drive/MyDrive/Python/NLP/shopping_comments/spaCy-text-classification/spaCy-product-cat-model/model-best/meta.json&quot; with open(meta_path) as json_file: metrics = json.load(json_file) metrics . {&#39;author&#39;: &#39;&#39;, &#39;components&#39;: [&#39;tok2vec&#39;, &#39;textcat&#39;], &#39;description&#39;: &#39;&#39;, &#39;disabled&#39;: [], &#39;email&#39;: &#39;&#39;, &#39;labels&#39;: {&#39;textcat&#39;: [&#39;平板&#39;, &#39;水果&#39;, &#39;洗髮水&#39;, &#39;衣服&#39;, &#39;酒店&#39;], &#39;tok2vec&#39;: []}, &#39;lang&#39;: &#39;zh&#39;, &#39;license&#39;: &#39;&#39;, &#39;name&#39;: &#39;pipeline&#39;, &#39;performance&#39;: {&#39;cats_f_per_type&#39;: {&#39;平板&#39;: {&#39;f&#39;: 0.8329853862, &#39;p&#39;: 0.9140893471, &#39;r&#39;: 0.7651006711}, &#39;水果&#39;: {&#39;f&#39;: 0.9107981221, &#39;p&#39;: 0.9525368249, &#39;r&#39;: 0.8725637181}, &#39;洗髮水&#39;: {&#39;f&#39;: 0.8430637386, &#39;p&#39;: 0.8827818284, &#39;r&#39;: 0.8067657611}, &#39;衣服&#39;: {&#39;f&#39;: 0.8985658409, &#39;p&#39;: 0.9303455724, &#39;r&#39;: 0.868885527}, &#39;酒店&#39;: {&#39;f&#39;: 0.9810741688, &#39;p&#39;: 0.9932677369, &#39;r&#39;: 0.9691763517}}, &#39;cats_macro_auc&#39;: 0.9846299582, &#39;cats_macro_auc_per_type&#39;: 0.0, &#39;cats_macro_f&#39;: 0.8932974513, &#39;cats_macro_p&#39;: 0.9346042619, &#39;cats_macro_r&#39;: 0.8564984058, &#39;cats_micro_f&#39;: 0.8939148603, &#39;cats_micro_p&#39;: 0.9357025697, &#39;cats_micro_r&#39;: 0.8557, &#39;cats_score&#39;: 0.8932974513, &#39;cats_score_desc&#39;: &#39;macro F&#39;, &#39;textcat_loss&#39;: 0.1365536493, &#39;tok2vec_loss&#39;: 0.0}, &#39;pipeline&#39;: [&#39;tok2vec&#39;, &#39;textcat&#39;], &#39;spacy_git_version&#39;: &#39;f4f46b617&#39;, &#39;spacy_version&#39;: &#39;&gt;=3.0.3,&lt;3.1.0&#39;, &#39;url&#39;: &#39;&#39;, &#39;vectors&#39;: {&#39;keys&#39;: 0, &#39;name&#39;: None, &#39;vectors&#39;: 0, &#39;width&#39;: 0}, &#39;version&#39;: &#39;0.0.0&#39;} . Specifically, values of the performance key are what we&#39;re looking for. . performance = metrics[&#39;performance&#39;] performance . {&#39;cats_f_per_type&#39;: {&#39;平板&#39;: {&#39;f&#39;: 0.8329853862, &#39;p&#39;: 0.9140893471, &#39;r&#39;: 0.7651006711}, &#39;水果&#39;: {&#39;f&#39;: 0.9107981221, &#39;p&#39;: 0.9525368249, &#39;r&#39;: 0.8725637181}, &#39;洗髮水&#39;: {&#39;f&#39;: 0.8430637386, &#39;p&#39;: 0.8827818284, &#39;r&#39;: 0.8067657611}, &#39;衣服&#39;: {&#39;f&#39;: 0.8985658409, &#39;p&#39;: 0.9303455724, &#39;r&#39;: 0.868885527}, &#39;酒店&#39;: {&#39;f&#39;: 0.9810741688, &#39;p&#39;: 0.9932677369, &#39;r&#39;: 0.9691763517}}, &#39;cats_macro_auc&#39;: 0.9846299582, &#39;cats_macro_auc_per_type&#39;: 0.0, &#39;cats_macro_f&#39;: 0.8932974513, &#39;cats_macro_p&#39;: 0.9346042619, &#39;cats_macro_r&#39;: 0.8564984058, &#39;cats_micro_f&#39;: 0.8939148603, &#39;cats_micro_p&#39;: 0.9357025697, &#39;cats_micro_r&#39;: 0.8557, &#39;cats_score&#39;: 0.8932974513, &#39;cats_score_desc&#39;: &#39;macro F&#39;, &#39;textcat_loss&#39;: 0.1365536493, &#39;tok2vec_loss&#39;: 0.0} . Let&#39;s make a nice DataFrame object out of the metrics of the overall performance. . score = performance[&#39;cats_score&#39;] auc = performance[&#39;cats_macro_auc&#39;] f1 = performance[&#39;cats_macro_f&#39;] precision = performance[&#39;cats_macro_p&#39;] recall = performance[&#39;cats_macro_r&#39;] overall_dict = {&#39;score&#39;: score, &#39;precision&#39;: precision, &#39;recall&#39;: recall, &#39;F1&#39;: f1, &#39;AUC&#39;: auc} overall_df = pd.DataFrame(overall_dict, index=[0]) overall_df . score precision recall F1 AUC . 0 0.893297 | 0.934604 | 0.856498 | 0.893297 | 0.98463 | . We can also break down the metrics into specific categories, which are saved as the values of the cats_f_per_type key of performance. . per_cat_dict = performance[&#39;cats_f_per_type&#39;] per_cat_df = pd.DataFrame(per_cat_dict) per_cat_df . 平板 水果 洗髮水 衣服 酒店 . p 0.914089 | 0.952537 | 0.882782 | 0.930346 | 0.993268 | . r 0.765101 | 0.872564 | 0.806766 | 0.868886 | 0.969176 | . f 0.832985 | 0.910798 | 0.843064 | 0.898566 | 0.981074 | . Previously, I also used the fasttext library to train a multiclass classification model on the same dataset. Here&#39;re the values of the parameters I set up. And the overall accuracy is 0.886, which pretty close to our spaCy model. . {&#39;dim&#39;: 200, &#39;epoch&#39;: 5, &#39;loss&#39;: &#39;softmax&#39;, &#39;lr&#39;: 0.1, &#39;test_size&#39;: 0.2, &#39;window&#39;: 5, &#39;wordNgrams&#39;: 1} . And here&#39;s the breakdown for each category. One noticeable difference between the spaCy and fastText model is that the spaCy model has the highest scores in the HOTEL category (酒店) across all three metrics whereas the fastText model consistently performs the best in the TABLET category (平板) in terms of all three metrics. I wonder why... . . cat 平板 水果 洗髮水 衣服 酒店 . p 0.976 | 0.892 | 0.851 | 0.899 | 0.816 | . r 0.972 | 0.908 | 0.832 | 0.885 | 0.836 | . f 0.974 | 0.9 | 0.841 | 0.892 | 0.825 | . Recap . Like fastText, spaCy is good for training text classification models, and this task has become a no-brainer with the release of spaCy v3.0. Even in cases where both methods work equally well, spaCy still has an edge over fastText. That is, with spaCy you don&#39;t need to deal with text preprocessing or tokenization. So, watch this space-y! .",
            "url": "https://howard-haowen.github.io/blog.ai/spacy/text-classification/sentiment-analysis/customer-reviews/fasttext/facets/2021/03/12/Classifying-customer-reviews-with-spaCy-v3.html",
            "relUrl": "/spacy/text-classification/sentiment-analysis/customer-reviews/fasttext/facets/2021/03/12/Classifying-customer-reviews-with-spaCy-v3.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Adding a custom tokenizer to spaCy and extracting keywords",
            "content": ". Intro . spaCy is an industrial-strength natural language processing library in Python, and supports multiple human languages, including Chinese. For segmenting Chinese texts into words, spaCy uses Jieba or PKUSeg under the hood. However, neither of them beats CKIP Transformers in accuracy when it comes to traditional Chinese (see my previous post for a comparison). So I&#39;ll show how to plug in CKIP Transformers to spaCy to get the best out of both. . For the purpose of demonstration, I&#39;ll situate this integration in a pipeline for extracting keywords from texts. Compared with other NLP tasks, keyword extraction is a relatively easy job. TextRank and RAKE seem to be among the most widely adopted algorithms for keyword extraction. I tried most of the methods mentioned in this article, but there doesn&#39;t seem to be any easy-peasy implementation of TextRank or RAKE that produces decent results for traditional Chinese texts. So the first part of this post walks through a pipeline that actually works, and the second part records other methods that failed. I included the second part because I believe in this quote: . “We learn wisdom from failure much more than from success. We often discover what will do, by finding out what will not do; and probably he who never made a mistake never made a discovery.” ― Samuel Smiles . . Note: TextRank is based on Google&#8217;s PageRank, which is used to compute the rank of webpages. This article on Natural Language Processing for Hackers demonstrates the connection between the two. From it I learned a tidbit: I always assumed that Page as in PageRank refers to webpages, but it turns out to be the family name of Larry Page, the creator of PageRank. . Working pipeline . Set variables . Let&#39;s start with defining two variables that users of our keyword extraction program might want to modify: CUSTOM_STOPWORDS for a list of words that users definitely hope to exclude from keyword candidates and KW_NUM for the number of keywords that they&#39;d like to extract from a document. . CUSTOM_STOPWORDS = [ &quot;民眾&quot;,&quot;朋友&quot;,&quot;市民&quot;,&quot;人數&quot;, &quot;全民&quot;,&quot;人員&quot;,&quot;人士&quot;,&quot;里民&quot;, &quot;影本&quot;,&quot;系統&quot;, &quot;項目&quot;, &quot;證件&quot;, &quot;資格&quot;,&quot;公民&quot;, &quot;對象&quot;,&quot;個人&quot;, ] KW_NUM = 10 . Preprocess texts . I took an announcement from Land Administration Bureau of Kaohsiung City Goverment as a sample text, but you can basically take any text in traditional Chinese to test the program. . . Tip: To run the program with your own text, follow the following steps: . Click on Open in Colab at the upper right corner of this page. | Click on File and then Save a copy in Drive. | Replace the following text with your own text. | Click on Runtime and then Run all. | Go to the section Put it together to see the outcome. | raw_text = &#39;&#39;&#39; 市府地政局109年度第4季開發區土地標售，共計推出8標9筆優質建地，訂於109年12月16日開標，合計總底價12 億4049萬6164 元。 第93期重劃區，原為國軍眷村，緊鄰國定古蹟-「原日本海軍鳳山無線電信所」，市府為保存古蹟同時活化眷村遷移後土地，以重劃方式整體開發，新闢住宅區、道路、公園及停車場，使本區具有歷史文化內涵與綠色休閒特色，生活機能更加健全。地政局首次推出1筆大面積土地，面積約2160坪，地形方整，雙面臨路，利於規劃興建景觀大樓，附近有市場、學校、公園及大東文化園區，距捷運大東站、鳳山國中站及鳳山火車站僅數分鐘車程，交通四通八達，因土地稀少性及區位條件絕佳，勢必成為投資人追逐焦點。 第87期重劃區，位於省道台1線旁，鄰近捷運南岡山站，重劃後擁有完善的道路系統、公園綠地及毗鄰醒村懷舊文化景觀建築群，具備優質居住環境及交通便捷要件，地政局一推出土地標售，即掀起搶標熱潮，本季再釋出1筆面積約93坪土地，臨20米介壽路及鵬程東路，附近有岡山文化中心、兆湘國小、公13、公14、陽明公園及劉厝公園，區位條件佳，投資人準備搶進！ 第77期市地重劃區，位於鳳山區快速道路省道台88線旁，近中山高五甲系統交流道，近年推出土地標售皆順利完銷。本季再推出2筆土地，其中1筆面積約526坪，臨保華一路，適合商業使用；1筆面積107坪，位於代德三街，自用投資兩相宜。 高雄大學區段徵收區，為北高雄優質文教特區，優質居住環境，吸引投資人進駐，本季再推出2標2筆土地，其中1筆第三種商業區土地，面積約639坪，位於大學26街，近高雄大學正門及萬坪藍田公園，地形方正，使用強度高，適合興建優質住宅大樓；另1筆住三用地，面積約379坪，臨28米藍昌路，近高雄大學及中山高中，交通便捷。 另第37期重劃區及前大寮農地重劃區各推出1至2筆土地，價格合理。 第4季土地標售作業於109年12月1日公告，投資大眾可前往地政局土地開發處土地處分科索取標售海報及標單，或直接上網高雄房地產億年旺網站、地政局及土地開發處網站查詢下載相關資料，在期限前完成投標，另再提醒投標人，本年度已更新投標單格式，投標大眾請注意應以新式投標單投標以免投標無效作廢。 為配合防疫需求，本季開標作業除於地政局第一會議室辦理外，另將於地政局Facebook粉絲專頁同步直播，請大眾多加利用。 洽詢專線：(07)3373451或(07)3314942 高雄房地產億年旺網站（網址：http://eland.kcg.gov.tw/） 高雄市政府地政局網站（網址：http://landp.kcg.gov.tw/） 高雄市政府地政局土地開發處網站（網址：http://landevp.kcg.gov.tw/）　 &#39;&#39;&#39; raw_text[-300:] . &#39;及土地開發處網站查詢下載相關資料，在期限前完成投標，另再提醒投標人，本年度已更新投標單格式，投標大眾請注意應以新式投標單投標以免投標無效作廢。 n n n n為配合防疫需求，本季開標作業除於地政局第一會議室辦理外，另將於地政局Facebook粉絲專頁同步直播，請大眾多加利用。 n n n n洽詢專線：(07)3373451或(07)3314942 n n高雄房地產億年旺網站（網址：http://eland.kcg.gov.tw/） n n高雄市政府地政局網站（網址：http://landp.kcg.gov.tw/） n n高雄市政府地政局土地開發處網站（網址：http://landevp.kcg.gov.tw/） u3000 n&#39; . I find this lightweight library nlp2 quite handy for text cleaning. The clean_all function removes URL links, HTML elements, and unused tags. . . Note: I want to give a shoutout to Eric Lam, who created nlp2 and other useful NLP tools such as NLPrep, TFkit, and nlp2go. . !pip install nlp2 from nlp2 import clean_all . After cleaning, our sample text looks like this. Notice that all the URL links are gone now. . text = clean_all(raw_text) text[-300:] . &#39;合理。 n n n n第4季土地標售作業於109年12月1日公告，投資大眾可前往地政局土地開發處土地處分科索取標售海報及標單，或直接上網高雄房地產億年旺網站、地政局及土地開發處網站查詢下載相關資料，在期限前完成投標，另再提醒投標人，本年度已更新投標單格式，投標大眾請注意應以新式投標單投標以免投標無效作廢。 n n n n為配合防疫需求，本季開標作業除於地政局第一會議室辦理外，另將於地政局Facebook粉絲專頁同步直播，請大眾多加利用。 n n n n洽詢專線： 3373451或 3314942 n n高雄房地產億年旺網站（網址： ） n n高雄市政府地政局網站（網址： ） n n高雄市政府地政局土地開發處網站（網址： ）&#39; . Install spacy and ckip-transformers . !pip install -U pip setuptools wheel !pip install -U spacy !python -m spacy download zh_core_web_sm . !pip install -U ckip-transformers . Tokenize texts with ckip-transformers . Let&#39;s create a driver for word segmentation and one for parts of speech. CKIP Transformers also has a built-in driver for named entity recognition, i.e. CkipNerChunker. But we won&#39;t use it here. . . Tip: By default, CPU is used. If you want to use GPU to speed up word segmentation, initialize ws_driver this way instead: ws_driver = CkipWordSegmenter(device=-1) . from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger ws_driver = CkipWordSegmenter() pos_driver = CkipPosTagger() . . Important: Make sure that the input to ws_driver() is a list even if you&#8217;re only dealing with a single text. Otherwise, words won&#8217;t be properly segmented. Notice that the input to pos_driver() is the output of ws_driver(). . ws = ws_driver([text]) pos = pos_driver(ws) . Here&#39;re the segmented tokens. . tokens = ws[0] print(tokens) . [&#39;市府&#39;, &#39;地政局&#39;, &#39;109年度&#39;, &#39;第4&#39;, &#39;季&#39;, &#39;開發區&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;，&#39;, &#39;共計&#39;, &#39;推出&#39;, &#39;8&#39;, &#39;標&#39;, &#39;9&#39;, &#39;筆&#39;, &#39;優質&#39;, &#39;建地&#39;, &#39;，&#39;, &#39;訂&#39;, &#39;於&#39;, &#39;109年&#39;, &#39;12月&#39;, &#39;16日&#39;, &#39;開標&#39;, &#39;，&#39;, &#39;合計&#39;, &#39;總底價&#39;, &#39;12 億&#39;, &#39;4049萬&#39;, &#39;6164 &#39;, &#39;元&#39;, &#39;。&#39;, &#39; n n n n&#39;, &#39;第93&#39;, &#39;期&#39;, &#39;重劃區&#39;, &#39;，&#39;, &#39;原&#39;, &#39;為&#39;, &#39;國軍&#39;, &#39;眷村&#39;, &#39;，&#39;, &#39;緊鄰&#39;, &#39;國定&#39;, &#39;古蹟&#39;, &#39;-&#39;, &#39;「&#39;, &#39;原&#39;, &#39;日本&#39;, &#39;海軍&#39;, &#39;鳳山&#39;, &#39;無線&#39;, &#39;電信所&#39;, &#39;」&#39;, &#39;，&#39;, &#39;市府&#39;, &#39;為&#39;, &#39;保存&#39;, &#39;古蹟&#39;, &#39;同時&#39;, &#39;活化&#39;, &#39;眷村&#39;, &#39;遷移&#39;, &#39;後&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;以&#39;, &#39;重劃&#39;, &#39;方式&#39;, &#39;整體&#39;, &#39;開發&#39;, &#39;，&#39;, &#39;新&#39;, &#39;闢&#39;, &#39;住宅區&#39;, &#39;、&#39;, &#39;道路&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;停車場&#39;, &#39;，&#39;, &#39;使&#39;, &#39;本&#39;, &#39;區&#39;, &#39;具有&#39;, &#39;歷史&#39;, &#39;文化&#39;, &#39;內涵&#39;, &#39;與&#39;, &#39;綠色&#39;, &#39;休閒&#39;, &#39;特色&#39;, &#39;，&#39;, &#39;生活&#39;, &#39;機能&#39;, &#39;更加&#39;, &#39;健全&#39;, &#39;。&#39;, &#39;地政局&#39;, &#39;首次&#39;, &#39;推出&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;大&#39;, &#39;面積&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;2160&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;地形&#39;, &#39;方整&#39;, &#39;，&#39;, &#39;雙面&#39;, &#39;臨&#39;, &#39;路&#39;, &#39;，&#39;, &#39;利於&#39;, &#39;規劃&#39;, &#39;興建&#39;, &#39;景觀&#39;, &#39;大樓&#39;, &#39;，&#39;, &#39;附近&#39;, &#39;有&#39;, &#39;市場&#39;, &#39;、&#39;, &#39;學校&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;大東&#39;, &#39;文化&#39;, &#39;園區&#39;, &#39;，&#39;, &#39;距&#39;, &#39;捷運&#39;, &#39;大東站&#39;, &#39;、&#39;, &#39;鳳山&#39;, &#39;國中站&#39;, &#39;及&#39;, &#39;鳳山&#39;, &#39;火車站&#39;, &#39;僅&#39;, &#39;數&#39;, &#39;分鐘&#39;, &#39;車程&#39;, &#39;，&#39;, &#39;交通&#39;, &#39;四通八達&#39;, &#39;，&#39;, &#39;因&#39;, &#39;土地&#39;, &#39;稀少性&#39;, &#39;及&#39;, &#39;區位&#39;, &#39;條件&#39;, &#39;絕佳&#39;, &#39;，&#39;, &#39;勢必&#39;, &#39;成為&#39;, &#39;投資人&#39;, &#39;追逐&#39;, &#39;焦點&#39;, &#39;。&#39;, &#39; n n n n&#39;, &#39;第87&#39;, &#39;期&#39;, &#39;重劃區&#39;, &#39;，&#39;, &#39;位於&#39;, &#39;省道&#39;, &#39;台1線&#39;, &#39;旁&#39;, &#39;，&#39;, &#39;鄰近&#39;, &#39;捷運&#39;, &#39;南&#39;, &#39;岡山站&#39;, &#39;，&#39;, &#39;重劃&#39;, &#39;後&#39;, &#39;擁有&#39;, &#39;完善&#39;, &#39;的&#39;, &#39;道路&#39;, &#39;系統&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;綠地&#39;, &#39;及&#39;, &#39;毗鄰&#39;, &#39;醒村&#39;, &#39;懷舊&#39;, &#39;文化&#39;, &#39;景觀&#39;, &#39;建築群&#39;, &#39;，&#39;, &#39;具備&#39;, &#39;優質&#39;, &#39;居住&#39;, &#39;環境&#39;, &#39;及&#39;, &#39;交通&#39;, &#39;便捷&#39;, &#39;要件&#39;, &#39;，&#39;, &#39;地政局&#39;, &#39;一&#39;, &#39;推出&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;，&#39;, &#39;即&#39;, &#39;掀起&#39;, &#39;搶標&#39;, &#39;熱潮&#39;, &#39;，&#39;, &#39;本&#39;, &#39;季&#39;, &#39;再&#39;, &#39;釋出&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;93&#39;, &#39;坪&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;臨&#39;, &#39;20&#39;, &#39;米&#39;, &#39;介壽路&#39;, &#39;及&#39;, &#39;鵬程東路&#39;, &#39;，&#39;, &#39;附近&#39;, &#39;有&#39;, &#39;岡山&#39;, &#39;文化&#39;, &#39;中心&#39;, &#39;、&#39;, &#39;兆湘&#39;, &#39;國小&#39;, &#39;、&#39;, &#39;公13&#39;, &#39;、&#39;, &#39;公14&#39;, &#39;、&#39;, &#39;陽明&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;劉厝&#39;, &#39;公園&#39;, &#39;，&#39;, &#39;區位&#39;, &#39;條件&#39;, &#39;佳&#39;, &#39;，&#39;, &#39;投資人&#39;, &#39;準備&#39;, &#39;搶進&#39;, &#39;！&#39;, &#39; n n n n&#39;, &#39;第77&#39;, &#39;期&#39;, &#39;市地&#39;, &#39;重劃區&#39;, &#39;，&#39;, &#39;位於&#39;, &#39;鳳山區&#39;, &#39;快速&#39;, &#39;道路&#39;, &#39;省道&#39;, &#39;台88&#39;, &#39;線&#39;, &#39;旁&#39;, &#39;，&#39;, &#39;近&#39;, &#39;中山高&#39;, &#39;五甲&#39;, &#39;系統&#39;, &#39;交流道&#39;, &#39;，&#39;, &#39;近年&#39;, &#39;推出&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;皆&#39;, &#39;順利&#39;, &#39;完銷&#39;, &#39;。&#39;, &#39;本&#39;, &#39;季&#39;, &#39;再&#39;, &#39;推出&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;其中&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;526&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;臨&#39;, &#39;保華一路&#39;, &#39;，&#39;, &#39;適合&#39;, &#39;商業&#39;, &#39;使用&#39;, &#39;；&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;面積&#39;, &#39;107&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;位於&#39;, &#39;代德三街&#39;, &#39;，&#39;, &#39;自用&#39;, &#39;投資&#39;, &#39;兩&#39;, &#39;相宜&#39;, &#39;。&#39;, &#39; n n n n&#39;, &#39;高雄&#39;, &#39;大學&#39;, &#39;區段&#39;, &#39;徵收區&#39;, &#39;，&#39;, &#39;為&#39;, &#39;北&#39;, &#39;高雄&#39;, &#39;優質&#39;, &#39;文教&#39;, &#39;特區&#39;, &#39;，&#39;, &#39;優質&#39;, &#39;居住&#39;, &#39;環境&#39;, &#39;，&#39;, &#39;吸引&#39;, &#39;投資人&#39;, &#39;進駐&#39;, &#39;，&#39;, &#39;本&#39;, &#39;季&#39;, &#39;再&#39;, &#39;推出&#39;, &#39;2&#39;, &#39;標&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;其中&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;第三&#39;, &#39;種&#39;, &#39;商業區&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;639&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;位於&#39;, &#39;大學&#39;, &#39;26街&#39;, &#39;，&#39;, &#39;近&#39;, &#39;高雄&#39;, &#39;大學&#39;, &#39;正門&#39;, &#39;及&#39;, &#39;萬&#39;, &#39;坪&#39;, &#39;藍田&#39;, &#39;公園&#39;, &#39;，&#39;, &#39;地形&#39;, &#39;方正&#39;, &#39;，&#39;, &#39;使用&#39;, &#39;強度&#39;, &#39;高&#39;, &#39;，&#39;, &#39;適合&#39;, &#39;興建&#39;, &#39;優質&#39;, &#39;住宅&#39;, &#39;大樓&#39;, &#39;；&#39;, &#39;另&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;住三&#39;, &#39;用地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;379&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;臨&#39;, &#39;28&#39;, &#39;米&#39;, &#39;藍昌路&#39;, &#39;，&#39;, &#39;近&#39;, &#39;高雄&#39;, &#39;大學&#39;, &#39;及&#39;, &#39;中山&#39;, &#39;高中&#39;, &#39;，&#39;, &#39;交通&#39;, &#39;便捷&#39;, &#39;。&#39;, &#39; n n n n&#39;, &#39;另&#39;, &#39;第37&#39;, &#39;期&#39;, &#39;重劃區&#39;, &#39;及&#39;, &#39;前&#39;, &#39;大寮&#39;, &#39;農地&#39;, &#39;重劃區&#39;, &#39;各&#39;, &#39;推出&#39;, &#39;1&#39;, &#39;至&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;價格&#39;, &#39;合理&#39;, &#39;。&#39;, &#39; n n n n&#39;, &#39;第4&#39;, &#39;季&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;作業&#39;, &#39;於&#39;, &#39;109年&#39;, &#39;12月&#39;, &#39;1日&#39;, &#39;公告&#39;, &#39;，&#39;, &#39;投資&#39;, &#39;大眾&#39;, &#39;可&#39;, &#39;前往&#39;, &#39;地政局&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;土地處&#39;, &#39;分科&#39;, &#39;索取&#39;, &#39;標售&#39;, &#39;海報&#39;, &#39;及&#39;, &#39;標單&#39;, &#39;，&#39;, &#39;或&#39;, &#39;直接&#39;, &#39;上網&#39;, &#39;高雄&#39;, &#39;房地產&#39;, &#39;億年旺&#39;, &#39;網站&#39;, &#39;、&#39;, &#39;地政局&#39;, &#39;及&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;網站&#39;, &#39;查詢&#39;, &#39;下載&#39;, &#39;相關&#39;, &#39;資料&#39;, &#39;，&#39;, &#39;在&#39;, &#39;期限&#39;, &#39;前&#39;, &#39;完成&#39;, &#39;投標&#39;, &#39;，&#39;, &#39;另&#39;, &#39;再&#39;, &#39;提醒&#39;, &#39;投標人&#39;, &#39;，&#39;, &#39;本&#39;, &#39;年度&#39;, &#39;已&#39;, &#39;更新&#39;, &#39;投標單&#39;, &#39;格式&#39;, &#39;，&#39;, &#39;投標&#39;, &#39;大眾&#39;, &#39;請&#39;, &#39;注意&#39;, &#39;應&#39;, &#39;以&#39;, &#39;新式&#39;, &#39;投標單&#39;, &#39;投標&#39;, &#39;以免&#39;, &#39;投標&#39;, &#39;無效&#39;, &#39;作廢&#39;, &#39;。&#39;, &#39; n n n n&#39;, &#39;為&#39;, &#39;配合&#39;, &#39;防疫&#39;, &#39;需求&#39;, &#39;，&#39;, &#39;本&#39;, &#39;季&#39;, &#39;開標&#39;, &#39;作業&#39;, &#39;除&#39;, &#39;於&#39;, &#39;地政局&#39;, &#39;第一&#39;, &#39;會議室&#39;, &#39;辦理&#39;, &#39;外&#39;, &#39;，&#39;, &#39;另&#39;, &#39;將&#39;, &#39;於&#39;, &#39;地政局&#39;, &#39;Facebook&#39;, &#39;粉絲&#39;, &#39;專頁&#39;, &#39;同步&#39;, &#39;直播&#39;, &#39;，&#39;, &#39;請&#39;, &#39;大眾&#39;, &#39;多加&#39;, &#39;利用&#39;, &#39;。&#39;, &#39; n n n n&#39;, &#39;洽詢&#39;, &#39;專線&#39;, &#39;：&#39;, &#39; 3373&#39;, &#39;451&#39;, &#39;或&#39;, &#39; 3314942&#39;, &#39; n&#39;, &#39; n&#39;, &#39;高雄&#39;, &#39;房地產&#39;, &#39;億&#39;, &#39;年&#39;, &#39;旺&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39; &#39;, &#39;）&#39;, &#39; n&#39;, &#39; n&#39;, &#39;高雄市&#39;, &#39;政府&#39;, &#39;地政局&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39; &#39;, &#39;）&#39;, &#39; n&#39;, &#39; n&#39;, &#39;高雄市&#39;, &#39;政府&#39;, &#39;地政局&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39; &#39;, &#39;）&#39;] . By contrast, Jieba produced lots of wrongly segmented tokens, which is precisely why we prefer CKIP Transformers. . import jieba print(list(jieba.cut(text))) . Building prefix dict from the default dictionary ... Dumping model to file cache /tmp/jieba.cache Loading model cost 0.958 seconds. Prefix dict has been built successfully. . [&#39;市府&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;109&#39;, &#39;年度&#39;, &#39;第&#39;, &#39;4&#39;, &#39;季開&#39;, &#39;發區&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;，&#39;, &#39;共計&#39;, &#39;推出&#39;, &#39;8&#39;, &#39;標&#39;, &#39;9&#39;, &#39;筆優質&#39;, &#39;建地&#39;, &#39;，&#39;, &#39;訂&#39;, &#39;於&#39;, &#39;109&#39;, &#39;年&#39;, &#39;12&#39;, &#39;月&#39;, &#39;16&#39;, &#39;日&#39;, &#39;開標&#39;, &#39;，&#39;, &#39;合計&#39;, &#39;總底價&#39;, &#39;12&#39;, &#39; &#39;, &#39;億&#39;, &#39;4049&#39;, &#39;萬&#39;, &#39;6164&#39;, &#39; &#39;, &#39;元&#39;, &#39;。&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;第&#39;, &#39;93&#39;, &#39;期重&#39;, &#39;劃區&#39;, &#39;，&#39;, &#39;原為國&#39;, &#39;軍&#39;, &#39;眷村&#39;, &#39;，&#39;, &#39;緊鄰&#39;, &#39;國定&#39;, &#39;古&#39;, &#39;蹟&#39;, &#39;-&#39;, &#39;「&#39;, &#39;原&#39;, &#39;日本海&#39;, &#39;軍鳳山&#39;, &#39;無線&#39;, &#39;電信&#39;, &#39;所&#39;, &#39;」&#39;, &#39;，&#39;, &#39;市府&#39;, &#39;為&#39;, &#39;保存&#39;, &#39;古&#39;, &#39;蹟&#39;, &#39;同時&#39;, &#39;活化&#39;, &#39;眷村&#39;, &#39;遷移&#39;, &#39;後&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;以&#39;, &#39;重劃&#39;, &#39;方式&#39;, &#39;整體&#39;, &#39;開發&#39;, &#39;，&#39;, &#39;新闢&#39;, &#39;住宅&#39;, &#39;區&#39;, &#39;、&#39;, &#39;道路&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;停車場&#39;, &#39;，&#39;, &#39;使本區&#39;, &#39;具有&#39;, &#39;歷史&#39;, &#39;文化&#39;, &#39;內涵&#39;, &#39;與&#39;, &#39;綠色&#39;, &#39;休閒&#39;, &#39;特色&#39;, &#39;，&#39;, &#39;生活&#39;, &#39;機能&#39;, &#39;更加&#39;, &#39;健全&#39;, &#39;。&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;首次&#39;, &#39;推出&#39;, &#39;1&#39;, &#39;筆大面積&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;2160&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;地形&#39;, &#39;方整&#39;, &#39;，&#39;, &#39;雙面&#39;, &#39;臨路&#39;, &#39;，&#39;, &#39;利&#39;, &#39;於&#39;, &#39;規劃&#39;, &#39;興建景&#39;, &#39;觀大樓&#39;, &#39;，&#39;, &#39;附近&#39;, &#39;有&#39;, &#39;市場&#39;, &#39;、&#39;, &#39;學校&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;大東&#39;, &#39;文化&#39;, &#39;園區&#39;, &#39;，&#39;, &#39;距捷&#39;, &#39;運大東&#39;, &#39;站&#39;, &#39;、&#39;, &#39;鳳山國&#39;, &#39;中站&#39;, &#39;及鳳&#39;, &#39;山火&#39;, &#39;車站&#39;, &#39;僅數&#39;, &#39;分鐘&#39;, &#39;車程&#39;, &#39;，&#39;, &#39;交通&#39;, &#39;四通&#39;, &#39;八達&#39;, &#39;，&#39;, &#39;因&#39;, &#39;土地&#39;, &#39;稀少&#39;, &#39;性及&#39;, &#39;區位&#39;, &#39;條件&#39;, &#39;絕佳&#39;, &#39;，&#39;, &#39;勢必成&#39;, &#39;為&#39;, &#39;投資人&#39;, &#39;追逐&#39;, &#39;焦點&#39;, &#39;。&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;第&#39;, &#39;87&#39;, &#39;期重&#39;, &#39;劃區&#39;, &#39;，&#39;, &#39;位&#39;, &#39;於&#39;, &#39;省&#39;, &#39;道&#39;, &#39;台&#39;, &#39;1&#39;, &#39;線旁&#39;, &#39;，&#39;, &#39;鄰近&#39;, &#39;捷運&#39;, &#39;南岡山&#39;, &#39;站&#39;, &#39;，&#39;, &#39;重劃&#39;, &#39;後&#39;, &#39;擁有&#39;, &#39;完善&#39;, &#39;的&#39;, &#39;道路&#39;, &#39;系統&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;綠地&#39;, &#39;及&#39;, &#39;毗&#39;, &#39;鄰醒&#39;, &#39;村懷舊&#39;, &#39;文化&#39;, &#39;景觀&#39;, &#39;建築群&#39;, &#39;，&#39;, &#39;具備&#39;, &#39;優質&#39;, &#39;居住&#39;, &#39;環境&#39;, &#39;及&#39;, &#39;交通&#39;, &#39;便捷&#39;, &#39;要件&#39;, &#39;，&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;一&#39;, &#39;推出&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;，&#39;, &#39;即&#39;, &#39;掀起&#39;, &#39;搶標&#39;, &#39;熱潮&#39;, &#39;，&#39;, &#39;本季&#39;, &#39;再釋&#39;, &#39;出&#39;, &#39;1&#39;, &#39;筆面&#39;, &#39;積約&#39;, &#39;93&#39;, &#39;坪&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;臨&#39;, &#39;20&#39;, &#39;米&#39;, &#39;介壽路&#39;, &#39;及鵬程&#39;, &#39;東路&#39;, &#39;，&#39;, &#39;附近&#39;, &#39;有岡山&#39;, &#39;文化&#39;, &#39;中心&#39;, &#39;、&#39;, &#39;兆&#39;, &#39;湘國&#39;, &#39;小&#39;, &#39;、&#39;, &#39;公&#39;, &#39;13&#39;, &#39;、&#39;, &#39;公&#39;, &#39;14&#39;, &#39;、&#39;, &#39;陽明&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;劉厝公園&#39;, &#39;，&#39;, &#39;區位&#39;, &#39;條件&#39;, &#39;佳&#39;, &#39;，&#39;, &#39;投資人&#39;, &#39;準備&#39;, &#39;搶進&#39;, &#39;！&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;第&#39;, &#39;77&#39;, &#39;期市&#39;, &#39;地&#39;, &#39;重劃區&#39;, &#39;，&#39;, &#39;位&#39;, &#39;於&#39;, &#39;鳳山區&#39;, &#39;快速道路&#39;, &#39;省道&#39;, &#39;台&#39;, &#39;88&#39;, &#39;線旁&#39;, &#39;，&#39;, &#39;近&#39;, &#39;中山&#39;, &#39;高&#39;, &#39;五甲&#39;, &#39;系統&#39;, &#39;交流&#39;, &#39;道&#39;, &#39;，&#39;, &#39;近年&#39;, &#39;推出&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;皆&#39;, &#39;順利&#39;, &#39;完銷&#39;, &#39;。&#39;, &#39;本季&#39;, &#39;再&#39;, &#39;推出&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;其中&#39;, &#39;1&#39;, &#39;筆面&#39;, &#39;積約&#39;, &#39;526&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;臨保華&#39;, &#39;一路&#39;, &#39;，&#39;, &#39;適合&#39;, &#39;商業&#39;, &#39;使用&#39;, &#39;；&#39;, &#39;1&#39;, &#39;筆面積&#39;, &#39;107&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;位&#39;, &#39;於&#39;, &#39;代德三街&#39;, &#39;，&#39;, &#39;自用&#39;, &#39;投資&#39;, &#39;兩&#39;, &#39;相宜&#39;, &#39;。&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;高雄&#39;, &#39;大學區&#39;, &#39;段&#39;, &#39;徵收&#39;, &#39;區&#39;, &#39;，&#39;, &#39;為&#39;, &#39;北高雄&#39;, &#39;優質&#39;, &#39;文教&#39;, &#39;特區&#39;, &#39;，&#39;, &#39;優質&#39;, &#39;居住&#39;, &#39;環境&#39;, &#39;，&#39;, &#39;吸引&#39;, &#39;投資人&#39;, &#39;進駐&#39;, &#39;，&#39;, &#39;本季&#39;, &#39;再&#39;, &#39;推出&#39;, &#39;2&#39;, &#39;標&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;其中&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;第三&#39;, &#39;種商業區&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;639&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;位&#39;, &#39;於&#39;, &#39;大學&#39;, &#39;26&#39;, &#39;街&#39;, &#39;，&#39;, &#39;近高雄&#39;, &#39;大學&#39;, &#39;正門&#39;, &#39;及&#39;, &#39;萬坪&#39;, &#39;藍田公園&#39;, &#39;，&#39;, &#39;地形&#39;, &#39;方正&#39;, &#39;，&#39;, &#39;使用&#39;, &#39;強度&#39;, &#39;高&#39;, &#39;，&#39;, &#39;適合&#39;, &#39;興建&#39;, &#39;優質&#39;, &#39;住宅&#39;, &#39;大樓&#39;, &#39;；&#39;, &#39;另&#39;, &#39;1&#39;, &#39;筆住&#39;, &#39;三&#39;, &#39;用地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;379&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;臨&#39;, &#39;28&#39;, &#39;米&#39;, &#39;藍昌路&#39;, &#39;，&#39;, &#39;近高雄&#39;, &#39;大學及&#39;, &#39;中山&#39;, &#39;高中&#39;, &#39;，&#39;, &#39;交通&#39;, &#39;便捷&#39;, &#39;。&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;另&#39;, &#39;第&#39;, &#39;37&#39;, &#39;期重&#39;, &#39;劃區&#39;, &#39;及&#39;, &#39;前&#39;, &#39;大&#39;, &#39;寮&#39;, &#39;農地&#39;, &#39;重劃區&#39;, &#39;各&#39;, &#39;推出&#39;, &#39;1&#39;, &#39;至&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;價格&#39;, &#39;合理&#39;, &#39;。&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;第&#39;, &#39;4&#39;, &#39;季&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;作業&#39;, &#39;於&#39;, &#39;109&#39;, &#39;年&#39;, &#39;12&#39;, &#39;月&#39;, &#39;1&#39;, &#39;日&#39;, &#39;公告&#39;, &#39;，&#39;, &#39;投資大眾&#39;, &#39;可&#39;, &#39;前往&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;土地&#39;, &#39;處&#39;, &#39;分科&#39;, &#39;索取&#39;, &#39;標售&#39;, &#39;海報&#39;, &#39;及&#39;, &#39;標單&#39;, &#39;，&#39;, &#39;或&#39;, &#39;直接&#39;, &#39;上網&#39;, &#39;高雄房&#39;, &#39;地產&#39;, &#39;億年&#39;, &#39;旺&#39;, &#39;網站&#39;, &#39;、&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;及&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;網站&#39;, &#39;查詢&#39;, &#39;下載&#39;, &#39;相關&#39;, &#39;資料&#39;, &#39;，&#39;, &#39;在&#39;, &#39;期限&#39;, &#39;前&#39;, &#39;完成&#39;, &#39;投標&#39;, &#39;，&#39;, &#39;另&#39;, &#39;再&#39;, &#39;提醒&#39;, &#39;投標&#39;, &#39;人&#39;, &#39;，&#39;, &#39;本年度&#39;, &#39;已&#39;, &#39;更新&#39;, &#39;投標&#39;, &#39;單&#39;, &#39;格式&#39;, &#39;，&#39;, &#39;投標&#39;, &#39;大眾&#39;, &#39;請&#39;, &#39;注意&#39;, &#39;應以&#39;, &#39;新式&#39;, &#39;投標單&#39;, &#39;投標&#39;, &#39;以免&#39;, &#39;投標&#39;, &#39;無效作&#39;, &#39;廢&#39;, &#39;。&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;為&#39;, &#39;配合&#39;, &#39;防疫&#39;, &#39;需求&#39;, &#39;，&#39;, &#39;本季&#39;, &#39;開標&#39;, &#39;作業&#39;, &#39;除&#39;, &#39;於&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;第一&#39;, &#39;會議室&#39;, &#39;辦理外&#39;, &#39;，&#39;, &#39;另將&#39;, &#39;於&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;Facebook&#39;, &#39;粉絲&#39;, &#39;專頁&#39;, &#39;同步&#39;, &#39;直播&#39;, &#39;，&#39;, &#39;請大眾&#39;, &#39;多加&#39;, &#39;利用&#39;, &#39;。&#39;, &#39; n&#39;, &#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;洽詢&#39;, &#39;專線&#39;, &#39;：&#39;, &#39; &#39;, &#39;3373451&#39;, &#39;或&#39;, &#39; &#39;, &#39;3314942&#39;, &#39; n&#39;, &#39; n&#39;, &#39;高雄房&#39;, &#39;地產&#39;, &#39;億年&#39;, &#39;旺&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39; &#39;, &#39;）&#39;, &#39; n&#39;, &#39; n&#39;, &#39;高雄市&#39;, &#39;政府&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39; &#39;, &#39;）&#39;, &#39; n&#39;, &#39; n&#39;, &#39;高雄市&#39;, &#39;政府&#39;, &#39;地&#39;, &#39;政局&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39; &#39;, &#39;）&#39;] . Feed tokenized results to spacy using WhitespaceTokenizer . The official website of spaCy describes several ways of adding a custom tokenizer. The simplest is to define the WhitespaceTokenizer class, which tokenizes a text on space characters. The output of tokenization can then be fed into subsequent operations down the pipeline, including tagger for parts-of-speech (POS) tagging, parser for dependency parsing, and ner for named entity recognition. This is possible primarily because tokenizer creates a Doc object whereas the other three steps operate on the Doc object, as illustrated in this graph. . . Note: The original code for words is words = text.split(&quot; &quot;), but it caused an error to my text. So I revised it into words = text.strip().split(). . from spacy.tokens import Doc class WhitespaceTokenizer: def __init__(self, vocab): self.vocab = vocab def __call__(self, text): words = text.strip().split() return Doc(self.vocab, words=words) . Next, let&#39;s load the zh_core_web_sm model for Chinese, which we&#39;ll need for POS tagging. Then here comes the crucial part: nlp.tokenizer = WhitespaceTokenizer(nlp.vocab). This line of code sets the default tokenizer from Jieba to WhitespaceTokenizer, which we just defined above. . import spacy nlp = spacy.load(&#39;zh_core_web_sm&#39;) nlp.tokenizer = WhitespaceTokenizer(nlp.vocab) . Then we join the tokenized result from CKIP Transformers to a single string of space-seperated tokens. . token_str = &quot; &quot;.join(tokens) token_str . &#39;市府 地政局 109年度 第4 季 開發區 土地 標售 ， 共計 推出 8 標 9 筆 優質 建地 ， 訂 於 109年 12月 16日 開標 ， 合計 總底價 12 億 4049萬 6164 元 。 n n n n 第93 期 重劃區 ， 原 為 國軍 眷村 ， 緊鄰 國定 古蹟 - 「 原 日本 海軍 鳳山 無線 電信所 」 ， 市府 為 保存 古蹟 同時 活化 眷村 遷移 後 土地 ， 以 重劃 方式 整體 開發 ， 新 闢 住宅區 、 道路 、 公園 及 停車場 ， 使 本 區 具有 歷史 文化 內涵 與 綠色 休閒 特色 ， 生活 機能 更加 健全 。 地政局 首次 推出 1 筆 大 面積 土地 ， 面積 約 2160 坪 ， 地形 方整 ， 雙面 臨 路 ， 利於 規劃 興建 景觀 大樓 ， 附近 有 市場 、 學校 、 公園 及 大東 文化 園區 ， 距 捷運 大東站 、 鳳山 國中站 及 鳳山 火車站 僅 數 分鐘 車程 ， 交通 四通八達 ， 因 土地 稀少性 及 區位 條件 絕佳 ， 勢必 成為 投資人 追逐 焦點 。 n n n n 第87 期 重劃區 ， 位於 省道 台1線 旁 ， 鄰近 捷運 南 岡山站 ， 重劃 後 擁有 完善 的 道路 系統 、 公園 綠地 及 毗鄰 醒村 懷舊 文化 景觀 建築群 ， 具備 優質 居住 環境 及 交通 便捷 要件 ， 地政局 一 推出 土地 標售 ， 即 掀起 搶標 熱潮 ， 本 季 再 釋出 1 筆 面積 約 93 坪 土地 ， 臨 20 米 介壽路 及 鵬程東路 ， 附近 有 岡山 文化 中心 、 兆湘 國小 、 公13 、 公14 、 陽明 公園 及 劉厝 公園 ， 區位 條件 佳 ， 投資人 準備 搶進 ！ n n n n 第77 期 市地 重劃區 ， 位於 鳳山區 快速 道路 省道 台88 線 旁 ， 近 中山高 五甲 系統 交流道 ， 近年 推出 土地 標售 皆 順利 完銷 。 本 季 再 推出 2 筆 土地 ， 其中 1 筆 面積 約 526 坪 ， 臨 保華一路 ， 適合 商業 使用 ； 1 筆 面積 107 坪 ， 位於 代德三街 ， 自用 投資 兩 相宜 。 n n n n 高雄 大學 區段 徵收區 ， 為 北 高雄 優質 文教 特區 ， 優質 居住 環境 ， 吸引 投資人 進駐 ， 本 季 再 推出 2 標 2 筆 土地 ， 其中 1 筆 第三 種 商業區 土地 ， 面積 約 639 坪 ， 位於 大學 26街 ， 近 高雄 大學 正門 及 萬 坪 藍田 公園 ， 地形 方正 ， 使用 強度 高 ， 適合 興建 優質 住宅 大樓 ； 另 1 筆 住三 用地 ， 面積 約 379 坪 ， 臨 28 米 藍昌路 ， 近 高雄 大學 及 中山 高中 ， 交通 便捷 。 n n n n 另 第37 期 重劃區 及 前 大寮 農地 重劃區 各 推出 1 至 2 筆 土地 ， 價格 合理 。 n n n n 第4 季 土地 標售 作業 於 109年 12月 1日 公告 ， 投資 大眾 可 前往 地政局 土地 開發處 土地處 分科 索取 標售 海報 及 標單 ， 或 直接 上網 高雄 房地產 億年旺 網站 、 地政局 及 土地 開發處 網站 查詢 下載 相關 資料 ， 在 期限 前 完成 投標 ， 另 再 提醒 投標人 ， 本 年度 已 更新 投標單 格式 ， 投標 大眾 請 注意 應 以 新式 投標單 投標 以免 投標 無效 作廢 。 n n n n 為 配合 防疫 需求 ， 本 季 開標 作業 除 於 地政局 第一 會議室 辦理 外 ， 另 將 於 地政局 Facebook 粉絲 專頁 同步 直播 ， 請 大眾 多加 利用 。 n n n n 洽詢 專線 ： 3373 451 或 3314942 n n 高雄 房地產 億 年 旺 網站 （ 網址 ： ） n n 高雄市 政府 地政局 網站 （ 網址 ： ） n n 高雄市 政府 地政局 土地 開發處 網站 （ 網址 ： ）&#39; . Next, we feed token_str, our tokenized text, to nlp to create a spaCy Doc object. From this point on, we are able to leverage the power of spaCy. For every token in a Doc object, we have access to its text via the attribute .text and its parts-of-speech label via the attribute .pos_. . doc = nlp(token_str) print([token.text for token in doc]) print([token.pos_ for token in doc]) . [&#39;市府&#39;, &#39;地政局&#39;, &#39;109年度&#39;, &#39;第4&#39;, &#39;季&#39;, &#39;開發區&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;，&#39;, &#39;共計&#39;, &#39;推出&#39;, &#39;8&#39;, &#39;標&#39;, &#39;9&#39;, &#39;筆&#39;, &#39;優質&#39;, &#39;建地&#39;, &#39;，&#39;, &#39;訂&#39;, &#39;於&#39;, &#39;109年&#39;, &#39;12月&#39;, &#39;16日&#39;, &#39;開標&#39;, &#39;，&#39;, &#39;合計&#39;, &#39;總底價&#39;, &#39;12&#39;, &#39;億&#39;, &#39;4049萬&#39;, &#39;6164&#39;, &#39;元&#39;, &#39;。&#39;, &#39;第93&#39;, &#39;期&#39;, &#39;重劃區&#39;, &#39;，&#39;, &#39;原&#39;, &#39;為&#39;, &#39;國軍&#39;, &#39;眷村&#39;, &#39;，&#39;, &#39;緊鄰&#39;, &#39;國定&#39;, &#39;古蹟&#39;, &#39;-&#39;, &#39;「&#39;, &#39;原&#39;, &#39;日本&#39;, &#39;海軍&#39;, &#39;鳳山&#39;, &#39;無線&#39;, &#39;電信所&#39;, &#39;」&#39;, &#39;，&#39;, &#39;市府&#39;, &#39;為&#39;, &#39;保存&#39;, &#39;古蹟&#39;, &#39;同時&#39;, &#39;活化&#39;, &#39;眷村&#39;, &#39;遷移&#39;, &#39;後&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;以&#39;, &#39;重劃&#39;, &#39;方式&#39;, &#39;整體&#39;, &#39;開發&#39;, &#39;，&#39;, &#39;新&#39;, &#39;闢&#39;, &#39;住宅區&#39;, &#39;、&#39;, &#39;道路&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;停車場&#39;, &#39;，&#39;, &#39;使&#39;, &#39;本&#39;, &#39;區&#39;, &#39;具有&#39;, &#39;歷史&#39;, &#39;文化&#39;, &#39;內涵&#39;, &#39;與&#39;, &#39;綠色&#39;, &#39;休閒&#39;, &#39;特色&#39;, &#39;，&#39;, &#39;生活&#39;, &#39;機能&#39;, &#39;更加&#39;, &#39;健全&#39;, &#39;。&#39;, &#39;地政局&#39;, &#39;首次&#39;, &#39;推出&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;大&#39;, &#39;面積&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;2160&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;地形&#39;, &#39;方整&#39;, &#39;，&#39;, &#39;雙面&#39;, &#39;臨&#39;, &#39;路&#39;, &#39;，&#39;, &#39;利於&#39;, &#39;規劃&#39;, &#39;興建&#39;, &#39;景觀&#39;, &#39;大樓&#39;, &#39;，&#39;, &#39;附近&#39;, &#39;有&#39;, &#39;市場&#39;, &#39;、&#39;, &#39;學校&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;大東&#39;, &#39;文化&#39;, &#39;園區&#39;, &#39;，&#39;, &#39;距&#39;, &#39;捷運&#39;, &#39;大東站&#39;, &#39;、&#39;, &#39;鳳山&#39;, &#39;國中站&#39;, &#39;及&#39;, &#39;鳳山&#39;, &#39;火車站&#39;, &#39;僅&#39;, &#39;數&#39;, &#39;分鐘&#39;, &#39;車程&#39;, &#39;，&#39;, &#39;交通&#39;, &#39;四通八達&#39;, &#39;，&#39;, &#39;因&#39;, &#39;土地&#39;, &#39;稀少性&#39;, &#39;及&#39;, &#39;區位&#39;, &#39;條件&#39;, &#39;絕佳&#39;, &#39;，&#39;, &#39;勢必&#39;, &#39;成為&#39;, &#39;投資人&#39;, &#39;追逐&#39;, &#39;焦點&#39;, &#39;。&#39;, &#39;第87&#39;, &#39;期&#39;, &#39;重劃區&#39;, &#39;，&#39;, &#39;位於&#39;, &#39;省道&#39;, &#39;台1線&#39;, &#39;旁&#39;, &#39;，&#39;, &#39;鄰近&#39;, &#39;捷運&#39;, &#39;南&#39;, &#39;岡山站&#39;, &#39;，&#39;, &#39;重劃&#39;, &#39;後&#39;, &#39;擁有&#39;, &#39;完善&#39;, &#39;的&#39;, &#39;道路&#39;, &#39;系統&#39;, &#39;、&#39;, &#39;公園&#39;, &#39;綠地&#39;, &#39;及&#39;, &#39;毗鄰&#39;, &#39;醒村&#39;, &#39;懷舊&#39;, &#39;文化&#39;, &#39;景觀&#39;, &#39;建築群&#39;, &#39;，&#39;, &#39;具備&#39;, &#39;優質&#39;, &#39;居住&#39;, &#39;環境&#39;, &#39;及&#39;, &#39;交通&#39;, &#39;便捷&#39;, &#39;要件&#39;, &#39;，&#39;, &#39;地政局&#39;, &#39;一&#39;, &#39;推出&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;，&#39;, &#39;即&#39;, &#39;掀起&#39;, &#39;搶標&#39;, &#39;熱潮&#39;, &#39;，&#39;, &#39;本&#39;, &#39;季&#39;, &#39;再&#39;, &#39;釋出&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;93&#39;, &#39;坪&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;臨&#39;, &#39;20&#39;, &#39;米&#39;, &#39;介壽路&#39;, &#39;及&#39;, &#39;鵬程東路&#39;, &#39;，&#39;, &#39;附近&#39;, &#39;有&#39;, &#39;岡山&#39;, &#39;文化&#39;, &#39;中心&#39;, &#39;、&#39;, &#39;兆湘&#39;, &#39;國小&#39;, &#39;、&#39;, &#39;公13&#39;, &#39;、&#39;, &#39;公14&#39;, &#39;、&#39;, &#39;陽明&#39;, &#39;公園&#39;, &#39;及&#39;, &#39;劉厝&#39;, &#39;公園&#39;, &#39;，&#39;, &#39;區位&#39;, &#39;條件&#39;, &#39;佳&#39;, &#39;，&#39;, &#39;投資人&#39;, &#39;準備&#39;, &#39;搶進&#39;, &#39;！&#39;, &#39;第77&#39;, &#39;期&#39;, &#39;市地&#39;, &#39;重劃區&#39;, &#39;，&#39;, &#39;位於&#39;, &#39;鳳山區&#39;, &#39;快速&#39;, &#39;道路&#39;, &#39;省道&#39;, &#39;台88&#39;, &#39;線&#39;, &#39;旁&#39;, &#39;，&#39;, &#39;近&#39;, &#39;中山高&#39;, &#39;五甲&#39;, &#39;系統&#39;, &#39;交流道&#39;, &#39;，&#39;, &#39;近年&#39;, &#39;推出&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;皆&#39;, &#39;順利&#39;, &#39;完銷&#39;, &#39;。&#39;, &#39;本&#39;, &#39;季&#39;, &#39;再&#39;, &#39;推出&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;其中&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;526&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;臨&#39;, &#39;保華一路&#39;, &#39;，&#39;, &#39;適合&#39;, &#39;商業&#39;, &#39;使用&#39;, &#39;；&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;面積&#39;, &#39;107&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;位於&#39;, &#39;代德三街&#39;, &#39;，&#39;, &#39;自用&#39;, &#39;投資&#39;, &#39;兩&#39;, &#39;相宜&#39;, &#39;。&#39;, &#39;高雄&#39;, &#39;大學&#39;, &#39;區段&#39;, &#39;徵收區&#39;, &#39;，&#39;, &#39;為&#39;, &#39;北&#39;, &#39;高雄&#39;, &#39;優質&#39;, &#39;文教&#39;, &#39;特區&#39;, &#39;，&#39;, &#39;優質&#39;, &#39;居住&#39;, &#39;環境&#39;, &#39;，&#39;, &#39;吸引&#39;, &#39;投資人&#39;, &#39;進駐&#39;, &#39;，&#39;, &#39;本&#39;, &#39;季&#39;, &#39;再&#39;, &#39;推出&#39;, &#39;2&#39;, &#39;標&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;其中&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;第三&#39;, &#39;種&#39;, &#39;商業區&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;639&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;位於&#39;, &#39;大學&#39;, &#39;26街&#39;, &#39;，&#39;, &#39;近&#39;, &#39;高雄&#39;, &#39;大學&#39;, &#39;正門&#39;, &#39;及&#39;, &#39;萬&#39;, &#39;坪&#39;, &#39;藍田&#39;, &#39;公園&#39;, &#39;，&#39;, &#39;地形&#39;, &#39;方正&#39;, &#39;，&#39;, &#39;使用&#39;, &#39;強度&#39;, &#39;高&#39;, &#39;，&#39;, &#39;適合&#39;, &#39;興建&#39;, &#39;優質&#39;, &#39;住宅&#39;, &#39;大樓&#39;, &#39;；&#39;, &#39;另&#39;, &#39;1&#39;, &#39;筆&#39;, &#39;住三&#39;, &#39;用地&#39;, &#39;，&#39;, &#39;面積&#39;, &#39;約&#39;, &#39;379&#39;, &#39;坪&#39;, &#39;，&#39;, &#39;臨&#39;, &#39;28&#39;, &#39;米&#39;, &#39;藍昌路&#39;, &#39;，&#39;, &#39;近&#39;, &#39;高雄&#39;, &#39;大學&#39;, &#39;及&#39;, &#39;中山&#39;, &#39;高中&#39;, &#39;，&#39;, &#39;交通&#39;, &#39;便捷&#39;, &#39;。&#39;, &#39;另&#39;, &#39;第37&#39;, &#39;期&#39;, &#39;重劃區&#39;, &#39;及&#39;, &#39;前&#39;, &#39;大寮&#39;, &#39;農地&#39;, &#39;重劃區&#39;, &#39;各&#39;, &#39;推出&#39;, &#39;1&#39;, &#39;至&#39;, &#39;2&#39;, &#39;筆&#39;, &#39;土地&#39;, &#39;，&#39;, &#39;價格&#39;, &#39;合理&#39;, &#39;。&#39;, &#39;第4&#39;, &#39;季&#39;, &#39;土地&#39;, &#39;標售&#39;, &#39;作業&#39;, &#39;於&#39;, &#39;109年&#39;, &#39;12月&#39;, &#39;1日&#39;, &#39;公告&#39;, &#39;，&#39;, &#39;投資&#39;, &#39;大眾&#39;, &#39;可&#39;, &#39;前往&#39;, &#39;地政局&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;土地處&#39;, &#39;分科&#39;, &#39;索取&#39;, &#39;標售&#39;, &#39;海報&#39;, &#39;及&#39;, &#39;標單&#39;, &#39;，&#39;, &#39;或&#39;, &#39;直接&#39;, &#39;上網&#39;, &#39;高雄&#39;, &#39;房地產&#39;, &#39;億年旺&#39;, &#39;網站&#39;, &#39;、&#39;, &#39;地政局&#39;, &#39;及&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;網站&#39;, &#39;查詢&#39;, &#39;下載&#39;, &#39;相關&#39;, &#39;資料&#39;, &#39;，&#39;, &#39;在&#39;, &#39;期限&#39;, &#39;前&#39;, &#39;完成&#39;, &#39;投標&#39;, &#39;，&#39;, &#39;另&#39;, &#39;再&#39;, &#39;提醒&#39;, &#39;投標人&#39;, &#39;，&#39;, &#39;本&#39;, &#39;年度&#39;, &#39;已&#39;, &#39;更新&#39;, &#39;投標單&#39;, &#39;格式&#39;, &#39;，&#39;, &#39;投標&#39;, &#39;大眾&#39;, &#39;請&#39;, &#39;注意&#39;, &#39;應&#39;, &#39;以&#39;, &#39;新式&#39;, &#39;投標單&#39;, &#39;投標&#39;, &#39;以免&#39;, &#39;投標&#39;, &#39;無效&#39;, &#39;作廢&#39;, &#39;。&#39;, &#39;為&#39;, &#39;配合&#39;, &#39;防疫&#39;, &#39;需求&#39;, &#39;，&#39;, &#39;本&#39;, &#39;季&#39;, &#39;開標&#39;, &#39;作業&#39;, &#39;除&#39;, &#39;於&#39;, &#39;地政局&#39;, &#39;第一&#39;, &#39;會議室&#39;, &#39;辦理&#39;, &#39;外&#39;, &#39;，&#39;, &#39;另&#39;, &#39;將&#39;, &#39;於&#39;, &#39;地政局&#39;, &#39;Facebook&#39;, &#39;粉絲&#39;, &#39;專頁&#39;, &#39;同步&#39;, &#39;直播&#39;, &#39;，&#39;, &#39;請&#39;, &#39;大眾&#39;, &#39;多加&#39;, &#39;利用&#39;, &#39;。&#39;, &#39;洽詢&#39;, &#39;專線&#39;, &#39;：&#39;, &#39;3373&#39;, &#39;451&#39;, &#39;或&#39;, &#39;3314942&#39;, &#39;高雄&#39;, &#39;房地產&#39;, &#39;億&#39;, &#39;年&#39;, &#39;旺&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39;）&#39;, &#39;高雄市&#39;, &#39;政府&#39;, &#39;地政局&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39;）&#39;, &#39;高雄市&#39;, &#39;政府&#39;, &#39;地政局&#39;, &#39;土地&#39;, &#39;開發處&#39;, &#39;網站&#39;, &#39;（&#39;, &#39;網址&#39;, &#39;：&#39;, &#39;）&#39;] [&#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;CCONJ&#39;, &#39;NUM&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;DET&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;ADJ&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADJ&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;ADJ&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NUM&#39;, &#39;CCONJ&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;CCONJ&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;ADV&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;ADP&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;ADP&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;PUNCT&#39;, &#39;DET&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;ADV&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;PUNCT&#39;, &#39;NUM&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;NUM&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NUM&#39;, &#39;NUM&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;VERB&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;] . The POS tagging is made possible by the zh_core_web_sm model. Notice that spaCy uses coarse labels such as NOUN and VERB. By contrast, CKIP Transformers adopts a more fine-grained tagset, such as Nc for locative nouns and Nd for temporal nouns. Here&#39;re the POS labels for the same text produced by CKIP Transformers. We&#39;ll be using the spaCy&#39;s POS tagging to filter out words that we don&#39;t want in the candicate pool for keywords. . pos_tags = pos[0] print(pos_tags) . [&#39;Nc&#39;, &#39;Nc&#39;, &#39;Nd&#39;, &#39;Neu&#39;, &#39;Nd&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;P&#39;, &#39;Nd&#39;, &#39;Nd&#39;, &#39;Nd&#39;, &#39;VA&#39;, &#39;COMMACATEGORY&#39;, &#39;VG&#39;, &#39;Na&#39;, &#39;Neu&#39;, &#39;Neu&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;D&#39;, &#39;VG&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;DASHCATEGORY&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;A&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;A&#39;, &#39;Nc&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;COMMACATEGORY&#39;, &#39;Nc&#39;, &#39;P&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Nd&#39;, &#39;VHC&#39;, &#39;Nc&#39;, &#39;VC&#39;, &#39;Ng&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;VC&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Na&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VL&#39;, &#39;Nes&#39;, &#39;Nc&#39;, &#39;VJ&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Dfa&#39;, &#39;VHC&#39;, &#39;PERIODCATEGORY&#39;, &#39;Nc&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;A&#39;, &#39;VCL&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VK&#39;, &#39;VC&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nc&#39;, &#39;V_2&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nb&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;Cbb&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;D&#39;, &#39;VG&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Ncd&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VC&#39;, &#39;Ng&#39;, &#39;VJ&#39;, &#39;VH&#39;, &#39;DE&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;VH&#39;, &#39;Nc&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;A&#39;, &#39;VA&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nc&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Nv&#39;, &#39;COMMACATEGORY&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;VD&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nes&#39;, &#39;Nd&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;Nc&#39;, &#39;V_2&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Na&#39;, &#39;PAUSECATEGORY&#39;, &#39;Na&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VF&#39;, &#39;VA&#39;, &#39;EXCLAMATIONCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Nf&#39;, &#39;Ncd&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nd&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Nv&#39;, &#39;D&#39;, &#39;VH&#39;, &#39;VC&#39;, &#39;PERIODCATEGORY&#39;, &#39;Nes&#39;, &#39;Nd&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nep&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;SEMICOLONCATEGORY&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;Neu&#39;, &#39;VH&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;COMMACATEGORY&#39;, &#39;VG&#39;, &#39;Ncd&#39;, &#39;Nc&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;A&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VJ&#39;, &#39;Na&#39;, &#39;VCL&#39;, &#39;COMMACATEGORY&#39;, &#39;Nes&#39;, &#39;Nd&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nep&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;VC&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;SEMICOLONCATEGORY&#39;, &#39;Nes&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;VCL&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;Da&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;VH&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Cbb&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Nes&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Neu&#39;, &#39;Caa&#39;, &#39;Neu&#39;, &#39;Nf&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;Neu&#39;, &#39;Nd&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;P&#39;, &#39;Nd&#39;, &#39;Nd&#39;, &#39;Nd&#39;, &#39;VE&#39;, &#39;COMMACATEGORY&#39;, &#39;VC&#39;, &#39;Nh&#39;, &#39;D&#39;, &#39;VCL&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;VD&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Caa&#39;, &#39;VH&#39;, &#39;VA&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nb&#39;, &#39;Nc&#39;, &#39;PAUSECATEGORY&#39;, &#39;Nc&#39;, &#39;Caa&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;Nc&#39;, &#39;VE&#39;, &#39;VC&#39;, &#39;VH&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;P&#39;, &#39;Na&#39;, &#39;Ng&#39;, &#39;VC&#39;, &#39;VA&#39;, &#39;COMMACATEGORY&#39;, &#39;Cbb&#39;, &#39;D&#39;, &#39;VE&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nes&#39;, &#39;Na&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;VA&#39;, &#39;Nh&#39;, &#39;VF&#39;, &#39;VK&#39;, &#39;D&#39;, &#39;P&#39;, &#39;A&#39;, &#39;Na&#39;, &#39;VA&#39;, &#39;Cbb&#39;, &#39;VA&#39;, &#39;VH&#39;, &#39;VH&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;P&#39;, &#39;VC&#39;, &#39;VA&#39;, &#39;Na&#39;, &#39;COMMACATEGORY&#39;, &#39;Nes&#39;, &#39;Nd&#39;, &#39;Nv&#39;, &#39;Na&#39;, &#39;P&#39;, &#39;P&#39;, &#39;Nc&#39;, &#39;Neu&#39;, &#39;Nc&#39;, &#39;VC&#39;, &#39;Ng&#39;, &#39;COMMACATEGORY&#39;, &#39;Cbb&#39;, &#39;D&#39;, &#39;P&#39;, &#39;Nc&#39;, &#39;FW&#39;, &#39;Na&#39;, &#39;Na&#39;, &#39;VH&#39;, &#39;D&#39;, &#39;COMMACATEGORY&#39;, &#39;VF&#39;, &#39;Nh&#39;, &#39;D&#39;, &#39;VC&#39;, &#39;PERIODCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;VE&#39;, &#39;Na&#39;, &#39;COLONCATEGORY&#39;, &#39;FW&#39;, &#39;Neu&#39;, &#39;Caa&#39;, &#39;FW&#39;, &#39;WHITESPACE&#39;, &#39;WHITESPACE&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nb&#39;, &#39;Nf&#39;, &#39;VH&#39;, &#39;Nc&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;Na&#39;, &#39;COLONCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;WHITESPACE&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;Nc&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;Na&#39;, &#39;COLONCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;WHITESPACE&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;Nc&#39;, &#39;Na&#39;, &#39;VC&#39;, &#39;Nc&#39;, &#39;PARENTHESISCATEGORY&#39;, &#39;Na&#39;, &#39;COLONCATEGORY&#39;, &#39;WHITESPACE&#39;, &#39;PARENTHESISCATEGORY&#39;] . Convert stopwords in spaCy from simplified to Taiwanese traditional . spaCy comes with a built-in set of stopwords (basically words that we&#39;d like to ignore), accessible via spacy.lang.zh.stop_words. To make good use of it, let&#39;s convert all the words from simplified characters to traditional ones with the help of OpenCC. . !pip install OpenCC import opencc . OpenCC does not just convert characters mechanically. It has the ability to convert words from simplified characters to their equivalent phrasing in Taiwan Mandarin, which is done by s2twp.json. . from spacy.lang.zh.stop_words import STOP_WORDS converter = opencc.OpenCC(&#39;s2twp.json&#39;) spacy_stopwords_sim = list(STOP_WORDS) print(spacy_stopwords_sim[:5]) spacy_stopwords_tra = [converter.convert(w) for w in spacy_stopwords_sim] print(spacy_stopwords_tra[:5]) . [&#39;因为&#39;, &#39;奇&#39;, &#39;嘿嘿&#39;, &#39;其次&#39;, &#39;偏偏&#39;] [&#39;因為&#39;, &#39;奇&#39;, &#39;嘿嘿&#39;, &#39;其次&#39;, &#39;偏偏&#39;] . Define a class for implementing TextRank . If you&#39;re dealing with English texts, you can implement TextRank quite easily with textaCy, the tagline of which is NLP, before and after spaCy. But I couldn&#39;t get it to work for Chinese texts, so I had to implement TextRank from scratch. Luckily, I got a jump-start from this gist, which offers a blueprint for the following definitions. . from collections import OrderedDict import numpy as np class TextRank4Keyword(): &quot;&quot;&quot;Extract keywords from text&quot;&quot;&quot; def __init__(self): self.d = 0.85 # damping coefficient, usually is .85 self.min_diff = 1e-5 # convergence threshold self.steps = 10 # iteration steps self.node_weight = None # save keywords and its weight def set_stopwords(self, custom_stopwords): &quot;&quot;&quot;Set stop words&quot;&quot;&quot; for word in set(spacy_stopwords_tra).union(set(custom_stopwords)): lexeme = nlp.vocab[word] lexeme.is_stop = True def sentence_segment(self, doc, candidate_pos, lower): &quot;&quot;&quot;Store those words only in cadidate_pos&quot;&quot;&quot; sentences = [] for sent in doc.sents: selected_words = [] for token in sent: # Store words only with cadidate POS tag if token.pos_ in candidate_pos and token.is_stop is False: if lower is True: selected_words.append(token.text.lower()) else: selected_words.append(token.text) sentences.append(selected_words) return sentences def get_vocab(self, sentences): &quot;&quot;&quot;Get all tokens&quot;&quot;&quot; vocab = OrderedDict() i = 0 for sentence in sentences: for word in sentence: if word not in vocab: vocab[word] = i i += 1 return vocab def get_token_pairs(self, window_size, sentences): &quot;&quot;&quot;Build token_pairs from windows in sentences&quot;&quot;&quot; token_pairs = list() for sentence in sentences: for i, word in enumerate(sentence): for j in range(i+1, i+window_size): if j &gt;= len(sentence): break pair = (word, sentence[j]) if pair not in token_pairs: token_pairs.append(pair) return token_pairs def symmetrize(self, a): return a + a.T - np.diag(a.diagonal()) def get_matrix(self, vocab, token_pairs): &quot;&quot;&quot;Get normalized matrix&quot;&quot;&quot; # Build matrix vocab_size = len(vocab) g = np.zeros((vocab_size, vocab_size), dtype=&#39;float&#39;) for word1, word2 in token_pairs: i, j = vocab[word1], vocab[word2] g[i][j] = 1 # Get Symmeric matrix g = self.symmetrize(g) # Normalize matrix by column norm = np.sum(g, axis=0) g_norm = np.divide(g, norm, where=norm!=0) # this is to ignore the 0 element in norm return g_norm # I revised this function to return keywords as a list def get_keywords(self, number=10): &quot;&quot;&quot;Print top number keywords&quot;&quot;&quot; node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True)) keywords = [] for i, (key, value) in enumerate(node_weight.items()): keywords.append(key) if i &gt; number: break return keywords def analyze(self, text, candidate_pos=[&#39;NOUN&#39;, &#39;VERB&#39;], window_size=5, lower=False, stopwords=list()): &quot;&quot;&quot;Main function to analyze text&quot;&quot;&quot; # Set stop words self.set_stopwords(stopwords) # Pare text with spaCy doc = nlp(token_str) # Filter sentences sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words # Build vocabulary vocab = self.get_vocab(sentences) # Get token_pairs from windows token_pairs = self.get_token_pairs(window_size, sentences) # Get normalized matrix g = self.get_matrix(vocab, token_pairs) # Initionlization for weight(pagerank value) pr = np.array([1] * len(vocab)) # Iteration previous_pr = 0 for epoch in range(self.steps): pr = (1-self.d) + self.d * np.dot(g, pr) if abs(previous_pr - sum(pr)) &lt; self.min_diff: break else: previous_pr = sum(pr) # Get weight for each node node_weight = dict() for word, index in vocab.items(): node_weight[word] = pr[index] self.node_weight = node_weight . Now we can create an instace of the TextRank4Keyword class and call the set_stopwords function with our CUSTOM_STOPWORDS variable. This created a set of stopwords resulting from the union of both our custom stopwords and spaCy&#39;s built-in stopwords. And only words that meet these two criteria would become candidates for keywords: . they are not in the set of stopwords; | their POS labels are one of those listed in candidate_pos, which includes NOUN and VERB by default. | . tr4w = TextRank4Keyword() tr4w.set_stopwords(CUSTOM_STOPWORDS) . Put it together . Let&#39;s put it all together by defining a main function for keyword extraction. . def extract_keys_from_str(raw_text): text = clean_all(raw_text) #clean the raw text ws = ws_driver([text]) #tokenize the text with CKIP Transformers tokenized_text = &quot; &quot;.join(ws[0]) #join a list into a string tr4w.analyze(tokenized_text) #create a spaCy Doc object with the string and calculate weights for words keys = tr4w.get_keywords(KW_NUM) #get top 10 keywords, as set by the KW_NUM variable return keys . Here&#39;re the top ten keywords for our sample text. The results are quite satisfactory. . keys = extract_keys_from_str(raw_text) keys = [k for k in keys if len(k) &gt; 1] keys . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 221.73it/s] Inference: 100%|██████████| 1/1 [00:05&lt;00:00, 5.20s/it] . [&#39;土地&#39;, &#39;公園&#39;, &#39;地政局&#39;, &#39;文化&#39;, &#39;推出&#39;, &#39;面積&#39;, &#39;標售&#39;, &#39;道路&#39;, &#39;優質&#39;, &#39;投標&#39;] . As a comparison, here&#39;re the top 10 keywords produced by Jieba&#39;s implementation of TextRank, 7 of which are identical to the list above. Although extracting keywords with Jieba is quick and easy, it tends to give rise to wrongly segmented tokens, such as 政局 in this example, which should have been 地政局 for Land Administration Bureau. . import jieba.analyse as KE jieba_kw = KE.textrank(text, topK=10) jieba_kw . [&#39;土地&#39;, &#39;政局&#39;, &#39;投標&#39;, &#39;公園&#39;, &#39;投資&#39;, &#39;標售&#39;, &#39;文化&#39;, &#39;開發&#39;, &#39;優質&#39;, &#39;推出&#39;] . Other libraries that failed . textaCy . !pip install textacy . With textaCy, you can load a spaCy language model and then create a spaCy Doc object using that model. . import textacy zh = textacy.load_spacy_lang(&quot;zh_core_web_sm&quot;) doc = textacy.make_spacy_doc(text, lang=zh) doc._.preview . &#39;Doc(612 tokens: &#34;市府地政局109年度第4季開發區土地標售，共計推出8標9筆優質建地，訂於109年12月16日開...&#34;)&#39; . textaCy implements four algorithms for keyword extraction, including TextRank. But I got useless results by calling the textacy.ke.textrank function with doc. . import textacy.ke as ke ke.textrank(doc) . [(&#39; &#39;, 6.0)] . pyate . !pip install pyate . pyate has a built-in TermExtractionPipeline class for extracting keywords, which can be added to spaCy&#39;s pipeline. But it didn&#39;t work and this error message showed up: TypeError: load() got an unexpected keyword argument &#39;parser&#39;. . from pyate.term_extraction_pipeline import TermExtractionPipeline nlp.add_pipe(TermExtractionPipeline()) . TypeError Traceback (most recent call last) &lt;ipython-input-27-f5a8398fbc3b&gt; in &lt;module&gt;() 1 #collapse-output -&gt; 2 from pyate.term_extraction_pipeline import TermExtractionPipeline 3 nlp.add_pipe(TermExtractionPipeline()) /usr/local/lib/python3.6/dist-packages/pyate/__init__.py in &lt;module&gt;() -&gt; 1 from .term_extraction import TermExtraction, add_term_extraction_method 2 from .basic import basic 3 from .combo_basic import combo_basic 4 from .cvalues import cvalues 5 from .term_extractor import term_extractor /usr/local/lib/python3.6/dist-packages/pyate/term_extraction.py in &lt;module&gt;() 20 21 &gt; 22 class TermExtraction: 23 # TODO: find some way to prevent redundant loading of csv files 24 nlp = spacy.load(&#34;en_core_web_sm&#34;, parser=False, entity=False) /usr/local/lib/python3.6/dist-packages/pyate/term_extraction.py in TermExtraction() 22 class TermExtraction: 23 # TODO: find some way to prevent redundant loading of csv files &gt; 24 nlp = spacy.load(&#34;en_core_web_sm&#34;, parser=False, entity=False) 25 matcher = Matcher(nlp.vocab) 26 language = &#34;en&#34; TypeError: load() got an unexpected keyword argument &#39;parser&#39; . I found on the documentation page that pyate only supports English and Italian, which may account for the error I got. . pytextrank . !pip install pytextrank . To add TextRank to the spaCy pipeline, I followed the instructions found on spaCy&#39;s documentation. But an error popped up. Luckily, ValueError offers possible ways to fix the problem. . import pytextrank tr = pytextrank.TextRank() nlp.add_pipe(tr.PipelineComponent, name=&#39;textrank&#39;, last=True) . ValueError Traceback (most recent call last) &lt;ipython-input-29-cd319957f3b6&gt; in &lt;module&gt;() 2 import pytextrank 3 tr = pytextrank.TextRank() -&gt; 4 nlp.add_pipe(tr.PipelineComponent, name=&#39;textrank&#39;, last=True) /usr/local/lib/python3.6/dist-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate) 746 batch_size=1000, 747 disable=[], --&gt; 748 cleanup=False, 749 component_cfg=None, 750 n_process=1, ValueError: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got &lt;bound method TextRank.PipelineComponent of &lt;pytextrank.pytextrank.TextRank object at 0x7f4fea403550&gt;&gt; (name: &#39;textrank&#39;). - If you created your component with `nlp.create_pipe(&#39;name&#39;)`: remove nlp.create_pipe and call `nlp.add_pipe(&#39;name&#39;)` instead. - If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe(&#39;textcat&#39;)`. - If you&#39;re using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component(&#39;your_name&#39;)`. You can then run `nlp.add_pipe(&#39;your_name&#39;)` to add it to the pipeline. . So I used the @Language.factory decorator to define a TextRank component, and then called the nlp.add_pipe function with textrank. But this didn&#39;t work either. The error message reads: &#39;Chinese&#39; object has no attribute &#39;sents&#39;. . from spacy.language import Language tr = pytextrank.TextRank() @Language.factory(&quot;textrank&quot;) def create_textrank_component(nlp: Language, name: str): return tr.PipelineComponent(nlp) . nlp.add_pipe(&#39;textrank&#39;) . AttributeError Traceback (most recent call last) &lt;ipython-input-31-1c84bbe50472&gt; in &lt;module&gt;() 1 #collapse-output -&gt; 2 nlp.add_pipe(&#39;textrank&#39;) /usr/local/lib/python3.6/dist-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate) 770 if is_python2 and n_process != 1: 771 warnings.warn(Warnings.W023) --&gt; 772 n_process = 1 773 if n_threads != -1: 774 warnings.warn(Warnings.W016, DeprecationWarning) /usr/local/lib/python3.6/dist-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate) 656 link_vectors_to_models(self.vocab) 657 if self.vocab.vectors.data.shape[1]: --&gt; 658 cfg[&#34;pretrained_vectors&#34;] = self.vocab.vectors.name 659 if sgd is None: 660 sgd = create_default_optimizer(Model.ops) /usr/local/lib/python3.6/dist-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate) /usr/local/lib/python3.6/dist-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate) /usr/local/lib/python3.6/dist-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides) &lt;ipython-input-30-fb02aff6bab9&gt; in create_textrank_component(nlp, name) 5 @Language.factory(&#34;textrank&#34;) 6 def create_textrank_component(nlp: Language, name: str): -&gt; 7 return tr.PipelineComponent(nlp) /usr/local/lib/python3.6/dist-packages/pytextrank/pytextrank.py in PipelineComponent(self, doc) 559 Doc.set_extension(&#34;phrases&#34;, force=True, default=[]) 560 Doc.set_extension(&#34;textrank&#34;, force=True, default=self) --&gt; 561 doc._.phrases = self.calc_textrank() 562 563 return doc /usr/local/lib/python3.6/dist-packages/pytextrank/pytextrank.py in calc_textrank(self) 375 t0 = time.time() 376 --&gt; 377 for sent in self.doc.sents: 378 self.link_sentence(sent) 379 AttributeError: &#39;Chinese&#39; object has no attribute &#39;sents&#39; . rake-spacy . I couldn&#39;t even install rake-spacy. . !pip install rake-spacy . ERROR: Could not find a version that satisfies the requirement rake-spacy ERROR: No matching distribution found for rake-spacy . rake-keyword . !pip install rake-keyword . According to the documentation on PYPI, the import is done by from rake import Rake, but it didn&#39;t work. . from rake import Rake . ImportError Traceback (most recent call last) &lt;ipython-input-34-fe043f886018&gt; in &lt;module&gt;() 1 #collapse-output -&gt; 2 from rake import Rake ImportError: cannot import name &#39;Rake&#39; NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the &#34;Open Examples&#34; button below. . However, based on the documentation on GitHub, this is done by from rake import RAKE instead. But it didn&#39;t work either. . from rake import RAKE . ImportError Traceback (most recent call last) &lt;ipython-input-35-59e63adf01ef&gt; in &lt;module&gt;() 1 #collapse-output -&gt; 2 from rake import RAKE ImportError: cannot import name &#39;RAKE&#39; NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the &#34;Open Examples&#34; button below. . . Recap . Integration of CKIP Transformers with spaCy and the TextRank algorithm generates decent results for extracting keywords from texts in traditional Chinese. Although there are many Python libraries out there that implement TextRank, none of them works better than the TextRank4Keyword class crafted from scratch. Until I figure out how to properly add the TextRank component to the spaCy pipeline, I&#39;ll stick with my working pipeline shown here. As a final thought, spaCy recently released v3.0, which supports pretrained transformer models. I can&#39;t wait to give it a try and see how this would change the workflow of extracting keywords or other NLP tasks. But that&#39;ll have to wait until next post. .",
            "url": "https://howard-haowen.github.io/blog.ai/keyword-extraction/spacy/textacy/ckip-transformers/jieba/textrank/rake/2021/02/16/Adding-a-custom-tokenizer-to-spaCy-and-extracting-keywords.html",
            "relUrl": "/keyword-extraction/spacy/textacy/ckip-transformers/jieba/textrank/rake/2021/02/16/Adding-a-custom-tokenizer-to-spaCy-and-extracting-keywords.html",
            "date": " • Feb 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Many ways to segment Chinese",
            "content": ". Intro . Unlike English, Chinese does not use spaces in its writing system, which can be a pain in the neck (or in the eyes, for that matter) if you&#39;re learning to read Chinese. In a way, it&#39;s like trying to make sense out of long German words like Lebensabschnittspartner, which roughly means &quot;the person I&#39;m with today&quot; (taken from David Sedaris&#39;s language lessons published on the New Yorker). We&#39;ll see how computer models can help us with breaking a stretch of Chinese text into words (called tokenization in NLP jargon). To give computer models a hard time, we&#39;ll test out this text without punctuations. . text = &quot;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&quot; . This text is challenging not only because it can be segmented multiple ways but also because it could potentially express quite different meanings depending on how you interprete it. For instance, this part 今年好煩惱少不得打官司 could either mean &quot;This year will be great for you. You&#39;ll have few worries. Don&#39;t file any lawsuit&quot; or &quot;This year, you&#39;ll be very worried. A lawsuit is inevitable&quot;. Either way, it sounds like the kind of aphorism you&#39;d find in fortune cookies. Now that you know the secret to aphorisms being always right is ambiguity, we&#39;ll turn to five Python libraries for doing the hard work for us. . Jieba . Of the five tools to be introduced here, Jieba is perhaps the most widely used one, and it&#39;s even pre-installed on Colab and supported by spaCy. Unfortunately, Jieba told us that a lawsuit is inevitable this year... 😭 . import jieba tokens = jieba.cut(text) jieba_default = &quot; | &quot;.join(tokens) print(jieba_default) . . Building prefix dict from the default dictionary ... Dumping model to file cache /tmp/jieba.cache Loading model cost 0.741 seconds. Prefix dict has been built successfully. . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸養 | 牛 | 隻 | 隻 | 大如山 | 老鼠 | 隻 | 隻 | 死 . The result is quite satisfying, except for 酸養, which is not even a word. Jieba is famouse for being super fast. If we run the segmentation function 1000000 times, top results we got are 256 nanoseconds per loop! . %timeit jieba.cut(text) . . The slowest run took 12.90 times longer than the fastest. This could mean that an intermediate result is being cached. 1000000 loops, best of 3: 256 ns per loop . Let&#39;s write a function for later use. . def Jieba_tokenizer(text): tokens = jieba.cut(text) result = &quot; | &quot;.join(tokens) return result . PKUSeg . As its name suggests, PKUSeg is built by the Language Computing and Machine Learning Group at Peking (aka. Beijing) University. It&#39;s been recently integrated into spaCy. . !pip install -U pkuseg . Collecting pkuseg Downloading https://files.pythonhosted.org/packages/ed/68/2dfaa18f86df4cf38a90ef024e18b36d06603ebc992a2dcc16f83b00b80d/pkuseg-0.0.25-cp36-cp36m-manylinux1_x86_64.whl (50.2MB) |████████████████████████████████| 50.2MB 66kB/s Requirement already satisfied, skipping upgrade: numpy&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from pkuseg) (1.19.5) Requirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.6/dist-packages (from pkuseg) (0.29.21) Installing collected packages: pkuseg Successfully installed pkuseg-0.0.25 . . Here&#39;s the result. . import pkuseg pku = pkuseg.pkuseg() result = pku.cut(text) result = &quot; | &quot;.join(result) result . . &#39;今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀 | 酒剛 | 剛 | 好 | 做 | 醋 | 格外 | 酸養 | 牛 | 隻隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死&#39; . Compared with Jieba, PKUSeg not only got more wrong tokens (酸養 and 酒剛) but also ran at a much slower speed. . %timeit pku.cut(text) . . 1000 loops, best of 3: 648 µs per loop . Yet, PKUSeg has one nice feature absent from Jieba. . Users have the option to choose from four domain-specific models, including news, web, medicine, and tourism. . This can be quite helpful if you&#39;re specifically dealing with texts in any of the four domains. Let&#39;s test the news domain with the first paragraph of a news article about Covid-19 published on Yahoo News. . article = &#39;&#39;&#39; 台灣新冠肺炎連續第6天零本土病例破功！中央流行疫情指揮中心指揮官陳時中今天宣布國內新增4例本土確定病例，均為桃園醫院感染事件之確診個案相關接觸者，其中3例為案863之同住家人(案907、909、910)，研判與案863、864、865為一起家庭群聚案，其中1人（案907）死亡，是相隔8個月以來再添死亡病例；另1例為案889之就醫相關接觸者(案908)。此外，今天也新增6例境外移入確定病例，分別自印尼(案901)、捷克(案902)及巴西(案903至906)入境。衛福部桃園醫院感染累計達19例(其中1人死亡)，全台達909例、8死。 &#39;&#39;&#39; . Here&#39;s the result with the default settinng. . pku = pkuseg.pkuseg() result = pku.cut(article) result = &quot; | &quot;.join(result) result . . &#39;台灣 | 新冠 | 肺炎 | 連續 | 第6 | 天 | 零 | 本土 | 病例 | 破功 | ！ | 中央 | 流行 | 疫情 | 指揮 | 中心 | 指揮官 | 陳時 | 中 | 今天 | 宣布 | 國內 | 新增 | 4 | 例 | 本土 | 確定 | 病例 | ， | 均 | 為 | 桃園 | 醫院 | 感染 | 事件 | 之 | 確 | 診個案 | 相關 | 接觸者 | ， | 其中 | 3 | 例 | 為案 | 863 | 之 | 同 | 住家人 | ( | 案 | 907 | 、 | 909 | 、 | 910 | ) | ， | 研判 | 與案 | 863 | 、 | 864 | 、 | 865 | 為 | 一起 | 家庭 | 群聚案 | ， | 其中 | 1 | 人 | （ | 案 | 907 | ） | 死亡 | ， | 是 | 相隔 | 8 | 個 | 月 | 以 | 來 | 再 | 添 | 死亡 | 病例 | ； | 另 | 1 | 例 | 為案 | 889 | 之 | 就 | 醫 | 相關 | 接觸者 | ( | 案 | 908 | ) | 。 | 此外 | ， | 今天 | 也 | 新增 | 6例 | 境外 | 移入 | 確定 | 病例 | ， | 分別 | 自 | 印尼 | ( | 案 | 901 | ) | 、 | 捷克 | ( | 案 | 902 | ) | 及 | 巴西 | ( | 案 | 903 | 至 | 906 | ) | 入境 | 。 | 衛福部 | 桃園 | 醫院 | 感染 | 累計 | 達 | 19 | 例 | ( | 其中 | 1 | 人 | 死亡 | ) | ， | 全 | 台 | 達 | 909 | 例 | 、 | 8 | 死 | 。&#39; . Here&#39;s the result with the model_name argument set to news. Both models made some mistakes here and there, but what&#39;s surprising to me is that the news-specific model even made a mistake when parsing 新冠肺炎, which literally means &quot;new coronavirus disease&quot; and refers to Covid-19. . pku = pkuseg.pkuseg(model_name=&#39;news&#39;) result = pku.cut(article) result = &quot; | &quot;.join(result) result . . Downloading: &#34;https://github.com/lancopku/pkuseg-python/releases/download/v0.0.16/news.zip&#34; to /root/.pkuseg/news.zip 100%|██████████| 43767759/43767759 [00:00&lt;00:00, 104004889.71it/s] . &#39;台灣 | 新 | 冠 | 肺 | 炎連 | 續 | 第6天 | 零本土 | 病例 | 破功 | ！ | 中央 | 流行疫情指揮中心 | 指揮 | 官 | 陳 | 時 | 中 | 今天 | 宣布 | 國內 | 新增 | 4例 | 本土 | 確定 | 病例 | ， | 均 | 為桃園醫院 | 感染 | 事件 | 之 | 確 | 診 | 個 | 案 | 相關 | 接觸 | 者 | ， | 其中 | 3例 | 為案 | 863 | 之 | 同 | 住 | 家人 | (案 | 907 | 、 | 909 | 、 | 910) | ， | 研判 | 與案 | 863 | 、 | 864 | 、 | 865為 | 一起 | 家庭 | 群 | 聚案 | ， | 其中 | 1 | 人 | （ | 案 | 907 | ） | 死亡 | ， | 是 | 相隔 | 8個月 | 以 | 來 | 再 | 添 | 死亡 | 病例 | ； | 另 | 1例 | 為案 | 889 | 之 | 就 | 醫 | 相關 | 接觸 | 者 | (案 | 908) | 。 | 此外 | ， | 今天 | 也 | 新增 | 6例 | 境外 | 移入 | 確定 | 病例 | ， | 分 | 別 | 自 | 印尼 | (案 | 901) | 、 | 捷克 | (案 | 902) | 及 | 巴西 | (案 | 903至906 | ) | 入境 | 。 | 衛 | 福部桃園醫院 | 感染 | 累 | 計達 | 19例 | ( | 其中 | 1 | 人 | 死亡 | ) | ， | 全 | 台 | 達 | 909例 | 、 | 8 | 死 | 。&#39; . Let&#39;s write a function for later use. . def PKU_tokenizer(text): pku = pkuseg.pkuseg() tokens = pku.cut(text) result = &quot; | &quot;.join(tokens) return result . PyHanLP . Next, we&#39;ll try PyHanLP. It&#39;ll take some time to download the model and data files (about 640MB in total). . !pip install pyhanlp . Collecting pyhanlp Downloading https://files.pythonhosted.org/packages/8f/99/13078d71bc9f77705a29f932359046abac3001335ea1d21e91120b200b21/pyhanlp-0.1.66.tar.gz (86kB) |████████████████████████████████| 92kB 9.0MB/s Collecting jpype1==0.7.0 Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB) |████████████████████████████████| 2.7MB 12.4MB/s Building wheels for collected packages: pyhanlp Building wheel for pyhanlp (setup.py) ... done Created wheel for pyhanlp: filename=pyhanlp-0.1.66-py2.py3-none-any.whl size=29371 sha256=cbe214d3e71b3e4e5692c0570e6eadbafc6845b99409abc5af1d790d9b7ee50f Stored in directory: /root/.cache/pip/wheels/25/8d/5d/6b642484b1abd87474914e6cf0d3f3a15d8f2653e15ff60f9e Successfully built pyhanlp Installing collected packages: jpype1, pyhanlp Successfully installed jpype1-0.7.0 pyhanlp-0.1.66 . from pyhanlp import * . 下载 https://file.hankcs.com/hanlp/hanlp-1.7.8-release.zip 到 /usr/local/lib/python3.6/dist-packages/pyhanlp/static/hanlp-1.7.8-release.zip 100.00%, 1 MB, 187 KB/s, 还有 0 分 0 秒 下载 https://file.hankcs.com/hanlp/data-for-1.7.5.zip 到 /usr/local/lib/python3.6/dist-packages/pyhanlp/static/data-for-1.7.8.zip 98.24%, 626 MB, 8117 KB/s, 还有 0 分 1 秒 . With PyHanLP, we got a similar parsing result, but without the error that Jieba produced. . tokens = HanLP.segment(text) token_list = [res.word for res in tokens] pyhan = &quot; | &quot;.join(token_list) print(pyhan) . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀 | 酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛 | 隻 | 隻 | 大 | 如山 | 老鼠 | 隻 | 隻 | 死 . However, PyHanLP is about 26 times slower than Jieba, as timed below. . %timeit HanLP.segment(text) . The slowest run took 11.80 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 3: 24.6 µs per loop . Let&#39;s write a function for later use. . def PyHan_tokenizer(text): tokens = HanLP.segment(text) token_list = [res.word for res in tokens] result = &quot; | &quot;.join(token_list) return result . SnowNLP . Next is SnowNLP, which I came across only recently. While PyHanLP is about 640MB in size, SnowNLP takes up only less than 40MB. . !pip install snownlp from snownlp import SnowNLP . Collecting snownlp Downloading https://files.pythonhosted.org/packages/3d/b3/37567686662100d3bce62d3b0f2adec18ab4b9ff2b61abd7a61c39343c1d/snownlp-0.12.3.tar.gz (37.6MB) |████████████████████████████████| 37.6MB 86kB/s Building wheels for collected packages: snownlp Building wheel for snownlp (setup.py) ... done Created wheel for snownlp: filename=snownlp-0.12.3-cp36-none-any.whl size=37760957 sha256=7de1997923cd51c8c45b896d9a29792e57652d5f55e3caf088212be684c50b36 Stored in directory: /root/.cache/pip/wheels/f3/81/25/7c197493bd7daf177016f1a951c5c3a53b1c7e9339fd11ec8f Successfully built snownlp Installing collected packages: snownlp Successfully installed snownlp-0.12.3 . SnowNLP gave a similar result, but made two parsing mistakes. Neither 做醋格 nor 外酸 is a legitimate word. . tokens = SnowNLP(text) token_list = [tokens.words][0] snow = &quot; | &quot;.join(token_list) print(snow) . 今 | 年 | 好 | 煩 | 惱 | 少不得 | 打 | 官司 | 釀 | 酒 | 剛 | 剛 | 好 | 做醋格 | 外酸 | 養 | 牛 | 隻 | 隻 | 大 | 如 | 山 | 老 | 鼠 | 隻 | 隻 | 死 . SnowNLP not only made more mistakes, but also took longer to run. . %timeit SnowNLP(text) . 10000 loops, best of 3: 35.4 µs per loop . But SnowNLP has a convenient feature inspired by TextBlob. Any instance of SnowNLP() has such attributes as words, pinyin (for romanization of words), tags (for parts of speech tags), and even sentiments, which calculates the probability of a text being positive. . print(tokens.words) . [&#39;今&#39;, &#39;年&#39;, &#39;好&#39;, &#39;煩&#39;, &#39;惱&#39;, &#39;少不得&#39;, &#39;打&#39;, &#39;官司&#39;, &#39;釀&#39;, &#39;酒&#39;, &#39;剛&#39;, &#39;剛&#39;, &#39;好&#39;, &#39;做醋格&#39;, &#39;外酸&#39;, &#39;養&#39;, &#39;牛&#39;, &#39;隻&#39;, &#39;隻&#39;, &#39;大&#39;, &#39;如&#39;, &#39;山&#39;, &#39;老&#39;, &#39;鼠&#39;, &#39;隻&#39;, &#39;隻&#39;, &#39;死&#39;] . print(tokens.pinyin) . [&#39;jin&#39;, &#39;nian&#39;, &#39;hao&#39;, &#39;煩&#39;, &#39;惱&#39;, &#39;shao&#39;, &#39;bu&#39;, &#39;de&#39;, &#39;da&#39;, &#39;guan&#39;, &#39;si&#39;, &#39;釀&#39;, &#39;jiu&#39;, &#39;剛&#39;, &#39;剛&#39;, &#39;hao&#39;, &#39;zuo&#39;, &#39;cu&#39;, &#39;ge&#39;, &#39;wai&#39;, &#39;suan&#39;, &#39;養&#39;, &#39;niu&#39;, &#39;隻&#39;, &#39;隻&#39;, &#39;da&#39;, &#39;ru&#39;, &#39;shan&#39;, &#39;lao&#39;, &#39;shu&#39;, &#39;隻&#39;, &#39;隻&#39;, &#39;si&#39;] . print(list(tokens.tags)) . [(&#39;今&#39;, &#39;Tg&#39;), (&#39;年&#39;, &#39;q&#39;), (&#39;好&#39;, &#39;a&#39;), (&#39;煩&#39;, &#39;Rg&#39;), (&#39;惱&#39;, &#39;Rg&#39;), (&#39;少不得&#39;, &#39;Rg&#39;), (&#39;打&#39;, &#39;v&#39;), (&#39;官司&#39;, &#39;n&#39;), (&#39;釀&#39;, &#39;u&#39;), (&#39;酒&#39;, &#39;n&#39;), (&#39;剛&#39;, &#39;i&#39;), (&#39;剛&#39;, &#39;Mg&#39;), (&#39;好&#39;, &#39;a&#39;), (&#39;做醋格&#39;, &#39;Ag&#39;), (&#39;外酸&#39;, &#39;Ng&#39;), (&#39;養&#39;, &#39;Dg&#39;), (&#39;牛&#39;, &#39;Ag&#39;), (&#39;隻&#39;, &#39;Bg&#39;), (&#39;隻&#39;, &#39;a&#39;), (&#39;大&#39;, &#39;a&#39;), (&#39;如&#39;, &#39;v&#39;), (&#39;山&#39;, &#39;n&#39;), (&#39;老&#39;, &#39;a&#39;), (&#39;鼠&#39;, &#39;Ng&#39;), (&#39;隻&#39;, &#39;Ag&#39;), (&#39;隻&#39;, &#39;Bg&#39;), (&#39;死&#39;, &#39;a&#39;)] . print(tokens.sentiments) . 0.04306320074116554 . Again, let&#39;s write a function for later use. . def Snow_tokenizer(text): tokens = SnowNLP(text) token_list = [tokens.words][0] result = &quot; | &quot;.join(token_list) return result . CKIP Transformers . While the four models above are primarily trained on simplified Chinese, CKIP Transformers is trained on traditional Chinese. It is created by the CKIP Lab at Academia Sinica. As its name suggests, CKIP Transformers is built on the Transformer architecture, such as BERT and ALBERT. . Note: Read this to find out How Google Changed NLP. . . !pip install -U ckip-transformers from ckip_transformers.nlp import CkipWordSegmenter . Collecting ckip-transformers Downloading https://files.pythonhosted.org/packages/19/53/81d1a8895cbbc02bf32771a7a43d78ad29a8c281f732816ac422bf54f937/ckip_transformers-0.2.1-py3-none-any.whl Collecting transformers&gt;=3.5.0 Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB) |████████████████████████████████| 1.8MB 22.8MB/s Requirement already satisfied, skipping upgrade: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (4.41.1) Requirement already satisfied, skipping upgrade: torch&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (1.7.0+cu101) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) |████████████████████████████████| 890kB 43.0MB/s Requirement already satisfied, skipping upgrade: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (0.8) Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (20.8) Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (3.0.12) Collecting tokenizers==0.9.4 Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB) |████████████████████████████████| 2.9MB 49.4MB/s Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (2019.12.20) Requirement already satisfied, skipping upgrade: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (3.4.0) Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (1.19.5) Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.5.0-&gt;ckip-transformers) (2.23.0) Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.1.0-&gt;ckip-transformers) (3.7.4.3) Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.1.0-&gt;ckip-transformers) (0.16.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.15.0) Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (7.1.2) Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.0.0) Requirement already satisfied, skipping upgrade: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2.4.7) Requirement already satisfied, skipping upgrade: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (3.4.0) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2020.12.5) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (2.10) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers&gt;=3.5.0-&gt;ckip-transformers) (1.24.3) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=010fd3e1a8d79574a0b5c323c333d1738886852c4c306fa9d161d1b51f7944b5 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sacremoses, tokenizers, transformers, ckip-transformers Successfully installed ckip-transformers-0.2.1 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2 . CKIP Transformers gives its users the freedom to choose between speed and accuracy. It comes with three levels; the smaller the number, the shorter the running time. All you need to do is pass a number to the level argument of CkipWordSegmenter(). Here&#39;re the models and F1 scores for each level: . Level 1: CKIP ALBERT Tiny, 96.66% | Level 2: CKIP ALBERT Base, 97.33% | Level 3: CKIP BERT Base, 97.60% | . By comparison, the F1 score for Jieba is only 81.18%. For more stats, visit the CKIP Lab&#39;s repo. . ws_driver = CkipWordSegmenter(level=1, device=0) . Here&#39;s the result at Level 1. What&#39;s suprising here is that this big chunk 大如山老鼠 was not further segmented. But this is not a mistake. It simply means that the model has learned it as an idiom. . tokens = ws_driver([text]) ckip_1 = &quot; | &quot;.join(tokens[0]) print(ckip_1) . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3284.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 3.98it/s] . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛 | 隻隻 | 大如山老鼠 | 隻隻 | 死 . . Of the five libraries covered here, CKIP Transformers by far takes the longest time to run. But where it lags behind in speed (i.e. 17.8 ms per loop for top 3 results), it makes it up in accuracy. . . Warning: Don&#8217;t toggle to show the output unless you really want to see a long list of details. . %timeit ws_driver([text]) . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1721.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 97.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1529.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1633.93it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 153.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4549.14it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 140.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1354.75it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.18it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1138.52it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2458.56it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1108.43it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 171.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1831.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 115.85it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3184.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.78it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3622.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 605.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1614.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.17it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2353.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.49it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2058.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3847.99it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1375.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.76it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1582.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 76.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3248.88it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.66it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3141.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.91it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2935.13it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2993.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 665.87it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1216.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 140.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 302.25it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.86it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3276.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 84.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 388.40it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4490.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.00it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4288.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.40it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3640.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 90.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 249.28it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 115.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1954.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 77.90it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 710.54it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 123.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1486.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 87.35it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1965.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 505.64it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3070.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2706.00it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 75.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2582.70it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 130.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 500.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.03it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 484.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 166.90it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 570.58it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 108.91it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2185.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 335.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 347.93it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3844.46it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 541.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.98it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2597.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4319.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4987.28it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 86.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 533.56it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.71it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 589.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.51it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 367.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4396.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 92.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 550.22it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3971.88it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 109.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 430.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 149.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2421.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3418.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 923.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1027.01it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 338.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3075.00it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 75.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1933.75it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 78.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4804.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5017.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4116.10it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 66.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3788.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 65.55it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3785.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5184.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 122.87it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 584.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.56it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2949.58it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.96it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1034.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.94it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3692.17it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 513.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1015.82it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 483.60it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.84it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 958.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.28it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4076.10it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 374.26it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 107.21it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 383.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3360.82it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 174.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5289.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.56it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 505.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 371.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 160.73it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4279.90it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 91.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2314.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.91it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1760.09it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 86.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2141.04it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2222.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 62.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3059.30it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5102.56it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 86.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1640.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 133.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1925.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5769.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4559.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1612.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 69.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2332.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 141.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3328.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.13it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4809.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 128.95it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4258.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5256.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 347.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.55it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5540.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 92.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2531.26it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 144.72it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2322.43it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.81it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5866.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3581.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3872.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4975.45it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 116.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2727.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 80.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4593.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5461.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.51it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3949.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 99.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4963.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.21it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2228.64it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.99it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5115.00it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 76.19it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 809.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.51it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5242.88it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 142.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5184.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.63it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5777.28it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5159.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1851.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.17it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 910.22it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 58.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1122.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 857.73it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 66.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3515.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.35it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1228.20it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5555.37it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 143.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5849.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 133.58it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5197.40it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2364.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 162.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 735.84it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 91.59it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4044.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 74.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1099.42it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 615.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2549.73it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 81.04it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 449.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.13it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2538.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2227.46it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5236.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4132.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1465.52it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 83.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1186.51it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.21it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1879.17it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 77.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2431.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3578.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4514.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4181.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 107.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5178.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4975.45it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4691.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 73.17it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2323.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 64.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2063.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 123.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 198.49it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4359.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 84.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5133.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1329.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 71.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1265.25it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 460.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.94it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4387.35it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.12it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4040.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1589.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4249.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4452.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 48.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 393.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 67.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 786.19it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 107.90it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 133.06it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 240.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.24it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2581.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5065.58it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.19it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3102.30it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 77.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4644.85it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.58it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4744.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2286.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 45.18it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2661.36it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.13it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1713.36it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 48.73it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 996.04it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 109.81it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2339.27it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1211.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.18it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1178.84it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4670.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.96it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4728.64it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 68.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4262.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.73it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3968.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4614.20it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4194.30it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4629.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4301.85it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 137.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4253.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 108.40it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5035.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.25it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5336.26it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.04it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5035.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 99.04it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5077.85it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 496.07it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 40.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2598.70it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.39it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5562.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 44.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3695.42it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 115.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.84it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4410.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5667.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.85it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4144.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3688.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 84.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 40.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 513.06it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2792.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 76.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1015.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 62.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 551.95it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 86.93it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 940.64it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 74.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 528.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 92.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4832.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5178.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 791.53it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.51it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4559.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1060.77it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 515.14it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 576.54it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4337.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4373.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 75.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4364.52it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 76.19it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4739.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4223.87it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 980.66it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.20it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4568.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 97.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4514.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 57.58it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3506.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 91.81it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4088.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.46it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4140.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 85.03it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4132.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 78.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4288.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.84it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4391.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 97.76it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4462.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 510.13it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.59it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4060.31it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 98.75it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4433.73it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 83.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2562.19it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4946.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5384.22it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 101.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1106.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 69.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4563.99it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 110.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2968.37it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2319.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 48.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5497.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.82it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5907.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.98it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5940.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 675.19it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 120.91it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5540.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 102.40it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1164.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.59it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 604.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5932.54it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.01it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1723.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5907.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6563.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 151.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4860.14it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6069.90it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 141.28it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5667.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.11it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5683.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 135.45it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6204.59it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6114.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4815.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 653.42it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 52.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5745.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5637.51it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.49it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5592.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 123.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4837.72it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 145.63it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1220.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 71.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 420.40it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 282.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.75it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 742.75it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5584.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 161.28it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3139.45it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 87.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 623.87it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 151.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 586.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 176.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5203.85it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 142.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5295.84it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 157.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 672.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6043.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 164.57it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 510.82it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 88.54it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5329.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3182.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6123.07it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 180.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5584.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 139.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6052.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5497.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 518.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.05it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6213.78it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 161.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2792.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 575.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.97it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 314.51it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.32it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 914.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 104.90it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5315.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 83.18it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 946.58it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 87.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 428.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 489.59it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.45it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2451.38it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 172.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2730.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 133.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 618.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.37it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 713.20it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 95.27it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1596.61it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 64.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 811.59it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2263.52it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 48.84it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3622.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2642.91it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 84.35it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 761.22it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 139.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2423.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 141.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5249.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 118.70it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6000.43it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.75it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5991.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 135.27it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2798.07it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 108.08it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3246.37it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 117.49it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5223.29it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6017.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 83.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3130.08it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 162.87it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2743.17it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2799.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 140.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1239.45it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.71it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3276.80it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 146.19it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1399.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.02it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6061.13it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 103.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 762.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 141.08it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5426.01it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5874.38it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.98it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4771.68it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 143.31it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3170.30it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 94.37it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3587.94it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 107.01it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4969.55it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 112.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5817.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.94it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5991.86it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 89.32it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5622.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 93.92it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 836.35it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.30it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5433.04it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 113.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1015.82it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.56it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5115.00it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 96.23it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 835.35it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 80.33it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2362.99it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 158.80it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2304.56it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 154.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6626.07it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 142.10it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5146.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 64.62it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 424.91it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4928.68it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 149.12it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5698.78it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 140.78it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6043.67it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 152.59it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5555.37it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.98it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 6842.26it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 162.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5675.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 169.06it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5229.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 49.94it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3313.04it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 51.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 829.90it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 69.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 539.74it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 81.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 649.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.52it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1151.96it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.85it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3731.59it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 118.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3518.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 143.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3008.83it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 184.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2641.25it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 153.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 559.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2803.68it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 166.56it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2931.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 168.61it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3084.05it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 155.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3826.92it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.72it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 935.18it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 70.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2504.06it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 100.86it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2931.03it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2590.68it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 146.09it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5140.08it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.99it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1217.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 134.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1049.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 97.22it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1402.78it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.37it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 887.12it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 128.88it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1734.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 58.15it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4804.47it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 132.32it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3401.71it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.64it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3795.75it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.07it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4922.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.14it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2186.81it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 130.60it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5210.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.68it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5236.33it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 139.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3155.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.93it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5753.50it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 737.40it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 125.77it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2498.10it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.55it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 4723.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 99.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3548.48it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 159.38it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3457.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.12it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 964.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 127.45it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1173.89it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.79it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2757.60it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 171.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3013.15it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 106.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2830.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 169.58it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3569.62it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 114.50it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1367.11it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 121.74it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2563.76it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.67it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5857.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 109.42it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1149.44it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.47it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5899.16it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.29it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5761.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 139.26it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5426.01it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.69it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 670.98it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 973.61it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 108.21it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5637.51it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 133.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 588.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 129.34it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2849.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 92.24it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2743.17it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 124.41it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5817.34it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 126.65it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5983.32it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3045.97it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 147.53it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 501.23it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 105.36it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2514.57it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 148.43it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 5548.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 111.28it/s] . 100 loops, best of 3: 17.8 ms per loop . . . Let&#39;s reinstantiate the CkipWordSegmenter() class and set the level to 2 this time. . ws_driver = CkipWordSegmenter(level=2, device=0) . Here&#39;s the result at Level 2, where 大如山老鼠 was properly segmented into 大, 如, and 山老鼠. . tokens = ws_driver([text]) ckip_2 = &quot; | &quot;.join(tokens[0]) print(ckip_2) . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 2253.79it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 47.86it/s] . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛好 | 做醋 | 格外 | 酸 | 養牛 | 隻隻 | 大 | 如 | 山老鼠 | 隻隻 | 死 . . Finally, let&#39;s create an instance of CkipWordSegmenter() at Level 3. . ws_driver = CkipWordSegmenter(level=3, device=0) . However, Level 3 didn&#39;t produce a better result than Level 2. For instance, 牛隻, though a legitimate token, is not appropriate in this context. . tokens = ws_driver([text]) ckip_3 = &quot; | &quot;.join(tokens[0]) print(ckip_3) . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 976.10it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 59.33it/s] . 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛隻 | 隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 . . Here&#39;s the function for later use, which takes two arguments instead of one, unlike in previous cases. . def Ckip_tokenizer(text, level): ws_driver = CkipWordSegmenter(level=level, device=0) tokens = ws_driver([text]) result = &quot; | &quot;.join(tokens[0]) return result . Comparison . To compare the five libraries, let&#39;s write a general function. . def Tokenizer(text, style): if style == &#39;jieba&#39;: result = Jieba_tokenizer(text) elif style == &#39;pku&#39;: result = PKU_tokenizer(text) elif style == &#39;pyhan&#39;: result = PyHan_tokenizer(text) elif style == &#39;snow&#39;: result = Snow_tokenizer(text) elif style == &#39;ckip&#39;: res1 = Ckip_tokenizer(text, 1) res2 = Ckip_tokenizer(text, 2) res3 = Ckip_tokenizer(text, 3) result = f&quot;Level 1: {res1} nLevel 2: {res2} nLevel 3: {res3}&quot; output = f&quot;Result tokenized by {style}: n{result}&quot; return output . Now I&#39;m interested in finding out whether simplified or traditional Chinese would have any effect on segmentation results. In addition to the text we&#39;ve been trying (let&#39;s rename it as text_A), we&#39;ll also test another challenging text taken from the PyHanLP repo (let&#39;s call it text_B), which is intended to be ambiguous in multiple places. Given these two texts, two versions of Chinese scripts (simplified and traditional), and five segmentation libraries, we end up having in total 20 combinations of texts and libraries. . import itertools textA_tra = &quot;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&quot; textA_sim = &quot;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&quot; textB_tra = &quot;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&quot; textB_sim = &quot;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&quot; texts = [textA_tra, textA_sim, textB_tra, textB_sim] tokenizers = [&#39;jieba&#39;, &#39;pku&#39;, &#39;pyhan&#39;, &#39;snow&#39;,&#39;ckip&#39;] testing_tup = list(itertools.product(texts, tokenizers)) testing_tup . [(&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;jieba&#39;), (&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;pku&#39;), (&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;pyhan&#39;), (&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;snow&#39;), (&#39;今年好煩惱少不得打官司釀酒剛剛好做醋格外酸養牛隻隻大如山老鼠隻隻死&#39;, &#39;ckip&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;jieba&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;pku&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;pyhan&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;snow&#39;), (&#39;今年好烦恼少不得打官司酿酒刚刚好做醋格外酸养牛隻隻大如山老鼠隻隻死&#39;, &#39;ckip&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;jieba&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;pku&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;pyhan&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;snow&#39;), (&#39;工信處女幹事每月經過下屬科室都要親口交代24口交換機等技術性器件的安裝工作&#39;, &#39;ckip&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;jieba&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;pku&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;pyhan&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;snow&#39;), (&#39;工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作&#39;, &#39;ckip&#39;)] . Here&#39;re the results for traditional textA. . for sent in testing_tup[:5]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸養 | 牛 | 隻 | 隻 | 大如山 | 老鼠 | 隻 | 隻 | 死 Result tokenized by pku: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀 | 酒剛 | 剛 | 好 | 做 | 醋 | 格外 | 酸養 | 牛 | 隻隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 Result tokenized by pyhan: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀 | 酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛 | 隻 | 隻 | 大 | 如山 | 老鼠 | 隻 | 隻 | 死 Result tokenized by snow: 今 | 年 | 好 | 煩 | 惱 | 少不得 | 打 | 官司 | 釀 | 酒 | 剛 | 剛 | 好 | 做醋格 | 外酸 | 養 | 牛 | 隻 | 隻 | 大 | 如 | 山 | 老 | 鼠 | 隻 | 隻 | 死 . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1287.78it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 136.95it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1394.38it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 66.44it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 998.41it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.47it/s] . Result tokenized by ckip: Level 1: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛 | 隻隻 | 大如山老鼠 | 隻隻 | 死 Level 2: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛好 | 做醋 | 格外 | 酸 | 養牛 | 隻隻 | 大 | 如 | 山老鼠 | 隻隻 | 死 Level 3: 今年 | 好 | 煩惱 | 少不得 | 打官司 | 釀酒 | 剛剛 | 好 | 做 | 醋 | 格外 | 酸 | 養 | 牛隻 | 隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 . . Here&#39;re the results for the simplified version of the same text. Notice that the outcome can be quite different simply because a traditional text is converted to its simplified counterpart. . for sent in testing_tup[5:10]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚 | 好 | 做 | 醋 | 格外 | 酸 | 养牛 | 隻 | 隻 | 大如山 | 老鼠 | 隻 | 隻 | 死 Result tokenized by pku: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚 | 好 | 做 | 醋 | 格外 | 酸养 | 牛隻 | 隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 Result tokenized by pyhan: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚好 | 做 | 醋 | 格外 | 酸 | 养牛 | 隻 | 隻 | 大 | 如山 | 老鼠 | 隻 | 隻 | 死 Result tokenized by snow: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚 | 好 | 做醋 | 格外 | 酸 | 养 | 牛 | 隻 | 隻 | 大 | 如 | 山 | 老 | 鼠 | 隻 | 隻 | 死 . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 303.61it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 123.89it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 695.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 66.45it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 392.84it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 72.00it/s] . Result tokenized by ckip: Level 1: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿 | 酒 | 刚刚 | 好 | 做 | 醋 | 格外 | 酸 | 养 | 牛隻隻 | 大如山老鼠 | 隻隻 | 死 Level 2: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚 | 好 | 做醋 | 格外 | 酸 | 养 | 牛 | 隻隻 | 大 | 如 | 山老鼠 | 隻隻 | 死 Level 3: 今年 | 好 | 烦恼 | 少不得 | 打官司 | 酿酒 | 刚刚好 | 做 | 醋 | 格外 | 酸 | 养 | 牛隻 | 隻 | 大 | 如 | 山 | 老鼠 | 隻隻 | 死 . . Here&#39;re the results for traditional textB. Serious mistakes include 處女 (for &quot;virgin&quot;) and 口交 (for &quot;blowjob&quot;). Both are correct words in Chinese, but not the intended ones in this context. . for sent in testing_tup[10:15]: result = Tokenizer(sent[0], sent[1]) print(result) . Result tokenized by jieba: 工信 | 處女 | 幹事 | 每月 | 經過 | 下屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口交 | 換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 Result tokenized by pku: 工信 | 處女 | 幹事 | 每月 | 經 | 過下 | 屬科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交 | 換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 Result tokenized by pyhan: 工 | 信 | 處女 | 幹 | 事 | 每月 | 經 | 過 | 下 | 屬 | 科室 | 都 | 要 | 親 | 口 | 交代 | 24 | 口交 | 換機 | 等 | 技 | 術 | 性 | 器件 | 的 | 安 | 裝 | 工作 Result tokenized by snow: 工 | 信 | 處 | 女 | 幹 | 事 | 每 | 月 | 經 | 過 | 下 | 屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交 | 換 | 機 | 等 | 技 | 術性 | 器件 | 的 | 安 | 裝 | 工作 . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 494.49it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 119.49it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 402.87it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.66it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 3942.02it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.56it/s] . Result tokenized by ckip: Level 1: 工信 | 處女 | 幹事 | 每 | 月 | 經過 | 下屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 Level 2: 工信處 | 女 | 幹事 | 每 | 月 | 經過 | 下屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 Level 3: 工信處 | 女 | 幹事 | 每 | 月 | 經過 | 下屬 | 科室 | 都 | 要 | 親口 | 交代 | 24 | 口 | 交換機 | 等 | 技術性 | 器件 | 的 | 安裝 | 工作 . . Here&#39;re the results for the simplified version of textB. In terms of textB, CKIP Transformers Level 2 and 3 are most stable, giving the same error-free results regardless of the writing sytems. . for sent in testing_tup[15:]: result = Tokenizer(sent[0], sent[1]) print(result) . . Result tokenized by jieba: 工信处 | 女干事 | 每月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Result tokenized by pku: 工信 | 处女 | 干事 | 每月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Result tokenized by pyhan: 工信处 | 女干事 | 每月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Result tokenized by snow: 工 | 信处女 | 干事 | 每月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 . Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1220.69it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 131.83it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 878.39it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 71.48it/s] Tokenization: 100%|██████████| 1/1 [00:00&lt;00:00, 1254.65it/s] Inference: 100%|██████████| 1/1 [00:00&lt;00:00, 60.75it/s] . Result tokenized by ckip: Level 1: 工信处 | 女干 | 事 | 每 | 月 | 经过 | 下 | 属 | 科室 | 都 | 要 | 亲 | 口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Level 2: 工信处 | 女 | 干事 | 每 | 月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 Level 3: 工信处 | 女 | 干事 | 每 | 月 | 经过 | 下属 | 科室 | 都 | 要 | 亲口 | 交代 | 24 | 口 | 交换机 | 等 | 技术性 | 器件 | 的 | 安装 | 工作 . . Recap . This post has tested five word segmentation libraries against two challenging Chinese texts. Here&#39;re the takeaways: . If you value speed more than anything, Jieba is definitely the top choice. If you&#39;re dealing with traditional Chinese, it is a good practice to first convert your texts to simplified Chinese before feeding them to Jieba. Doing this may produce better results. . | If you care more about accuracy instead, it&#39;s best to use CKIP Transformers. Its Level 2 and 3 produce consistent results whether your texts are in traditional or simplified Chinese. . | Finally, if you hope to levarage the power of NLP libraries such as spaCy and Texthero (by the way, their slogan is really awesome: from zero to hero), you&#39;ll have to go for Jieba or PKUSeg. I hope spaCy will also add CKIP to its inventory of tokenizers in the near future. . | .",
            "url": "https://howard-haowen.github.io/blog.ai/tokenization/jieba/pkuseg/pyhanlp/snownlp/ckip-transformers/2021/01/29/Many-ways-to-segment-Chinese.html",
            "relUrl": "/tokenization/jieba/pkuseg/pyhanlp/snownlp/ckip-transformers/2021/01/29/Many-ways-to-segment-Chinese.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Visualizing real estate prices with Altair",
            "content": "Intro . Several months into the journey of Python programming, I was already aware of visualization tools like Matplotlib Seaborn, and Plotly, which are commonly discussed on Medium. But I&#39;d never heard of Altair until I came across fastpages. Since I plan to keep writing on this fastpages-powered blog, I did some experiments with Altair. For illustration purprose, I&#39;ll be using the dataset of real estate prices in Kaohsiung, TW, which I&#39;ve cleaned and put together in my GitHub repo. For those of you who don&#39;t know Kaohsiung, it&#39;s selected by the New York Times as one of the 52 places to love in 2021. Maybe you&#39;ll consider buying an apartment in Kaohsiung after reading this post. Who knows? . Import dependencies . Altair is alrealdy pre-installed on Colab. So there&#39;s no need to pip-install it if you&#39;re doing this on Colab. . import pandas as pd import altair as alt from altair import datum . Load the dataset . The first thing to do is to git-clone the dataset into your environment. . !git clone -l -s https://github.com/howard-haowen/kh-real-estate cloned-repo %cd cloned-repo !ls . Cloning into &#39;cloned-repo&#39;... warning: --local is ignored remote: Enumerating objects: 100, done. remote: Counting objects: 100% (100/100), done. remote: Compressing objects: 100% (100/100), done. remote: Total 100 (delta 46), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (100/100), 3.30 MiB | 1.03 MiB/s, done. Resolving deltas: 100% (46/46), done. /content/cloned-repo catboost-model-feature-importance.png catboost-model-residuals.png catboost-model-feature-importance-shap-value.png compare-models.png catboost-model-learning-curve.png kh-house-prices.csv catboost-model-outliers.png kh-house-prices.pkl catboost-model.png LICENSE catboost-model-prediction-errors.png README.md . . Let&#39;s take a look at 5 random observations. . df = pd.read_pickle(&#39;kh-house-prices.pkl&#39;) df.sample(5) . . purpose trading_target land_area property_type living_room bedroom bathroom partition property_area is_managed total_floor parking_area parking_price parking_type land_use district trading_date trading_year built_date built_year price_per_sqm . 25204 住家用 | 房地(土地+建物)+車位 | 13.53 | 住宅大樓(11層含以上有電梯) | 2 | 4 | 2 | 有 | 129.39 | 有 | 13 | 0.00 | 0 | 坡道平面 | 商 | 楠梓區 | 2017-01-20 | 2017 | 1995-01-26 | 1995 | 33233.0 | . 19272 住家用 | 房地(土地+建物)+車位 | 18.24 | 住宅大樓(11層含以上有電梯) | 0 | 0 | 0 | 無 | 360.51 | 有 | 36 | 61.10 | 0 | 坡道平面 | 商 | 鼓山區 | 2016-05-20 | 2016 | 2014-06-26 | 2014 | 62717.0 | . 12575 住家用 | 房地(土地+建物)+車位 | 13.12 | 住宅大樓(11層含以上有電梯) | 2 | 3 | 2 | 有 | 145.90 | 有 | 15 | 12.66 | 840000 | 坡道機械 | 住 | 鼓山區 | 2015-07-14 | 2015 | 2014-05-15 | 2014 | 73101.0 | . 15299 住家用 | 房地(土地+建物)+車位 | 15.42 | 住宅大樓(11層含以上有電梯) | 2 | 3 | 2 | 有 | 125.39 | 有 | 15 | 11.24 | 0 | 坡道機械 | 住 | 左營區 | 2015-11-08 | 2015 | 2007-01-12 | 2007 | 43066.0 | . 31446 住家用 | 房地(土地+建物)+車位 | 13.91 | 住宅大樓(11層含以上有電梯) | 2 | 3 | 2 | 有 | 177.61 | 有 | 13 | 0.00 | 0 | 坡道機械 | 商 | 鼓山區 | 2017-12-12 | 2017 | 1996-04-05 | 1996 | 44479.0 | . The dataset includes 45717 observations and 21 columns. . df.shape . (45717, 21) . Most of the column names should be self-explanatory since I&#39;ve translated them from the original Chinese to English. . columns = df.columns.tolist() columns . [&#39;purpose&#39;, &#39;trading_target&#39;, &#39;land_area&#39;, &#39;property_type&#39;, &#39;living_room&#39;, &#39;bedroom&#39;, &#39;bathroom&#39;, &#39;partition&#39;, &#39;property_area&#39;, &#39;is_managed&#39;, &#39;total_floor&#39;, &#39;parking_area&#39;, &#39;parking_price&#39;, &#39;parking_type&#39;, &#39;land_use&#39;, &#39;district&#39;, &#39;trading_date&#39;, &#39;trading_year&#39;, &#39;built_date&#39;, &#39;built_year&#39;, &#39;price_per_sqm&#39;] . Here&#39;re some basic stats. . df.describe() . land_area living_room bedroom bathroom property_area total_floor parking_area parking_price trading_year built_year price_per_sqm . count 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 45717.000000 | 4.571700e+04 | 45717.000000 | 45717.000000 | 4.571700e+04 | . mean 24.949719 | 1.739987 | 2.921058 | 1.907540 | 145.261129 | 13.729947 | 6.606456 | 9.966087e+04 | 2016.760702 | 1999.837938 | 5.222278e+04 | . std 32.301563 | 0.583373 | 1.299294 | 1.084739 | 89.910644 | 7.810174 | 81.029070 | 5.323162e+05 | 1.699207 | 11.445783 | 2.236209e+04 | . min 0.010000 | 0.000000 | 0.000000 | 0.000000 | 0.020000 | 1.000000 | 0.000000 | 0.000000e+00 | 2012.000000 | 1913.000000 | 0.000000e+00 | . 25% 10.450000 | 2.000000 | 2.000000 | 1.000000 | 89.080000 | 8.000000 | 0.000000 | 0.000000e+00 | 2015.000000 | 1994.000000 | 3.849700e+04 | . 50% 16.630000 | 2.000000 | 3.000000 | 2.000000 | 128.440000 | 14.000000 | 0.000000 | 0.000000e+00 | 2017.000000 | 1999.000000 | 4.829400e+04 | . 75% 26.200000 | 2.000000 | 3.000000 | 2.000000 | 171.200000 | 15.000000 | 0.000000 | 0.000000e+00 | 2018.000000 | 2009.000000 | 6.233000e+04 | . max 2140.100000 | 22.000000 | 52.000000 | 50.000000 | 4119.900000 | 85.000000 | 17098.000000 | 1.000000e+07 | 2020.000000 | 2020.000000 | 1.048343e+06 | . MaxRowsError is the first trouble I got! It turns out that by default Altair only allows you to plot a dataset with a maximum of 5000 rows. . alt.Chart(df).mark_point().encode( x=&#39;trading_year&#39;, y=&#39;price_per_sqm&#39;, color=&#39;district&#39;, ).interactive() . MaxRowsError Traceback (most recent call last) /usr/local/lib/python3.6/dist-packages/altair/vegalite/v4/api.py in to_dict(self, *args, **kwargs) 361 copy = self.copy(deep=False) 362 original_data = getattr(copy, &#34;data&#34;, Undefined) --&gt; 363 copy.data = _prepare_data(original_data, context) 364 365 if original_data is not Undefined: /usr/local/lib/python3.6/dist-packages/altair/vegalite/v4/api.py in _prepare_data(data, context) 82 # convert dataframes or objects with __geo_interface__ to dict 83 if isinstance(data, pd.DataFrame) or hasattr(data, &#34;__geo_interface__&#34;): &gt; 84 data = _pipe(data, data_transformers.get()) 85 86 # convert string input to a URLData /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /usr/local/lib/python3.6/dist-packages/altair/vegalite/data.py in default_data_transformer(data, max_rows) 17 @curried.curry 18 def default_data_transformer(data, max_rows=5000): &gt; 19 return curried.pipe(data, limit_rows(max_rows=max_rows), to_values) 20 21 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /usr/local/lib/python3.6/dist-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /usr/local/lib/python3.6/dist-packages/altair/utils/data.py in limit_rows(data, max_rows) 82 &#34;than the maximum allowed ({}). &#34; 83 &#34;For information on how to plot larger datasets &#34; &gt; 84 &#34;in Altair, see the documentation&#34;.format(max_rows) 85 ) 86 return data MaxRowsError: The number of rows in your dataset is greater than the maximum allowed (5000). For information on how to plot larger datasets in Altair, see the documentation . alt.Chart(...) . . The limitation can be lifted by calling this function. . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . According to the official documentation, this is not a good solution. But I did it anyway because I didn&#39;t know better. I was then able to make a plot, but it only took seconds for my Colab notebook to crash. So the lesson learned is this: . . Warning: Never disable the restriction for max rows if you&#8217;re dealing with a huge amount of data! . A better way to deal with this is to pass data by URL, which only supports json and csv files. So I converted my dataframe to csv and then uploaded it to my GitHub repo. Then all that&#39;s needed to start using Altair is the URL to that file. . with open(&#39;kh-house-prices.csv&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as file: df.to_csv(file, index=False) . . Tip: For Altair to load your dataset properly, make sure the dataset is viewable by entering the URL in your browser. If your dataset is stored on GitHub, that means the URL has to start with https://raw.githubusercontent.com rather than https://github.com. . This URL is the data source from which we&#39;ll be making all the charts. . url= &quot;https://raw.githubusercontent.com/howard-haowen/kh-real-estate/main/kh-house-prices.csv&quot; . Simple charts . After we got the data loading and performance issue taken care of, let&#39;s break down the syntax of Altair. . I&#39;m a visual learner, so I personally think the easiest way to get started is to go to the Example Gallery and pick the kind of charts that you&#39;d like to draw. Most of the time, all you need to do is copy-paste the codes and change the data source as well as column names. . All fancy charts start with something simple.In the case of Altair, it&#39;s alt.Chart(), which takes either URL or a pandas DataFrame object (like df in our failed example above) as its argument. . Then you decide what kinds of marks you&#39;d like to draw on the chart by calling the .mark_X() function, where X could be circle if you want to represent an observation with a circle. Other types of marks used in this post include point, line, bar, and area. . Finally, you need to call the encode() function in order to map the properties of your dataset onto the chart you&#39;re making. In this example below, the function takes three arguments: . x for which column to be mapped to the x axis | y for which column to be mapped to the y axis | color for which column to be colored on the chart | . Once you pass url to alt.Chart() and the column names in your dataset to encode(), you&#39;ll get this chart. . alt.Chart(url).mark_circle().encode( x=&#39;built_date:T&#39;, y=&#39;price_per_sqm:Q&#39;, color=&#39;district:N&#39;,) . . . Note: If your data source is a dataframe, then column names are sufficient. But if your data source is an URL as is the case here, you have to specify your data types with :X right after the column names, where X can be one of these: . Q for quantitative data | O for ordinal data | N for nominal data | T for temporal data | G for geographic data | . And one thing that I like about Altair is that there&#39;re lots of predefined aggregate functions that you can use on the fly. For instance, you can pass temporal data to the function yearmonth(), which aggreates data points in terms of year and month. Or you can pass quantitative data to average(), which calculates the mean for you. This way, you won&#39;t have to create additional columns using pandas and keep your raw data as minimal as possible. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,) . . In pandas, we&#39;d filter data using df[FILTER]. In Altair, this is done by .transform_filter(). In the chart above, we see that the majority of data points gather in the lower right corner. So one way to zoom in is to set a range for built_year on the x axis, which represents the year a property was built. Suppose we want built_year to fall within 1950 and 2020, we do alt.FieldRangePredicate(field=&#39;built_year&#39;, range=[1950, 2020]). . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( alt.FieldRangePredicate(field=&#39;built_year&#39;, range=[1950, 2020]) ) . . Similarly, if we want price_per_sqm on the y axis, which represents property prices per square meter (in NT$ of course!) to be in the range of 10k and 300k, then we do alt.FieldRangePredicate(field=&#39;price_per_sqm&#39;, range=[10000, 300000]). . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( alt.FieldRangePredicate(field=&#39;price_per_sqm&#39;, range=[10000, 300000]) ) . . But what if we want to filter data from multiple columns? I found that an easy way to do that is to use datum.X, where X is a column name. Then the syntax is just like what you&#39;d see in pandas. Suppose we want built_year to be greater than 1950 and price_per_sqm less than 300k, then we do (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000). . . Important: It took me a while to figure what what kind of object datum is. It turns out that Altair is smart enough to take care of everything for you as long as you import datum. So be sure to do this: from altair import datum. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000) ) . . Finally, if you want to give viewers of your chart the liberty to zoom in and out, you can make an interactive chart simply by adding .interactive() to the end of your syntax. To see the effect, click on any grid of the following chart and then scroll your mouse or move two of your fingers up and down on your Magic Trackpad. . . Warning: Try not to make too many interactive charts if your dataset is huge because they can cause serious performance issues. . alt.Chart(url).mark_circle().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;average(price_per_sqm):Q&#39;, color=&#39;district:N&#39;,).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 300000) ).interactive() . . I think that&#39;s enough for the basics and for you to keep the ball rolling. Coming up are some of the numerous fancy charts that you can make with Altair. . Complex charts . Suppose we want to create a scatter plot where viewers can focus on data points from a particular district of their choice, the .add_selection() function can be quite handy. Let&#39;s first check out the unique districts in the datasets. (Btw, there&#39;re more districts in Kaohsiung. These are simply more densely populated areas.) . districts = df.district.unique().tolist() districts . [&#39;鼓山區&#39;, &#39;前金區&#39;, &#39;前鎮區&#39;, &#39;三民區&#39;, &#39;楠梓區&#39;, &#39;左營區&#39;, &#39;鳳山區&#39;, &#39;新興區&#39;, &#39;苓雅區&#39;] . We first create a variable selection, which we&#39;ll pass to .add_selection() later. The selection itself is a built-in function called alt.selection_single(), which takes the following arguments: . name for the name you want to display in the selection area | fields for a list of column names that views can choose from | init for a dictionary specifying the default value for each selectable column | bind for a dictionary specifying the way a column is to be selected (in this case, alt.binding_select() for a drop down box) and its possible values (indicated by the argument options) | . Additionally, if we want to display information about a data point upon mouseover, we can pass a list of column names to the argument tooltip of the .encode() function. . Importantly, for the interaction to work, we have to add .add_selection(selection) right before the .encode() function. . selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;district&#39;, ], init={&#39;district&#39;: &#39;左營區&#39;, }, bind={&#39;district&#39;: alt.binding_select(options=districts), } ) alt.Chart(url).mark_circle().add_selection(selection).encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;price_per_sqm:Q&#39;, color=alt.condition(selection, &#39;district:N&#39;, alt.value(&#39;lightgray&#39;)), tooltip=[&#39;property_type:N&#39;,&#39;property_area:Q&#39;,&#39;parking_area:Q&#39;, &#39;built_date:T&#39;,&#39;tradinng_date:T&#39;,&#39;price_per_sqm:Q&#39;], ).transform_filter( (datum.built_year &gt; 1950) &amp; (datum.price_per_sqm &lt; 200000) ) . . We can also make two charts and then concatenat them vertically by calling the function alt.vconcat(), which takes chart objects and data as its arguments. . selection = alt.selection_multi(fields=[&#39;district&#39;]) top = alt.Chart().mark_line().encode( x=&#39;yearmonth(built_date):T&#39;, y=&#39;mean(price_per_sqm):Q&#39;, color=&#39;district:N&#39; ).properties( width=600, height=200 ).transform_filter( selection ) bottom = alt.Chart().mark_bar().encode( x=&#39;yearmonth(trading_date):T&#39;, y=&#39;mean(price_per_sqm):Q&#39;, color=alt.condition(selection, alt.value(&#39;steelblue&#39;), alt.value(&#39;lightgray&#39;)) ).properties( width=600, height=100 ).add_selection( selection ) alt.vconcat( top, bottom, data=url ) . . We can make one chart respond to another chart based on selection on the second one. This can be useful when we want to have both a global and detailed view of the same chart. The key function we need is alt.Scale(). Watch the top chart change as you select different areas of the bottom chart. . brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(url).mark_area().encode( x = &#39;yearmonth(built_date):T&#39;, y = &#39;price_per_sqm:Q&#39; ).properties( width=600, height=200 ) upper = base.encode( alt.X(&#39;yearmonth(built_date):T&#39;, scale=alt.Scale(domain=brush)) ) lower = base.properties( height=60 ).add_selection(brush) upper &amp; lower . . Finally, you can also pick three random variables from your dataset and make a 3 times 3 grid of charts, with each varing in the x and y axis combination. To do that, we&#39;ll need to specify repetition in two places: once in the argument of the x and y axis (i.e. alt.repeat() within alt.X and alt.Y) and the other time in the outmost layer of the syntax (i.e. .repeat() at the very end). . alt.Chart(url).mark_circle().encode( alt.X(alt.repeat(&quot;column&quot;), type=&#39;quantitative&#39;), alt.Y(alt.repeat(&quot;row&quot;), type=&#39;quantitative&#39;), color=&#39;district:N&#39; ).properties( width=150, height=150 ).repeat( row=[&#39;property_area&#39;, &#39;price_per_sqm&#39;, &#39;built_year&#39;], column=[&#39;built_year&#39;, &#39;price_per_sqm&#39;, &#39;property_area&#39;] ) . . Recap . Altair is a Python library worth looking into if you want to show interactive charts on your websites and give your visitors some freedom to play with the outcome. This post only shows what I&#39;ve tried. If you wish to dig deeper into this library, uwdata/visualization-curriculum seems like a great resource, aside from the official documentation. Now that you know the average price of real estate in Kaohsiung, TW, would you consider moving down here? 👨‍💻 .",
            "url": "https://howard-haowen.github.io/blog.ai/visualization/real-estate-prices/altair/2021/01/24/Visualizing-real-estate-prices-with-Altair.html",
            "relUrl": "/visualization/real-estate-prices/altair/2021/01/24/Visualizing-real-estate-prices-with-Altair.html",
            "date": " • Jan 24, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "fastText embeddings for traditional Chinese",
            "content": ". Intro . This video explains to you what fastText is all about as if you were five years old. If the video doesn&#39;t load, click on this link. Basically, a fastText model maps a word to a series of numbers (called vectors or embeddings in NLP jargon) so that word similarity can be calcuated based on those numbers. . fastText cbow 300 dimensions from Facebook . Here&#39;re the simple steps for loading the Chinese model released by Facebook, abbreviated here as ft. . !pip install fasttext import fasttext . Collecting fasttext Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB) |████████████████████████████████| 71kB 4.4MB/s Requirement already satisfied: pybind11&gt;=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1) Requirement already satisfied: setuptools&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (51.1.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.19.5) Building wheels for collected packages: fasttext Building wheel for fasttext (setup.py) ... done Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3039122 sha256=5aa81e1045293ebc74315d2013c28cd0018ec96b8868502d535b71438f1faa0c Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592 Successfully built fasttext Installing collected packages: fasttext Successfully installed fasttext-0.9.2 . import fasttext.util fasttext.util.download_model(&#39;zh&#39;, if_exists=&#39;ignore&#39;) # zh = Chinese . Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.bin.gz . &#39;cc.zh.300.bin&#39; . ft = fasttext.load_model(&#39;cc.zh.300.bin&#39;) . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . The ft model covers a whopping great number of words, 2000000 to be exact, because it&#39;s trained on a HUGE corpus. . len(ft.words) . 2000000 . Let&#39;s check out the top 10 words most similar to &quot;疫情&quot; (meaning &quot;pandemic situation&quot;) according to the ft model. The numbers indicate the degree of similarity. The larger the number, the greater the similarity. . ft.get_nearest_neighbors(&quot;疫情&quot;) . [(0.7571706771850586, &#39;禽流感&#39;), (0.6940484046936035, &#39;甲流&#39;), (0.6807129383087158, &#39;流感&#39;), (0.6670429706573486, &#39;疫病&#39;), (0.6640030741691589, &#39;防疫&#39;), (0.6531218886375427, &#39;萨斯病&#39;), (0.6506668329238892, &#39;H1N1&#39;), (0.6495682001113892, &#39;疫症&#39;), (0.6432098150253296, &#39;ＳＡＲＳ&#39;), (0.642063319683075, &#39;疫区&#39;)] . . The results are pretty good. But the downside is that the ft model is huge in size. After being unzipped, the model file is about 6.74G. . fastText cbow 300 dimensions from ToastyNews in Cantonese . This article is what inpired me to write this post. The author trained a fastText model on articles written in Cantonese, which uses traditional characters. Here&#39;re the simple steps for loading his model, abbreviated here as hk. . Since his model is stored on GDrive, I find it more convenient to use the gdown library to download the model. . import gdown . url = &#39;https://drive.google.com/u/0/uc?export=download&amp;confirm=4g-b&amp;id=1kmZ8NKYDngKtA_-1f3ZdmbLV0CDBy1xA&#39; output = &#39;toasty_news.bin.gz&#39; gdown.download(url, output, quiet=False) . Downloading... From: https://drive.google.com/u/0/uc?export=download&amp;confirm=4g-b&amp;id=1kmZ8NKYDngKtA_-1f3ZdmbLV0CDBy1xA To: /content/toasty_news.bin.gz 2.77GB [00:26, 106MB/s] . &#39;toasty_news.bin.gz&#39; . The file needs to be first unzipped to be loaded as a fastText model. An easy way to do that is the command !gunzip plus a file name. . !gunzip toasty_news.bin.gz . hk = fasttext.load_model(&#39;/content/toasty_news.bin&#39;) hk.get_dimension() . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . 300 . The hk model covers 222906 words in total. . len(hk.words) . 222906 . fastText cbow 100 dimensions from Taiwan news in traditional Chinese . I trained a fastText model on 5816 articles of Taiwan news in traditional Chinese, most of them related to health and diseases. . tw = fasttext.load_model(path) # &quot;path&quot; is where my model is stored. tw.get_dimension() . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. . 100 . The tw model covers only 11089 words in total because it&#39;s trained on a much smaller corpus than the hk model. . len(tw.words) . 11089 . Comparison . My original plan was to compare all the three models and see what similar words they come up with given the same keyword. But the ft model is huge so I can&#39;t load all of them into RAM. The RAM limit on Colab is about 12G. So we&#39;ll just compare the tw and hk model. . Since we&#39;re not concerned with the degree of similarity, let&#39;s write a simple function to show just similar words. . def similar_words(keyword, model): top_10 = model.get_nearest_neighbors(keyword) top_10 = [w[1] for w in top_10] return top_10 . Then, calling the function similar_word(), with a keyword and a fastText model as the required arguments, shows the top ten words most similar to the keyword. . similar_words(&quot;疫情&quot;, hk) . [&#39;疫症&#39;, &#39;病疫情&#39;, &#39;武漢肺炎&#39;, &#39;疫潮&#39;, &#39;疫&#39;, &#39;新冠肺炎&#39;, &#39;疫調&#39;, &#39;疫市&#39;, &#39;新型冠狀病毒&#39;, &#39;疫病&#39;] . Now let&#39;s write a function to show the results of the two models side by side in a dataframe. . import pandas as pd models = {&#39;hk&#39;: hk, &#39;tw&#39;: tw} def compare_models(keyword, **models): hk_results = similar_words(keyword, models[&#39;hk&#39;]) tw_results = similar_words(keyword, models[&#39;tw&#39;]) data = {&#39;HKNews_&#39;+keyword: hk_results, &#39;TWNews_&#39;+keyword: tw_results} df = pd.DataFrame(data) return df . Let&#39;s test it out with the keyword &quot;疫情&quot;. . test = compare_models(&quot;疫情&quot;, **models) test . HKNews_疫情 TWNews_疫情 . 0 疫症 | 疫情國 | . 1 病疫情 | 因應 | . 2 武漢肺炎 | 防堵 | . 3 疫潮 | 切記 | . 4 疫 | 擴散 | . 5 新冠肺炎 | 屬地 | . 6 疫調 | 疫情處 | . 7 疫市 | 升溫 | . 8 新型冠狀病毒 | 警訊 | . 9 疫病 | 嚴峻 | . It&#39;s interesting that similar words of &quot;總統&quot; (meaning &quot;the president&quot;) include &quot;蔡總統&quot; (meaning &quot;President Tsai&quot;, referring to Taiwan&#39;s incumbent president) according to the hk model but not the tw model. I&#39;d expect the opposite. . test = compare_models(&quot;總統&quot;, **models) test . HKNews_總統 TWNews_總統 . 0 代總統 | 主持 | . 1 美國總統 | 總統府 | . 2 前總統 | 部長 | . 3 民選總統 | 親臨 | . 4 李總統 | 局長 | . 5 副總統 | 蘇益仁 | . 6 下任總統 | 幹事長 | . 7 總理 | 副院長 | . 8 首相 | 李明亮 | . 9 蔡總統 | 座談會 | . Again, it is the hk model, not the tw model, that knows &quot;蔡英文&quot; (meaning &quot;Tsai Ing-wen&quot;) is most similar to &quot;蔡總統&quot; (meaning &quot;President Tsai&quot;). The two linguistic terms have the same reference. . test = compare_models(&quot;蔡總統&quot;, **models) test . HKNews_蔡總統 TWNews_蔡總統 . 0 蔡英文 | 總統 | . 1 賴清德 | 主持 | . 2 馬英九 | 部長 | . 3 李總統 | 親臨 | . 4 林全 | 局長 | . 5 民進黨 | 陳建仁 | . 6 柯文哲 | 座談會 | . 7 總統 | 吳 | . 8 川普 | 副院長 | . 9 總統府 | 總統府 | . Finally, let&#39;s write a function to quickly compare a list of keywords. . def concat_dfs(keyword_list): dfs = [] for word in keyword_list: df = compare_models(word, **models) dfs.append(df) results = pd.concat(dfs, axis=1) return results . keywords = &quot;疫情 疫苗 病毒 肺炎 檢疫 流感 台灣&quot; key_list = keywords.split() concat_dfs(key_list) . HKNews_疫情 TWNews_疫情 HKNews_疫苗 TWNews_疫苗 HKNews_病毒 TWNews_病毒 HKNews_肺炎 TWNews_肺炎 HKNews_檢疫 TWNews_檢疫 HKNews_流感 TWNews_流感 HKNews_台灣 TWNews_台灣 . 0 疫症 | 疫情國 | 流感疫苗 | 接種 | 輪狀病毒 | 病毒型 | 武漢肺炎 | 豬鏈球菌 | 檢疫所 | 檢疫官 | 流感病毒 | 新流感 | 臺灣 | 臺灣 | . 1 病疫情 | 因應 | 免疫針 | 接種地 | 含病毒 | 腺病毒 | 武肺 | 鏈球菌 | 檢疫中心 | 檢疫站 | 流行性感冒 | 防流感 | 台灣國 | 根除 | . 2 武漢肺炎 | 防堵 | 抗體 | 接種為 | 冠狀病毒 | 病毒株 | 新冠肺炎 | 疾患 | 檢疫局 | 檢疫局 | 禽流感 | 打流感 | 台灣政府 | 歷史 | . 3 疫潮 | 切記 | 藥物 | 接種點 | 新病毒 | 病毒學 | 病疫 | 雙球菌 | 檢疫站 | 航機 | 流行病 | 對流感 | 中國大陸 | 一直 | . 4 疫 | 擴散 | 卡介苗 | 接種卡 | 腺病毒 | 型別 | 病疫情 | 心包膜炎 | 隔離 | 機場 | 疫症 | 豬流感 | 中國 | 亞太 | . 5 新冠肺炎 | 屬地 | 抗生素 | 疫苗量 | 殺病毒 | 流行株 | 疫症 | 特殊 | 自我隔離 | 入境 | 病疫情 | 抗流感 | 台灣人 | 諸多 | . 6 疫調 | 疫情處 | 輪狀病毒 | 卡介苗 | 麻疹病毒 | 株型別 | 非典型肺炎 | 侵襲性 | 隔離者 | 調查表 | 麻疹 | 流感疫 | 中國台灣 | 世紀 | . 7 疫市 | 升溫 | 接種 | 價 | 防病毒 | 腸 | 疫情 | 冠狀動脈 | 病毒檢測 | 港口 | 流行性腮腺炎 | 季節性 | 台灣獨立 | 跨國性 | . 8 新型冠狀病毒 | 警訊 | 預防接種 | 多合一 | 冠状病毒 | 重組 | 廢炎 | 症候群 | 健康申報 | 登機 | 流感疫苗 | 流感病 | 台灣社 | 面臨 | . 9 疫病 | 嚴峻 | 麻疹 | 廠牌 | 病原體 | 毒株 | 疫病 | 冠狀病毒 | 檢測 | 聲明卡 | 登革熱 | 新型 | 中華民國 | 之中 | . . keywords = &quot;頭痛 發燒 流鼻水 &quot; key_list = keywords.split() concat_dfs(key_list) . HKNews_頭痛 TWNews_頭痛 HKNews_發燒 TWNews_發燒 HKNews_流鼻水 TWNews_流鼻水 . 0 偏頭痛 | 肌肉痛 | 咳嗽 | 出現 | 鼻水 | 鼻水 | . 1 頭疼 | 骨頭痛 | 病徵 | 症狀 | 流鼻涕 | 流 | . 2 胃痛 | 噁心 | 發高燒 | 喉嚨痛 | 咳嗽 | 鼻塞 | . 3 痠痛 | 骨頭 | 發病 | 嗅覺 | 喉嚨痛 | 喉嚨 | . 4 酸痛 | 肌肉 | 喉嚨痛 | 味覺 | 出疹 | 喉嚨癢 | . 5 絞痛 | 後眼 | 流鼻水 | 鼻水 | 發燒 | 喉嚨痛 | . 6 腫痛 | 畏寒 | 症狀 | 咳嗽 | 皮疹 | 嗅覺 | . 7 頭暈 | 倦怠 | 徵狀 | 喉嚨 | 流鼻血 | 味覺 | . 8 心絞痛 | 窩痛 | 出疹 | 疲倦 | 肚瀉 | 紅疹 | . 9 腰背痛 | 結膜 | 呼吸困難 | 流 | 咳血 | 倦怠 | . Recap . You can easily find out words most similar to a keyword that you&#39;re interested in just by loading a fastText model. And for it to work pretty well, you don&#39;t even need to have a huge corpus at hand. Even if you don&#39;t know how to train a model from scratch, you can still make good use of fastText by loading pretrained models, like those released by Facebook. In total, 157 languages are covered, including even Malay and Malayalam! (Btw, check out this Malayalam grammar that I wrote and is now archived on Semantic Scholar.) . Note: This is my first post written in Jupyter notebook. After I uploaded the .ipynb file to GitHub, the post didn&#8217;t show up automatically and I got a CI failing warning in my repo. Listed in the tip section below is what I did to fix the problem, though I&#8217;m not sure which of them was the key. . Tip: 1. requested an automatic update by following the instructions in the troubleshooting guide 2. deleted the backtick symbol in the summary section of the front matter 3. uploaded the .ipynb file straight from Colab to GitHub instead of doing this manually .",
            "url": "https://howard-haowen.github.io/blog.ai/fasttext/embeddings/similar-words/2021/01/22/fastText-embeddings-for-traditional-Chinese.html",
            "relUrl": "/fasttext/embeddings/similar-words/2021/01/22/fastText-embeddings-for-traditional-Chinese.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "My First Post",
            "content": "My First Post: Setting tone for this blog . . There’s always an awesome list for X. . Don&#39;t reinvent the wheel is something you’ll hear a lot in the field of technology. That means, whenever possible, try to use existing tools out there instead of building from scratch. If you use GitHub long enough, you probably already know this: for almost everything you’d like to learn, there’s an awesome list for that, which collects all sorts of awesome resources, including tools and tutorials. All you need to do is search for awesome X on GitHub. Or better yet, there’s even a meta list of awesome lists on various topics, like sindresorhus/awesome. Included in it is an awesome list for linguistics, theimpossibleastronaut/awesome-linguistics, which is a good place to start learning about natural language processing (NLP) if you are a humanities major who knows nothing about programming, as I was about one year ago. To take one more example, I wish I had discovered keon/awesome-nlp much earlier, which could’ve saved me lots of time when I was still fumbling around and trying to wrap my head around how a tool, say gensim, fits into the broader picture of NLP. But sometimes the name of an awesome list doesn’t have the keyword awesome in it, such as this gem ivan-bilan/The-NLP-Pandect. In my opinion, although an awesome list by any other name would be as awesome, the The-NLP-Pandect repo would have got much more stars if awesome were in its name. . But the hardest part is to get the ball rolling. . However, awesome as they are, awesome lists can be quite intimidating to go through and easy to get lost in. And even when you are lucky enough to finally come across an awesome tool that you wanna try out, it sometimes takes lots of trials and errors to figure out the right way to get the ball rolling, especially when you are a fresh programmer. So on this blog, I plan to add my personal touch to various tools, documenting not only what I did right to get the ball rolling, but also what I did wrong to save you (or even future me) from abysmal frustration. For almost every tool, there is already a wide specturum of instructional documents available online, ranging from hardcore official documentations to professional posts on platforms like Medium. And this blog is meant to be a friendly complement to those. I’ll be writing in plain language because I was not trained for computer science or programming anyway. . The fast stack . I’d like to start with a series of tools that I dub the fast stack, including fastpages, fast.ai, fastText, and fastAPI. First of all, fastpages , designed by the awesome fast.ai team, is basically a template for building blogs (like this one!) within seconds. It does lots of awesome things for you hehind the scenes. Features that I like about it include: . automatically converts .md and .ipynb files on GitHub to posts on your blog | automatically adds links to Colab and GitHub | shows interactive visualizations of your data with the help of Altair | supports comments, tags, and fast search (super fast at that!) | is free from end to end | . Truth be told that I actually failed twice before I successfully set up this blog. The lesson learned is this: do exactly what’s said in the instructions! Humanities majors like me are often taught to be creative, but be sure to leave your creativity at the door when you set up computer programs. This then concludes my first post. Nothing is super technical here since it’s just a warm-up. I’ll save other tools in the fast stack for another day. . . I was clueless when I read PR in the instructions. It turns out to mean &#39;pull requests&#39;. Click on the tab that says &#39;pull requests&#39; when you are done forking the original repo. Then you&#39;re good to go by following the instructions there.",
            "url": "https://howard-haowen.github.io/blog.ai/awesome-list/fastpages/2020/01/17/my-first-post-dont-reinvent-the-wheel.html",
            "relUrl": "/awesome-list/fastpages/2020/01/17/my-first-post-dont-reinvent-the-wheel.html",
            "date": " • Jan 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "From a linguist to an engineer . My name is Haowen Jiang. I obtained a PhD of linguistics from Rice University and spent most of my early career in academia doing research on indigenous languages of Taiwan, all of them related to languages such as Tagalog and Malay/Indonesian. Then along came COVID-19, which disrupted most people&#39;s lives, including mine. Somehow I became hooked on machine learning and Python programming. After several months of self-learning, I transitioned from an IT muggle to an AI engineer. My current job primarily focuses on Natural Language Processing, including text classification, clustering, topic modeling, document similarity, and information retrieval. Microsoft Certified: Azure Data Scientist Associate .",
          "url": "https://howard-haowen.github.io/blog.ai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://howard-haowen.github.io/blog.ai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}